I ran into a Java deadlock in a large application, and based on the stack trace of the blocked threads.
It seems to be caused by each thread running WorkerExecutor.executeBlocking on the same two WorkerExecutor instances, but in a different order.
In my case, the deadlock issue is hit when the vertx.close and workerExecutor.close happened to be called at same time, one from main thread and another from eventloop thread.
POSTing many buffers to an echoing server gives thread blocked failures when we use drainHandler+writeQueueFull approach.
It's seems like at some time, something has kept the monitor of ASyncFileImpl instance, preventing the resume call to execute the method.
The current implementation of ConnectionBase#writeToChannel synchronises on the ConnectionBase (which is necessary) and holds this lock when calling ChannelHandlerContext#write or ChannelHandlerContext#writeAndFlush.
It happens that when the channel outbound buffer is full, the write operation drains the buffer on a flush changing the channel writability leading to potential deadlock in Vert.x.
The Consumer verticle acuires lock1 and gets stuck.
This can cause deadlocks when used from external threads or because the transport thread has changed (for instance a session reconnection with an scaled http server).
This can create deadlocks in SockJSSocket because a SockJSSocket may uses different request/websocket (reconnects) and event loops.
From my understanding of these stack traces and the threading model, this essentially means that any SockJSSession.write() call (regardless of the thread it is called from) can deadlock with a 'ServerConnection.handleClose()' call.
From my simple analysis it seems that the Exception handling and the request response seem to be locking on the HttpClientRequestImpl->handleException->getlock -> (this) and HttpClientRequestImpl->connect(this)->lamda->(this)
If you call Message.getValue() on the completion of a CompletableFuture returned by Pulsar Client, like Reader.readNextAsync() then there is a good chance to create a deadlock.
SimpleLoadManagerImpl.updateRanking should not cause deadlocks
We're seeing a deadlock when two different threads load different DataType classes at the same time.
But, this can cause a deadlock when multiple threads are trying to load these DataType classes because they each hold a lock on the class and end up waiting for the others to finish.
This is a problem because BinderTransport#handleTransaction() holds its 'this' lock while calling outgoingBinder.transact() in multiple places.
If two peer instances of BinderClientTransport and BinderServerTransport are running handleTransaction() on different threads at the same time, they can each end up holding their own lock while waiting (forever) for the other's.
ServerImpl.start() calls NettyServer.start() while holding ServerImpl.lock. NettyServer.start() awaits a submitted runnable in eventloop.
However, this pending runnable may never be executed because the eventloop might be executing some other task, like ServerListenerImpl.transportCreated(), that is trying to acquire ServerImpl.lock causing a deadlock.
There is a chance of deadlock when DelayedClientTransport is linked with an InProcessTransport. 
Methods of ClientTransport.Listener and ServerTransportListener are usually called under a lock.
However, these listeners usually call into channel layer code, and may in turn acquire locks from there, which forms a transport lock -> channel lock lock order.
On the other hand, when channel layer calls into transport layer, it's possible to form a channel lock -> transport lock lock order, which makes deadlock possible.
NettyClientTransport#newStream is currently a blocking operation. It blocks until the HEADERS frame has been written on the wire.
The blocking may also lead to deadlocking the EventLoop in cases where a DirectExecutor is used.
One scenario where a deadlock might happen is when the EventLoop is not able to completely flush the HEADERS frame on the wire because then Netty would internally create a task to flush the remaining bytes and put this task in its task queue.
This task can never be completed though as the EventLoop Thread is blocked by our very own newStream method waiting for the task to be completed.
In cases when a DirectExecutor is being used it might lead to the EventLoop blocking forever (deadlock effectively).
There is several scenarios how a deadlock could happen.
One such scenario is when you are writing a server in Netty and within that server you want to connect to a grpc service to fetch some data.
If you now use a DirectExecutor and reuse the EventLoop of the server with the grpc client, the TLS handshake would block the server's EventLoop, which is also the very EventLoop responsible for completing the TLS HandShake.
That way neither the server nor the client would ever make progress again.
Sometimes, even though the lock from the first thread is timed out and released automatically, the redisson_lock_queue and redisson_lock_timeout keys are stuck and the second thread can't acquire the lock, even though nobody is holding it.
The second thread is stuck for full acquisition timeout. Then, it fails to get the lock eventually, and keys redisson_lock_queue and redisson_lock_timeout disappear, making the lock available again.
We have experienced several threads locked forever waiting inside calls to RedissonLock.unlock() in our production system within a few hours after upgrading to Redisson 3.11.4 from Redisson 3.8.0.
To summarize the issue, we see that the timeouts in the redisson_lock_timeout sorted set created by RedissonFairLock can gradually increase over time to be hours or days in the future despite the queue only containing less than 10 threads;
This condition continues until the fair wait queue empties, or one of the threads dies and the rest of the threads are forced to wait for the far in the future timeout on the dead thread to lapse (this creates a dead lock-like situation).
JCache.put(K key, V value) calls RedissonLock.lock() with no lease time, resulting in RedissonLock's creating a 'watchdog' thread that continuously renews the lease on the lock in Redis.
What we've seen is in rare cases this watchdog thread never gets canceled, meaning it continues renewing the lease indefinitely until the instance containing the thread is restarted.
This causes deadlock of all other threads trying to grab the lock and can ultimately bring down an application as threads build up.
Threads using RedissonLock will wait forever for a lock to be released. There is no timeout when waiting for the lock.
I need a lease timeout to prevent deadlock in case the lock aquiring thread is killed or stuck.
I think there's a deadlock between 3 and 4, but I'm not going to try it out.
I can confirm the deadlock in RLock.lock().
It seems the async RedisPubSubConnection.unsubscribe() makes the channel in unsubscribing process can be used unexpectedly by another thread.
If it happens in a heavily concurrent situation, it is possible that the channel of the second thread is unsubscribed soon after the subscription, which leads to the deadlock.
I experience deadlocks at the point the logfile is processed and emptied.
When using the Flyway database migration system with HyperSQL 2.3.4, the attempted application of migrations results in deadlock.
For some context, one of the first things that Flyway does when it starts is open up a connection to the database and lock its metadata table.
Please, find the attachment with the Bamboo report for the Dead Lock around Session and SessionManager.
The synchronized Session.close() waits for the synchronized SessionManager.removeSession(), when the synchronized SessionManager.close() waits for the synchronized Session.close().
I am experiencing deadlock when two different threads attempt to close the database with hsqldb 2.0.0 and 2.1.0.
The issue is that Thread-1 acquires a lock on SessionManager@58a983 when SessionManager.closeAllSession() is called.
Thread-1 is also attempting to invoke Session.close() the active sessions (i.e. Session@e107d9) which requires a lock on the session.
Meanwhile, Thread-2 has already acquired a lock on Session@e107d9 when it called Session.execute().
Thread-2 is also attempting to acquire a lock on SessionManager@58a983 for SessionManager.closeAllSessions, but the lock was already obtained by Thread-1.
I got a deadlock when multiple threads persist lobs concurrently.
One thread got a lock on the database, but have to wait before a synchronized method (LobManager.adjustUsageCount).
The other thread wants the lock too, but is in the synchronized method LobManager.setCharsForNewClob.
I'm using HSQLDB 2.0.0 (in-memory) with Hibernate and am experiencing a deadlock when multiple threads persist lobs concurrently.
Circular dependency between SessionManager and Logger causes thread deadlocks (see attachment).
Hi, I'm getting a deadlock when doing a database shutdown.
The deadlock is easier to reproduce on slower system.
It appears that HsqlTimer$Task first locks the cancel_mutex and then inside HsqlTimer$TaskQueue.signalTaskCancelled, a task queue.
The HsqlTimer.nextTask first locks the queue, and then inside HsqlTimer$Task.isCancelled, the cancel_mutex.
Deadlock occurs when both HttpSession#invalidate() and HttpServletRequest#getAttribute(String) are called at the same time.
Servlet calls HttpSession#getAttribute() in synchronized block. In getAttribute method, session object is locked in order to check foreground session lock.
Moreover, HttpSessionListener#sessionDestroyed() uses synchronized block. HttpSessionListener#sessionDestroyed() is called after locking session object.
And in JDK 1.7_21, you get a deadlock if the LogManager is locked while another thread calls Logger.getLogger.
However, a deadlock can occur due to contention between DB connection pool and OfflineProvisioning.get() whenever a new message (or other mail item) is added.
The call to t.Fatalf exited the current goroutine which was consuming the channel, which caused a deadlock and eventual test timeout ratherthan a proper failure message.
This deadlock is triggered by a unit test DockerSuite.TestPsListContainersFilterHealth.
cleanupDeleted() takes devices.Lock() but does not drop it if there areno deleted devices. Hence docker deadlocks if one is using deferred device deletion feature.
The unmount will fail due to the mount being busy thus causing the timeout and the second rm will then trigger the deadlock.
This is causing a AB-BA deadlock if anyone at the same time tries to do any operation on that device like this:
This is a double RWlock. While holding a read lock, write lock cannot be acquired.
The proxy.connTrackLock.Lock() was not released in a for loop and acquired again in the next loop iteration.
The function daemon.checkpointAndSave(container) lock the locked container again.
The problem is that goroutine A helds container lock and B helds memoryStore lock, then A is blocked on the unlock of memoryStore which is blocked on container lock.
Goroutine A enters a loop, which calls wait in each iteration and does not leave the loop until some condition is satisfied.
This is a double lock bug. The lock for the struct svm has already been locked when calling svm.hotRemoveVHDsAtStart().
If this happens between p.activated = true and p.activateWait.Broadcast(), the first goroutine will not be waked up, leading to deadlock.
Inside podFitsOnNode function, there exists a path, where the podFitsOnNode function acquires ecache.Lock() and returns without invoking ecache.UnLock().
It has been noticed by the go developers that RLock should not be recursively used in the s.Rlock().
Then monitor() fails to close the resetChan because lock is already held by WriteFrame.
This is a double lock bug. s.Logf acquires the s.lock again, which is already held in serveStatus.
The function returns without releasing the lock, leading to deadlock.
Since only run can drain addSimpleTokenCh, the lock is never released.
The basic root cause is that one goroutine finishes with holding a RLock, which will block the second goroutine.
Up() function and Close() function can block each other.
The unit test causing the deadlock is copy-pasted as follows. The hang happens inside the second NewElection.
Basically, there are two goroutines blocking each other, and both of the two goroutines execute select statement.
It was deadlocking waiting for the recvLoop to finish since the stream recv doesn't close until the handler exits.
If the Context was not canceled, it will be blocked forever.
The anonymous goroutine is blocking forever, when context is canceled before the anonymous goroutine writing to “errc” channel.
If rdc.rangeCacheMu.Lock() is executed in between, deadlock will happen.
rdc.rangeCacheMu.RLock() is recursively invoked twice.
There are two goroutine involved in this deadlock.
If the second goroutine interleaves in between the two lock operations of the first goroutine, deadlock will happen.
The first goroutine acquires systemConfigMu.Lock() firstly, then tries to acquire systemConfigMu.RLock(). The second goroutine tries to acquire systemConfigMu.Lock().
