89
Under heavy use on production, Redisson's locks get all locked up, and the application stalls. I'm using Redisson 1.1.5
I have 1 thread locked trying to release a lock:
Also of note, I have about 30 other threads locked awaiting for a lock (a different one from the one used by the previous thread).
I checked the threads with jstack, here is the relevant output:
this key is the one corresponding to the thread blocked trying to release a lock. The other threads, that are waiting for a separate lock are locked even though there is no-one taking up such lock....

100
Currently if redisson client was shutdown abnormally locks created by this client will remain forever.
This solution will create all locks as keys with expiration time set in redis. I'm not sure if this is best solution and what to do with lock() - lock(time, unit) - lock() sequences but it works for me.
Any suggestions or changes are welcome.

101
the redisson is locked!
all Reference variables is locked!!

162
Thread dead lock when using distributed lock on 1.2.1
I run into a thread dead lock problem when using distributed lock on 1.2.1:
public static CryptorInstance getCryptorInstanceFromCluster(Redisson redisson) throws NoAvailableCyrptorException
After a certain period, some threads will be dead lock, and we can tell this by VisualVM's thread dump:
"pool-1-thread-12" #29 prio=5 os_prio=0 tid=0x16050800 nid=0x121c waiting on condition [0x1786f000]

163
Thread dead lock bug when using distributed lock on 1.2.1
I run into a thread dead lock problem when using distributed lock on 1.2.1:
public static CryptorInstance getCryptorInstanceFromCluster(Redisson redisson) throws NoAvailableCyrptorException
After a certain period, some threads will be dead lock, and we can tell this by VisualVM's thread dump:
I don't know why, is it a bug?

253
NPE during load testing
I'm relatively new to java, so sorry if i say something stupid.
When i'm storming my server with 50 concurrent requests using AB, i'm getting NPE during some channel negotiating (as i guess). The following line (CommandDecoder:267):
I'm not 100% sure where does this problem originate from, but i can tell that those requests use Redisson for inter-process locks and fetching/updating two string sets.
I'm using Redisson 2.1.3 against dockerized Redis 3.0.2 (the standard docker library image)
upd: debugger told me that client has received null for hannels.get() for redisson__lock__channel__{lock:pn:phone-number:79219767095}, which, i guess, means trouble with lock.

285
RLock.isLocked() get hung when I disable/enable my local network
I was crashed into a blocking issue when I was doing some configuration on my windows laptop. RLock.isLocked() got hung forever with callstack as following:

345
I use Redisson 2.2 to implement distributed lock in my project and use Redis cluster.  And I found the Redisson client will create new connection and do not use the connection in the pool while invoke some readOnly methods which execute evalRead. So when I start to run the test it will produce lots of TIME_WAIT connection on my server. The followings are some logs on my server.
The codes are very spectacular and I only understand a bit. But I doubt whether the client get the connection from the pool but also create a new one?

436
Read Write lock cannot be correctly unlocked
in ReadLock and WriteLock, the unlock method use thread id to determine whether it is owned by this thread. it works fine for a single machine.
However, when it is used in a distributed processing framework such as MapReduce, the Read and Write lock cannot be correctly unlocked.
I browse the source codes and find that the UUID is generated in the lock, I think it is better to use UUID as lock id istead of thread id or provides a way to let users to set a unque id.

455
Long latency issue while call Redis.
I use Redisson 2.2.10 to implement distributed lock in my project.
And then I run load test for this distributed lock using SINGLE redis mode.
But I found there are many long latency APIs and it's wired that almost all the long latency APIs are a little greater than 1000ms.
The average latency and 99 percent latecy is about 15ms and 30ms respectively.
And then I used jstack to get the call stack and found most of the threads are WAITING at the same place just as the following

486
Rlock performance issue
I am using reddison Rlock with a cluster setup , and sometimes I see latency( up to 1000ms) when trying to acquire the lock or unlock.
I saw this issue opened by zhxjouc (#455) with a similar problem and I am working with 2.2.13 but I am still getting latency when a thread is trying lock a key.
My code is running with Java thread pool for accessing redis, I notice that if I work with pool of size 1-2
almost no latency when getting the lock, but working with 30-50 threads cause the lock delay.
It could be thread overhead issue but I think that 1000ms is too long for that.
Any help on how can I get better performance when locking an unlocking?.
Thanks

491
Dead Locks Happen in lock() Process
when heavy concurrency happens in my application, a few lock requests will "sink" without any responses, even after the lock lease time has passed. All of these requests wait at RedissonLock.lockInterruptibly().The exact position is RedissonLock.get() after RedissonLock.subscribe().
In my opinion, this may be due to a thread removes the netty listener which is used by another thread. It can happen in this way:
Thread A is in the loop of getting the lock after subscription.
Thread B has also applied subscription and waits for result.
Thread A gets the lock very soon and enters RedissonLock.unsubscribe(). In this step, it possibly removes all the listeners on the same channel, which includes the listener used by Thread B. It causes Thread B can never get subscription response and hang on forever.
The similar issue is at [https://github.com//pull/93]. But I think it is not solved completely.
Also I suggest to apply the ttl algorithm to RedissonLock.get() because this step can cost some time. And if it has a timeout, dead lock can be prevented in a work-around way.
This is the thread dump when dead lock happens:

543
RLock trylock blocks forever
Found this in 2.2.16, seems like this was not around in 2.2.10. But still verifying.
When tryLock is called with 0 wait time, the thread blocks forever. I took a thread dump and it looks like the stack pasted below.
A note on the environment: We are running this against an Elasticache cluster in AWS and accessing from 4 EC2 instances. We have seen a lot of command timeouts. I am not sure if that is some way leading to this.
"pool-4-thread-5" #86 prio=5 os_prio=0 tid=0x00007f15ca23f800 nid=0xcd4 waiting for monitor entry [0x00007f15b90e5000]
java.lang.Thread.State: BLOCKED (on object monitor)

545
RedissonLock.isHeldByCurrentThread() doesn't check properly.
I am using RedissonLock in my system.
The lock realted logic in service is like this:
I can see the the thread name is same, so the thread got the lock and the thread try to unlock is same thread.
Is there any problem in my code? Or in the logic of lock.isHeldByCurrentThread() ?

558
loop lock blocked when master-slave failover
redisson version: 2.2.21
redis version: 3.2.1
description: after master-slave failover , the test code blocked forever .
redis cluster :

561
Redisson map get makes the thread waiting forever
The map is a regular <Integer, String> map, and by calling a get on some key we see the following thread trace:

562
attempt to unlock lock, not locked by current thread by node id
Redisson uses expiration of lock object by default to avoid "hanged" locks if you use lock() method without lease time parameter. This expiration is renewal every 30 seconds by internal scheduler, take a look at RedissonLock.scheduleExpirationRenewal(). In your case you're using such lock method at DistributedLock:34

575
A random delay is added when the lock is repeatedly acquired
When a client to acquire the lock failure, the client should retry after a random delay, the reason why the use of random delay is to avoid different client and try again. The results all clients can't get the lock.

624
Incorrect RedissonRedLock.tryLock behaviour
Lock could acquired by several threads at once when leaseTime parameter less than waitTime parameter passed into RedissonRedLock.tryLock

631
RedissonRedLock & RedissonMultiLock lock method stuck
If more than 3 locks are supplied to RedissonRedLock or RedissonMultiLock instance then lock method could stuck

656
RedissonRedLock trylock success while another thread already hold the lock in specific conditions
it's reproducible, my code is below.
redlock lock result will be success, but when I check redis, I found the multi locks belong to different thread.
like:
I did more test and found:
if the single lock is the first of redlock, it's ok.
when I move multilock2 to first position ,it will return false correctly.
the code below
if there are just 2 multi locks, it's ok.
the code below will return false.
I am wondering if it is a bug or not?

683
CommandAsyncService blocks indefinitely
Hi,
I have a thread stuck in the CommandAsyncService#get() indefinitely waiting for the CountdownLatch. I don't have a particular repro case but this happens every once in a while on our servers (under load). Redis itself is still delivering events and the instance receives objects on other threads as well.
Would it make sense instead of waiting indefinitely on the latch to only wait as long as the timeout is configured (as a safe belt) and abort the action if there hasn't been any success/failure by then?
Cheers,

753
Redisson get api  is very slow !!!!
I am using Redisson client with Redis and implemented Redisson as JSR 107 .
I am simply calling a cache through jsp but it is taking a lot of time in CountDownLatch.await().
This wait is common for every single request , so what is the possible reason for this wait ?????
Is there any client side or server side setting effecting it....
Please find Jprofiler snapshot showing Api call trace.
And please find attached code test code.

758
RReadWriteLock is not reentrant
I was expecting the RReadWriteLock to be reentrant, as the wiki page describes it as (emphasis mine):
Redisson distributed reentrant ReadWriteLock object for Java
However, in my simple test:
I see:
The "reentrancy" clause of the ReentrantReadWriteLock Javadoc states:
Additionally, a writer can acquire the read lock, but not vice-versa.
My use of redisson relies on these locks being reentrant (at least within the same Thread), however this does not seem to be the case - is this a bug, or a configuration issue on my end?

763
Problem with Rlock - unlock not releasing lock to waiting threads
I am seeing an intermittent issue with Rlocks. I am implementing what is basically a distributed cyclic barrier using Redisson.
I am running multiple instances of a service on different virtual machines. Each instance receives multiple HTTP REST api requests, handled by multiple threads. As each request is received it waits at a redisson count down latch until the threshold for # of requests has been reached.
At this point, I need one thread to calculate the results and store it in redis so it can be available to all the other threads across all instances. So each thread tries to acquire an Rlock (with TTL of 5 seconds). One thread gets the lock, does the calculation, and stores the result back to redis. It then releases the Rlock.
Now all the other threads that are waiting on the Rlock can (one at a time) acquire the lock, see that the data is already in redis, and simply release the lock and send the result back via HTTP, without repeating the calculation.
We are using a master/slave redis cluster with 1 master and 2 or more slaves. As well as using redisson for the countdown latch and rlock, we use jedis for standard redis reads and writes, but never using the same keys as we use for the latch and locks.
This is all working about 99% of the time. However periodically, I can see that after the Rlock is released, none of the threads that are waiting for it are able to acquire the lock. I was originally using lock(), and finding that HTTP threads were hanging forever. I switched to using trylock, and this returns, but it returns false, indicating a timeout waiting for the lock.
I have added logging to be 100% sure that unlock is being called on the lock in the failure case. Any suggestions on how I can debug this issue, or any known problems with our configuration that might be causing it?
We are running Redisson 3.2.3 and redis 3.2.5
Thanks for the help,
Sue

775
Indefinite lock lost during master failover
I've found my indefinitely held locks will sometimes disappear after a master/slave failover.  The block here will not reschedule a renewal if the attempt of the update fails.
The update can fail during a failover, in which case the lock is gone for good.
The exception that gets thrown when the update fails:

828
URIBuilder seems not to be thread safe
We test this in case of a multi cluster setup and sometimes went the java.net.URL.factory to null instead of the original factory.
Scenario:
Thread 1
replaceURLFactory
currentFactory is original value
start create URL
Thread 2
replaceURLFactory
currentFactory == newFactory -> currentFactory = null
start create URL
Thread 1 and 2
restoreURLFactory
set URL.factory to null
Next “new URL(…)” from different call cause to load new handlers.
Best,
ebersb

889
CommandAsyncService blocks indefinitely
all application threads are blocked with stack trace

891
RReadWriteLock is incompatible with reentry r&w op
While using RReadWriteLock, a bug (or not?) confuse me for a long time.
I described a wrong ops yesterday, and fix it now. Sorry about this.
There is a sure logic problem if ops like this:
write lock A (succeed)
read lock A (succeed)
read unlock A (succeed)
write unlock A (not hold by current thread!!!)
Assuming every unlock op is after a holding check.
So if there is no holding check and running in a concurrent environment, op4 may throw IllegalMonitorException.
But it's ok as follows:
write lock A (succeed)
write lock A again (succeed)
write unlock A (succeed)
write unlock A (succeed)
I found that "read unlock" will delete the whole lock without checking if there is any other write lock.
Is it a bug, or it just shouldn't do and need to do in another way?