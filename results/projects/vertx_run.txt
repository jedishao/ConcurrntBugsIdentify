57
Race condition in clustering
No description provided.

60
Race condition in event bus sending
When two or more event loops attempt to send to the new server concurrently and the connection has not yet been setup.

303
Race condition in deploying modules
If deploy two modules of same name at same time, can result in one not being deployed.

325
HttpClient really not designed for resuse
I have written once before in google groups about the about limitations in the current design of HttpClient given that it was intended to be reused and requests pipelined. I have recently come across another problem that is a serious issue and I think can really only be addressed by redesigning the HttpClient, which is something I'm happy to take on. However, I want to make sure that I have a full understanding of intents and reasons for the initial design, so I would like some peer review of the idea before I go off and do this.
Let's start with the easy problem: HttpClient is supposed to be reusable with pipelined requests being executed on a pool of connections. The first problem is that the client has setter methods, allowing mutation of some of the operating parameters mid use. While by the nature of a verticle only one thread will be using the HttpClient at a time, the same instance may be used in various parts of the code, and since the properties can be mutated, can be left in an unexpected state for the next place that uses it.
A more serious problem is that connection exceptions are reported on the HttpClient exception handler, and NOT on the HttpClientRequest's exception handler. This means either each user of the HttpClient sets it's exception handler (which overwrites the last) OR there is one generic exception handler that can do little more than log the error. The problem with the first case: Imagine two requests are made one after the other, potentially via different code paths. The second's use of the HttpClient overwrites the first's exception handler. Now, since the connections are made on a separate thread in the case of a pool, this means that if a connection exception occurs connecting on the first request, and there is enough delay such that the exception handler has already been set by the second use of HttpClient, the exception for the connection of the first attempt is reported to the second handler, even though the second attempt to connect may succeed. This makes it impossible to know which request actually had the error and act on closing down, cleaning up, reporting accordingly. There is a simple test case provided below. Finally, If the second method is used, where we set the HttpClient's exception handler once, and provide some generic logging implementation, the HttpClientRequest's exception handler is never invoked either. To make matters worse, since there is no timeout feature on HttpClient, this results in a dead end; neither the HttpClientRequest's end nor exception handler is ever invoked. There is no way to know that the HttpClientRequest will never complete.
I propose and would like comment on the following changes:
Deprecate both HttpClient and Vertx.createHttpClient()
Create the interface SharedHttpClient that is immutable, in that is has no setters and no way to set an exception handler. It has an isClosed() method so that a new client can be created if some code path decided to close the Client for some reason. All connection exceptions are passed to the exception handler of the HttpClientRequest, the HttpClient has no exceptionHandler() method. I would create a new interface to preserve backward compatibility, with the idea we would eventually remove the current HttpClient.
Create a new interface/class called HttpClientParams that has all of the setters on the current HttpClient.
Create a new method on Vertx: public SharedHttpClient createSharedHttpClient(HttpClientParams params);
Add a setTimeoutMs(long) method to HttpClientRequest the idea being if the HttpClient.request() (and related) does not complete and call the passed in response handler before the timeout, the exception handler for the request will be called with TimeoutException(). This would default to -1 which means no timeout and has the current behavior.
Add a setTimeoutMs(long) method to HttpClientResponse the idea being if the HttpClientRepsonse.end() (or exceptionHandler()) is not called the timeout, the exception handler for the response will be called with TimeoutException(). This would default to -1 which means no timeout and has the current behavior.
Alternatively to 5 and 6, there is one setTimeoutMs(long) on the SharedHttpClient, and the implementations of the DefaultHttpClientRequest and DefaultHttpClientResponse, handle each half of the timeout.
The implementation of DefaultSharedHttpClient is largely based on the current implementation of DefaultHttpClient, but delegates the connection exceptions to the appropriate HttpClientRequest so that code can know which request failed because of a connection error. It also does not need to implement any of the mutator methods that were present in DefaultHttpClient.
I believe without these changes (and from hours of debugging and trying to work around the current design) the HttpClient is not really reusable, the internal connection pool and boss threads notwithstanding.
Test Case to prove non-reuasbility of HttpClient when shared among code paths:
Results when run:
Got exception during connect for code path Two.
There was an exception on code path Two
You can see the exception handler for the first request was never called, so so it would just hang there, never having the exception handler nor end handler called.

363
Thread context classloader not being set
TCCL is not set in all places in vert.x. It should be.

423
DefaultEventBus not thread safe
I wrote a prototype for our project, I chose vert.x as a middleware which involves consumes messages from kafka and redirecting it to mongoDB, since consumer api in kafka is a blocking api so I create a work verticle, after consuming about 50k messages, I found my cpu utilization was nearly 100% and consuming verticle stuck , I then found it was cause by a infinitive loop in HashMap.put() which was called by getHandlerCloseHook(context).entries.add(new HandlerEntry(address, handler)) in DefaultEventBus.registerHandler().
I think it was caused by concurrently calling the put or remove on the HashMap by a worker verticle, thus causing the HashMap to be broken.
After I change the
It seems everything was ok.
I wondering if it is proper to change the HashSet to ConcurrentHashSet, if so shall I make a pull request?

477
Container.exit() doesn't undeploy modules
While trying to figure out why one of my new worker modules doesn't allow Vert.x to shut down cleanly, I realized that the approach I was taking doesn't work.  Container#exit() only calls VerticleManager#unblock(); it does not undeploy any verticles.  This means that a Verticle's stop() method is never called, and that any non-daemon threads spawned by a worker will keep the JVM from exiting.

646
Rare exception during auto-redeploy
When using the auto-redeploy feature in beta6 (and it is great, really great) I sometime get an exception like this:
INFO: Module dk.bckfnn~mymod~1.0.0-SNAPSHOT has changed, reloading it.
jun 17, 2013 10:06:46 AM io.netty.util.concurrent.SingleThreadEventExecutor$2 run
WARNING: An event executor terminated with non-empty task queue (2)
Exception in thread "vert.x-eventloop-thread-1" jun 17, 2013 10:06:48 AM org.vertx.java.core.logging.impl.JULLogDelegate error
SEVERE: Failed to run task
The exception is rare, much less than 1% of the reloads, so it does not prevent the use of redeploy.
I can not reproduce it but it have occured a couple of times.

682
Synchronize require() in Ruby verticles
When starting multiple instances of the same module/verticle type concurrently, and where the verticle does a require(), this can result in failures because require() in the shared ruby runtime is not threadsafe.
We can fix this by overriding the require() method with a version that synchronizes access to the original version.

663
Problem with shared map
I'm currently debugging an issue I have with a shared map. It might be due to a misunderstanding on my part. If so, I'm sorry for the noise. Below is a minimal test case that demonstrates the problem. In the Putter verticle, I add a new map entry, and in the Getter verticle I
Print the keySet().
Print the result of containsKey() with the inserted key.
But for some reason the two don't agree on the contents of the map. E.g. I get the output:
[gameId: 1, playerId: 4]
false
So I'm guessing there's some concurrency issue at play here.
I ran this test case with:
The last two commands are just to trigger the Putter and Getter to do their jobs, respectively.
Best regards,
Elvis Stansvik
Test case:
Starter.java