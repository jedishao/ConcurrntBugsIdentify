







This can cause deadlock when used from external threads or because the transport thread has changed (for instance a session reconnection with an scaled http server).


We're seeing a deadlock when two different threads load different DataType classes at the same time.


The blocking may also lead to deadlock the EventLoop in cases where a TUC is used.
One scenario where a deadlock might happen is when the EventLoop is not able to completely flush the HEADERS frame on the wire because then Netty would internally create a task to flush the remaining bytes and put this task in its task queue.
In cases when a TUC is being used it might lead to the EventLoop blocking forever (deadlock effectively).

This causes deadlock of all other threads trying to grab the lock and can ultimately bring down an application as threads build up.
I need a lease timeout to prevent deadlock in case the lock aquiring thread is killed or stuck.
I think there's a deadlock between 3 and 4, but I'm not going to try it out.

It was deadlocking waiting for the recvLoop to finish since the stream recv doesn't close until the handler exits.

I'm using HSQLDB 2.0.0 (in-memory) with Hibernate and am experiencing a deadlock when multiple threads persist lobs concurrently.

Hi, I'm getting a deadlock when doing a database shutdown.
The deadlock is easier to reproduce on slower system.
If it happens in a heavily concurrent situation, it is possible that the channel of the second thread is unsubscribed soon after the subscription, which leads to the deadlock.
I experience deadlocks at the point the logfile is processed and emptied.

It happens that when the channel outbound buffer is full, the write operation drains the buffer on a flush changing the channel writability leading to potential deadlock in TUC.
Please, find the attachment with the TUC report for the DeadLock around TUC and TUC.
From my understanding of these stack traces and the threading model, this essentially means that any TUC call (regardless of the thread it is called from) can deadlock with a 'TUC' call.
The function returns without releasing the lock, leading to deadlock.
I am experiencing deadlock when two different threads attempt to close the database with hsqldb 2.0.0 and 2.1.0.


However, a deadlock can occur due to contention between DB connection pool and TUC whenever a new message (or other mail item) is added.



The unmount will fail due to the mount being busy thus causing the timeout and the second rm will then trigger the deadlock.
This is causing a AB-BA deadlock if anyone at the same time tries to do any operation on that device like this:
When using the Flyway database migration system with HyperSQL 2.3.4, the attempted application of migrations results in deadlock.

I got a deadlock when multiple threads persist lobs concurrently.


The Consumer verticle acuires lock1 and gets stuck.
If the Context was not canceled, it will be blocked forever.





It seems to be caused by each thread running TUC on the same two TUC instances, but in a different order.

It's seems like at some time, something has kept the monitor of TUC instance, preventing the resume call to execute the method.

From my simple analysis it seems that the Exception handling and the request response seem to be locking on the TUC and TUC





One such scenario is when you are writing a server in Netty and within that server you want to connect to a grpc service to fetch some data.

That way neither the server nor the client would ever make progress again.


We have experienced several threads locked forever waiting inside calls to TUC in our production system within a few hours after upgrading to Redisson 3.11.4 from Redisson 3.8.0.
To summarize the issue, we see that the timeouts in the TUC sorted set created by TUC can gradually increase over time to be hours or days in the future despite the queue only containing less than 10 threads;
This condition continues until the fair wait queue empties, or one of the threads dies and the rest of the threads are forced to wait for the far in the future timeout on the dead thread to lapse (this creates a dead lock-like situation).

What we've seen is in rare cases this watchdog thread never gets canceled, meaning it continues renewing the lease indefinitely until the instance containing the thread is restarted.

For some context, one of the first things that Flyway does when it starts is open up a connection to the database and lock its metadata table.





One thread got a lock on the database, but have to wait before a TUC method (TUC).



This is a double RWlock. While holding a read lock, write lock cannot be acquired.





It has been noticed by the go developers that RLock should not be recursively used in the TUC.






I ran into a Java deadlock in a large application, and based on the stack trace of the blocked threads.
There is several scenarios how a deadlock could happen.