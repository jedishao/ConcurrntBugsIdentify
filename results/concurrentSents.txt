One thread looked like this:
(the workerExecutor is created by the vertx.createSharedWorkerExecutor)
To investigate, I've copied PipeImpl and ASyncFileImpl and added some trace. (Modified version in attached files with output)
We should avoid holding the lock and restrict the lock scope only for the reading/writing the ConnectionBase intrinsic state (i.e needsFlush and writeInProgress fields).
I have reproduced the same problem using HazelcastClusterManager and IgniteClusterManager.
Currently the SockJSSession uses the transport for write/close operations from any thread.
currently the ServerWebSocket and HttpServerRequest close handlers are called under synchronized lock of the connection.
Also, all locking here is OUTSIDE of our 'acme' code, and inside the vertx codebase.
org.jooq.impl.Val can always be initialized when another thread is concurrently initializing org.jooq.impl.AbstractField.
It does this by calling getPartitionedTopicMetadata and then using .get() to block while waiting for the results

# This can be an issue for code that attempt to call subscribeAsync on a non-blocking Pool (such as when using Netty). The signature of subscribeAsync implies that it's non-blocking. (And it seems somewhat pointless to have subscribeAsync if it blocks anyway)
BinderTransport locking was written under the assumption that calls to IBinder#transact() enqueued the Parcel for delivery to the peer and returned immediately.
However, Android guarantees the unique object identity of IBinder instances within a process.
And so when a client creates a Channel to a Server/Service within its own process, BinderClientTransport.outgoingBinder == BinderServerTransport.outgoingBinder. android.os.Binder#transact() on that object is implemented not as a system call to the binder driver but as a direct call to its own onTransact() method.
OkHttp has the same issue, because OkHttpClientStream.transportDataReceived(), which will call into application code, is called under lock.
Feels like lock expiry notification gets lost and Redisson waits for nothing. P.S. In this regard, would be great to have FairSpinLock not to rely on pubsub.

However, what we started with was a modification of the testTimeoutDrift where we changed the wait time from 500ms to 3s and changed the lock holding time from 30s to 100ms (see Thread.sleep(30000)); with this test case, instead of the tryLock failing due to wait timeout, the threads are able to lock and unlock the lock quickly. The new version of the test, the test fails with a timeout drift into the futre, in a similar way that the test failed in #1104.

The program execution sequence is as followsï¼šThread A locked the 'lockSoundbox' of lockA. It's OK? Thread B locked the 'lock2000' of lockB. It's OK?
Thread A locked the 'lock2000' of lockA. It's OK? Thread B locked the 'lockSoundbox' of lockB. It's OK?
I created a simple JUnit class where 20 threads dooing some simple logic like get lock, bucket, semafore.
And in 10 runs it periodically stucks on RLock.lock() method. I am using latest redisson library 2.2.14.


However, it seems to be easily avoided in this instance by removing line Log.java:430 in addition to the code commented out below it.
There's a second reference to database.sessionManager on 780, but it seems safe.
sync order against BufferedOutputStream and ScriptWriterText are inconsistent; checkpoint causes deadlock:


This occurs regularly if a large number of accounts exists, and could also easily happen with a smaller number. When this occurs, ZD freezes until jetty is restarted.