I ran into a Java deadlock in a large application, and based on the stack trace of the blocked threads, it seems to be caused by each thread running WorkerExecutor.executeBlocking on the same two WorkerExecutor instances, but in a different order. One thread looked like this:
In my case, the deadlock issue is hit when the vertx. close and workerExecutor.close happened to be called at same time, one from main thread and another from eventloop thread. (the workerExecutor is created by the vertx.createSharedWorkerExecutor)
POSTing many buffers to an echoing server gives thread blocked failures when we use drainHandler+writeQueueFull approach.
To investigate, I've copied PipeImpl and ASyncFileImpl and added some trace. (Modified version in attached files with output)
It's seems like at some time, something has kept the monitor of ASyncFileImpl instance, preventing the resume call to execute the method.
The current implementation of ConnectionBase#writeToChannel synchronises on the ConnectionBase (which is necessary) and holds this lock when calling ChannelHandlerContext#write or ChannelHandlerContext#writeAndFlush. It happens that when the channel outbound buffer is full, the write operation drains the buffer on a flush changing the channel writability leading to potential deadlock in Vert.x.
We should avoid holding the lock and restrict the lock scope only for the reading/writing the ConnectionBase intrinsic state (i.e needsFlush and writeInProgress fields).
The Consumer verticle acuires lock1 and gets stuck. I have reproduced the same problem using HazelcastClusterManager and IgniteClusterManager.
Currently the SockJSSession uses the transport for write/close operations from any thread. This can cause deadlocks when used from external threads or because the transport thread has changed (for instance a session reconnection with an scaled http server).
currently the ServerWebSocket and HttpServerRequest close handlers are called under synchronized lock of the connection. This can create deadlocks in SockJSSocket because a SockJSSocket may uses different request/websocket (reconnects) and event loops.
Also, all locking here is OUTSIDE of our 'acme' code, and inside the vertx codebase. From my understanding of these stack traces and the threading model, this essentially means that any SockJSSession.write() call (regardless of the thread it is called from) can deadlock with a 'ServerConnection.handleClose()' call.
From my simple analysis it seems that the Exception handling and the request response seem to be locking on the HttpClientRequestImpl->handleException->getlock -> (this) and HttpClientRequestImpl->connect(this)->lamda->(this)
If you call Message.getValue() on the completion of a CompletableFuture returned by Pulsar Client, like Reader.readNextAsync() then there is a good chance to create a deadlock.
org.jooq.impl.Val can always be initialized when another thread is concurrently initializing org.jooq.impl.AbstractField.
SimpleLoadManagerImpl.updateRanking should not cause deadlocks
It does this by calling getPartitionedTopicMetadata and then using .get() to block while waiting for the results
This can be an issue for code that attempt to call subscribeAsync on a non-blocking Pool (such as when using Netty). The signature of subscribeAsync implies that it's non-blocking. (And it seems somewhat pointless to have subscribeAsync if it blocks anyway)
We're seeing a deadlock when two different threads load different DataType classes at the same time. It looks like loading one DataType class causes the others to be loaded as well during class initialization. But, this can cause a deadlock when multiple threads are trying to load these DataType classes because they each hold a lock on the class and end up waiting for the others to finish.
BinderTransport locking was written under the assumption that calls to IBinder#transact() enqueued the Parcel for delivery to the peer and returned immediately. However, Android guarantees the unique object identity of IBinder instances within a process. And so when a client creates a Channel to a Server/Service within its own process, BinderClientTransport.outgoingBinder == BinderServerTransport.outgoingBinder. android.os.Binder#transact() on that object is implemented not as a system call to the binder driver but as a direct call to its own onTransact() method.
This is a problem because BinderTransport#handleTransaction() holds its 'this' lock while calling outgoingBinder.transact() in multiple places. If two peer instances of BinderClientTransport and BinderServerTransport are running handleTransaction() on different threads at the same time, they can each end up holding their own lock while waiting (forever) for the other's.
My requirement is similar to #6638 and #5856, but on JVM, not Android. I want to retry a bidi streaming call with a new auth token if the server responds with a particular status. In both of the aforementioned tickets, an interceptor option was discussed, but this comment also mentions using the builtin retry with ClientStreamTracer that was never elaborated on. I'd like to get some details on using the builtin retry with ClientStreamTracer option.
ServerImpl.start() calls NettyServer.start() while holding ServerImpl.lock. NettyServer.start() awaits a submitted runnable in eventloop. However, this pending runnable may never be executed because the eventloop might be executing some other task, , like ServerListenerImpl.transportCreated(), that is trying to acquire ServerImpl.lock causing a deadlock.
OkHttp has the same issue, because OkHttpClientStream.transportDataReceived(), which will call into application code, is called under lock.
There is a chance of deadlock when DelayedClientTransport is linked with an InProcessTransport. 
Methods of ClientTransport.Listener and ServerTransportListener are usually called under a lock. The biggest reason for locking is to guarantee the ordering of multiple methods on the same listener. However, these listeners usually call into channel layer code, and may in turn acquire locks from there, which forms a transport lock -> channel lock lock order. On the other hand, when channel layer calls into transport layer, it's possible to form a channel lock -> transport lock lock order, which makes deadlock possible.
NettyClientTransport#newStream is currently a blocking operation. It blocks until the HEADERS frame has been written on the wire. This is behavior is not what people who use our asynchronous API would come to expect.
The blocking may also lead to deadlocking the EventLoop in cases where a DirectExecutor is used. One scenario where a deadlock might happen is when the EventLoop is not able to completely flush the HEADERS frame on the wire because then Netty would internally create a task to flush the remaining bytes and put this task in its task queue. This task can never be completed though as the EventLoop Thread is blocked by our very own newStream method waiting for the task to be completed
In cases when a DirectExecutor is being used it might lead to the EventLoop blocking forever (deadlock effectively). There is several scenarios how a deadlock could happen. One such scenario is when you are writing a server in Netty and within that server you want to connect to a grpc service to fetch some data. If you now use a DirectExecutor and reuse the EventLoop of the server with the grpc client, the TLS handshake would block the server's EventLoop, which is also the very EventLoop responsible for completing the TLS HandShake. That way neither the server nor the client would ever make progress again.
Sometimes, even though the lock from the first thread is timed out and released automatically, the redisson_lock_queue and redisson_lock_timeout keys are stuck and the second thread can't acquire the lock, even though nobody is holding it. The second thread is stuck for full acquisition timeout. Then, it fails to get the lock eventually, and keys redisson_lock_queue and redisson_lock_timeout disappear, making the lock available again. Feels like lock expiry notification gets lost and Redisson waits for nothing. P.S. In this regard, would be great to have FairSpinLock not to rely on pubsub.
We have experienced several threads locked forever waiting inside calls to RedissonLock.unlock() in our production system within a few hours after upgrading to Redisson 3.11.4 from Redisson 3.8.0.
To summarize the issue, we see that the timeouts in the redisson_lock_timeout sorted set created by RedissonFairLock can gradually increase over time to be hours or days in the future despite the queue only containing less than 10 threads; this condition continues until the fair wait queue empties, or one of the threads dies and the rest of the threads are forced to wait for the far in the future timeout on the dead thread to lapse (this creates a dead lock-like situation).
I'll provide a PR for this issue to follow with additional test cases added to RedissonFairLockTest. However, what we started with was a modification of the testTimeoutDrift where we changed the wait time from 500ms to 3s and changed the lock holding time from 30s to 100ms (see Thread.sleep(30000)); with this test case, instead of the tryLock failing due to wait timeout, the threads are able to lock and unlock the lock quickly. The new version of the test, the test fails with a timeout drift into the futre, in a similar way that the test failed in #1104.
JCache.put(K key, V value) calls RedissonLock.lock() with no lease time, resulting in RedissonLock's creating a 'watchdog' thread that continuously renews the lease on the lock in Redis. We are using the default 'lockWatchdogTimeout' of 30 seconds so this thread runs every 10 seconds. What we've seen is in rare cases this watchdog thread never gets canceled, meaning it continues renewing the lease indefinitely until the instance containing the thread is restarted. This causes deadlock of all other threads trying to grab the lock and can ultimately bring down an application as threads build up.
Threads using RedissonLock will wait forever for a lock to be released. There is no timeout when waiting for the lock.
I need a lease timeout to prevent deadlock in case the lock aquiring thread is killed or stuck.
The program execution sequence is as followsï¼šThread A locked the 'lockSoundbox' of lockA. It's OK? Thread B locked the 'lock2000' of lockB. It's OK? Thread A locked the 'lock2000' of lockA. It's OK? Thread B locked the 'lockSoundbox' of lockB. It's OK? I think there's a deadlock between 3 and 4, but I'm not going to try it out.
I can confirm the deadlock in RLock.lock(). I created a simple JUnit class where 20 threads dooing some simple logic like get lock, bucket, semafore. And in 10 runs it periodically stucks on RLock.lock() method. I am using latest redisson library 2.2.14.
It seems the async RedisPubSubConnection.unsubscribe() makes the channel in unsubscribing process can be used unexpectedly by another thread. If it happens in a heavily concurrent situation, it is possible that the channel of the second thread is unsubscribed soon after the subscription, which leads to the deadlock.

I use HSQLDB for an image caching system. The images themselves are stored on disk but an image statistics (last access, usages etc) and location on disk are stored in HSQLDB. I experience deadlocks at the point the logfile is processed and emptied.
When using the Flyway database migration system with HyperSQL 2.3.4, the attempted application of migrations results in deadlock.
For some context, one of the first things that Flyway does when it starts is open up a connection to the database and lock its metadata table. The migrations are applied in a separate connection. The code pasted above gets run a few method calls below the executeCompiledStatement method in Session.java (this method is what is trying to execute my first migration).
Please, find the attachment with the Bamboo report for the Dead Lock around Session and SessionManager. The synchronized Session.close() waits for the synchronized SessionManager.removeSession(), when the synchronized SessionManager.close() waits for the synchronized Session.close().
I am experiencing deadlock when two different threads attempt to close the database with hsqldb 2.0.0 and 2.1.0. The issue is that Thread-1 acquires a lock on SessionManager@58a983 when SessionManager.closeAllSession() is called. Thread-1 is also attempting to invoke Session.close() the active sessions (i.e. Session@e107d9) which requires a lock on the session. Meanwhile, Thread-2 has already acquired a lock on Session@e107d9 when it called Session.execute(). Thread-2 is also attempting to acquire a lock on SessionManager@58a983 for SessionManager.closeAllSessions, but the lock was already obtained by Thread-1. I have posted a more concise summary and the relevant thread dumps below.
sync order against BufferedOutputStream and ScriptWriterText are inconsistent; checkpoint causes deadlock:
I got a deadlock when multiple threads persist lobs concurrently. One thread got a lock on the database, but have to wait before a synchronized method (LobManager.adjustUsageCount). The other thread wants the lock too, but is in the synchronized method LobManager.setCharsForNewClob.
I'm using HSQLDB 2.0.0 (in-memory) with Hibernate and am experiencing a deadlock when multiple threads persist lobs concurrently. I've replicated the problem in a very simple test case, and it happens immediately. See below for stack traces from deadlocked threads.
Circular dependency between SessionManager and Logger causes thread deadlocks (see attachment). However, it seems to be easily avoided in this instance by removing line Log.java:430 in addition to the code commented out below it. There's a second reference to database.sessionManager on 780, but it seems safe.
Hi, I'm getting a deadlock when doing a database shutdown. The deadlock is easier to reproduce on slower system. It appears that HsqlTimer$Task first locks the cancel_mutex and then inside HsqlTimer$TaskQueue.signalTaskCancelled, a task queue. The HsqlTimer.nextTask first locks the queue, and then inside HsqlTimer$Task.isCancelled, the cancel_mutex.

Dead lock occurs when both HttpSession#invalidate() and HttpServletRequest#getAttribute(String) are called at the same time. Servlet calls HttpSession#getAttribute() in synchronized block. In getAttribute method, session object is locked in order to check foreground session lock. Moreover, HttpSessionListener#sessionDestroyed() uses synchronized block. HttpSessionListener#sessionDestroyed() is called after locking session object.
The java.util.logging code does not guarantee that. In fact, it doesn't guarantee anything about what happens if you lock the LogManager object. And in JDK 1.7_21, you get a deadlock if the LogManager is locked while another thread calls Logger.getLogger.
After the fix for bug 50423, it is possible to add dozens of accounts to the system. However, a deadlock can occur due to contention between DB connection pool and OfflineProvisioning.get() whenever a new message (or other mail item) is added. This occurs regularly if a large number of accounts exists, and could also easily happen with a smaller number. When this occurs, ZD freezes until jetty is restarted.