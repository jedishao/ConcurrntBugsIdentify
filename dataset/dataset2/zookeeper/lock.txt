Need to replace Jute with supported code.
ZooKeeper currently uses Jute to serialize objects to put on the wire and on disk. We pulled Jute out of Hadoop and added a C binding. Both versions of Jute have evolved (although Hadoop still doesn't have a C binding). It would be nice to use a more standard serialization library. Some options include Thrift or Google's protocol buffers.Our main requirements would be Java and C bindings and good performance. (For example, serializing to XML would give us incredibly bad performance and would not be acceptible!)
Adding no disk persistence option in zookeeper.
Its been seen that some folks would like to use zookeeper for very fine grained locking. Also, in there use case they are fine with loosing all old zookeeper state if they reboot zookeeper or zookeeper goes down. The use case is more of a runtime locking wherein forgetting the state of locks is acceptable in case of a zookeeper reboot. Not logging to disk allows high throughput on and low latency on the writes to zookeeper. This would be a configuration option to set (ofcourse the default would be logging to disk).
perl Net::ZooKeeper segfaults when setting a watcher on get_children.
The issue I'm seeing seems strikingly similar to this: I have one writer process which adds sequenced children nodes to /queue and a separate reader process which sets a children watcher on /queue, waiting for children to be added or deleted. Long story short, every time a child node is added or deleted by the writer, the reader's watcher is supposed to trigger so the reader can check if it's time to get to work or go back to bed. Bad things seem to happen while the reader is waiting on the watcher and the writer adds or deletes a node.In versions prior to 3.3.2, my code that sets a watcher on the children of a node using the perl binding would either lock up when trying to retrieve the children or would segfault when a child node was added while waiting on the watch. In 3.3.2, it seems to just do the locking up.I'm seeing this: assertion botched (free()ed/realloc()ed-away memory was overwritten?): !(MallocCfgI managed to get a stack traceProgram received signal SIGABRT, Aborted.The code to reproduce: while (1) { my @children = $zkc-gt;get_children($root, watch=gt;$child_watch); else  print "Time to wait for the Children.\n";\n";\n"; }
zkServer.sh is duplicated in a couple of places
we have zkServer.sh in 4 different places. some differ only in whitespace. it would be much more maintainable if there were only one version that everything used.
zookeeper client close operation may block indefinitely, Moving the hang issue from specifically (from James): "I'm thinking the close() method should not wait() forever on the disconnect packet, just a closeTimeout length - say a few seconds. Afterall blocking and forcing a reconnect just to redeliver the disconnect packet seems a bit silly - when the server has to deal with clients which just have their sockets fail anyway"
Offers a node design for interacting with the Java Zookeeper client. Following up on my conversations with Patrick and Mahadev (This patch includes the implementation as well as unit tests. The first unit test gives a simple high level demo of using the node API.The current implementation is simple and is only what I need withe current project I am working on. However, I am very open to any and all suggestions for improvement.This is a proposal to support a simplified node (or File) like API into a Zookeeper tree, by wrapping the Zookeeper Java client. It is similar to Java's File API design.Although, I'm trying to make it easier in a few spots. For example, deleting a Node recursively is done by default. I also lean toward resolving Exceptions "under the hood" when it seems appropriate. For example, if you ask a Node if it exists, and its parent doesn't even exist, you just get a false back (rather than a nasty Exception).As for watches and ephemeral nodes, my current work does not need these things so I currently have no handling of them. But if potential users of the "Node a.k.a. File" design want these things, I'd be open to supporting them as reasonable.
FLE implementation should be improved to use non-blocking sockets. From earlier email exchanges:a) The first problem is in manager.toSend(). This invokes connectOne(), which does a blocking connect. While testing, I changed the code so that connectOne() starts a new thread called AsyncConnct(). AsyncConnect.run() does a socketChannel.connect(). After starting AsyncConnect, connectOne starts a timer. connectOne continues with normal operations if the connection is established before the timer expires, otherwise, when the timer expires it interrupts AsyncConnect() thread and returns. In this way, I can have an upper bound on the amount of time we need to wait for connect to succeed. Of course, this was a quick fix for my testing. Ideally, we should use Selector to do non-blocking connects/accepts. I am planning to do that later once we at least have a quick fix for the problem and consensus from others for the real fix (this problem is big blocker for us). Note that it is OK to do blocking IO in SenderWorker and RecvWorker threads since they block IO to the respective peer.b) The blocking IO problem is not just restricted to connectOne(), but also in receiveConnection(). The Listener thread calls receiveConnection() for each incoming connection request. receiveConnection does blocking IO to get peer's info (s.read(msgBuffer)). Worse, it invokes connectOne() back to the peer that had sent the connection request. All of this is happening from the Listener. In short, if a peer fails after initiating a connection, the Listener thread won't be able to accept connections from other peers, because it would be stuck in read() or connetOne(). Also the code has an inherent cycle. initiateConnection() and receiveConnection() will have to be very carefully synchronized otherwise, we could run into deadlocks. This code is going to be difficult to maintain/modify.Also see:
Slow add_auth calls with multi-threaded client. Calls to add_auth are a bit slow from the c client library. The auth callback typically takes multiple seconds to fire. I instrumented the java, c binding, and python binding with a few log statements to find out where the slowness was occuring ( i'm attaching a script that takes 13s or 1.6s for the auth callback depending on the session time out value (which in turn figures into the calculation of the poll timeout).
Sudden crash of all nodes in the cluster. In a 3 nodes ensemble, suddenly all the nodes seem to fail, displaying "ZooKeeper is not running" messages.This a request to understand what happened and probably to improve the logs when it does.See logs below:NODE1:#8211; no log for several days before this #8211;NODE2:#8211; no log for several days before this #8211;NODE3 (leader):#8211; no log for several days before this #8211;
Race condition in CMI leads to hang.
Using ZK 3.5.4, zookeeper_close() occasionally hangs with a backtrace of the form:At which point the zhandle_t struct appears to have already been freed, as it contains garbage:There appears to be a race condition in the following code:As api_epilog() may free zh in between zookeeper_close() setting zh-gt;close_requested=1 and incrementing the reference count.The following patch should fix the problem:
client xid overflow is not handled
Both Java and C clients use signed 32-bit int as XIDs. XIDs are assumed to be non-negative, and zookeeper uses some negative values as special XIDs (e.g. -2 for ping, -4 for auth). However, neither Java nor C client ensures the XIDs it generates are non-negative, and the server doesn't reject negative XIDs.Pat had some suggestions on how to fix this:--Michi
A zk instance can not be connected for ZooKeeperServer is not running
In our 5 node zk cluster, we found a zk node always can not be connected. From the stack we found the ZooKeeperServer hung at waiting the server to be running. But the node is running normally and synced with the leader.ZooKeeperServer stackAny suggestions about this problem? Thanks.
Install could not be done on powerpc Error: Unrecognized opcode: `lock'.
after (ant compile_jute) and(cd src/c/ ./configure), make can not be done!I don't know how to fix it
Server fails to join quorum when a peer is unreachable (5 ZK server setup).
In a 5-node ZK cluster setup, in the following state:The 4th server is not able to rejoin the quorum because the connection to the host that is not established, and apparently takes to long to timeout.Stack traces and additional information coming.
Lock recipe sorts sequenced children incorrectly
The lock recipe sorts sequenced children lexicographically. When the sequence number wraps, a lexicographical comparison will always place 2147483648 ahead of -2147483649, place -2147483648 ahead of -2147483649, and place -1 ahead of -2. Clearly, we want 2147483648 lt; -2147483649, -2147483649 lt; -2147483648, and -2 placed ahead of -1, since those sequence numbers were generated in that order.I suggest that the sequence numbers be converted to unsigned numbers before being compared in the comparison functor that gets passed to qsort().This leaves us with another issue. When comparing unsigned sequence numbers, there is a slim chance that 4294967296 lt; 0. So, I suggest that a fudge range be used, say, the number of nodes in the quorum * some fudge factor, in order to handle this comparison.Please close this if I'm way off base here.
CMI equality assertion failed.
Jenkins (Hudson) shows an error when running test-cppunit. I am not able to replicate this error on my own build machine, so I am unable to diagnose. Perhaps someone with access to the Apache Jenkins. Please see attached output from
Add a "getChildrenWithStat" operation
It would be extremely useful to be able to have a "getChildrenWithStat" method. This method would behave exactly the same as getChildren but in addition to returning the list of all child znode names it would also return a Stat for each child. I'm sure there are quite a few use cases for this but it could save a lot of extra reads for my application.
Multi Op Watch Events
Caches built on top of zookeeper clients can become inconsistent because of lack of multi op watches. Our clients receive watch notifications for paths one at a time, and in their watch handling, invalidate the path in the cache. However, the cache now has an inconsistent view of zookeeper, since it is receiving the notifications one at a time. In general, the watch handling semantics do not conform with the idea of a multi op. If changes can be made to multiple paths atomically, all clients should be notified of that change atomically.
hang in CMI
With patch , last_send = , last_ping = , next_deadline = , recv_timeout = 30000, input_buffer = 0x0, to_process = {head = 0x0, last = 0x0, lock = {__m_reserved = 0,, last_zxid = 82057372, outstanding_sync = 0, primer_buffer = , primer_storage = ,, active_node_watchers = 0x6901520,
Namespace partitioning in ZK.
Tracking JIRA for namespace partitioning in ZK From the mailing list (- courtesy: Mahadev / Flavio ) , discussion during Jan 2010 - "Hi, Mahadev said it all, we have been thinking about it for a while, but
Data cleanup Eviction policy.
We are using zookeeper to store versions of business objects in order to achieve coherence, distributed locks, etc. These business objects have limited lifespans (i.e. objects created over a week ago are rarely accessed), so effectively, after some time period, we do not need their information in zookeeper anymore. It would be nice to have a built-in tool or mechanism for expiring old data, much like how PurgeTxnLog cleans the snapshot and transaction log files. Any thoughts on whether this can be supported or how it can be accomplished? Currently we are walking the tree and deleting nodes with an old mtime.
let org.apache.zookeeper.recipes.lock.WriteLock implements java.util.concurrent.locks.Lock
and add a zookeeper distributed java.util.concurrent.locks.ReadWriteLock
Add getFirstChild API.
When building the distributed queue for my tutorial blog post, it was pointed out to me that there's a serious inefficiency here. Informally, the items in the queue are created as sequential nodes. For a 'dequeue' call, all items are retrieved and sorted by name by the client in order to find the name of the next item to try and take. This costs O( n ) bandwidth and O(n.log n) sorting time - per dequeue call! Clearly this doesn't scale very well. If the servers were able to maintain a data structure that allowed them to efficiently retrieve the children of a node in order of the zxid that created them this would make successful dequeue operations O( 1 ) at the cost of O( n ) memory on the server (to maintain, e.g. a singly-linked list as a queue). This is a win if it is generally true that clients only want the first child in creation order, rather than the whole set. We could expose this to the client via this API: getFirstChild(handle, path, name_buffer, watcher) which would have much the same semantics as getChildren, but only return one znode name. Sequential nodes would still allow the ordering of znodes to be made explicitly available to the client in one RPC should it need it. Although: since this ordering would now be available cheaply for every set of children, it's not completely clear that there would be that many use cases left for sequential nodes if this API was augmented with a getChildrenInCreationOrder call. However, that's for a different discussion. A halfway-house alternative with more flexibility is to add an 'order' parameter to getFirstChild and have the server compute the first child according to the requested order (creation time, update time, lexicographical order). This saves bandwidth at the expense of increased server load, although servers can be implemented to spend memory on pre-computing commonly requested orders. I am only in favour of this approach if servers maintain a data-structure for every possible order, and then the memory implications need careful consideration.
Chroot is an attribute of ClientCnxn.
It would be better to have one process that uses ZooKeeper for different things
TaskManagement Using Zookeeper Recipe.
A typical use case in distributed system is " There are T tasks and P processes running but only T processes must be active always [ P gt; T ] and remaining P-T processes acting as stand by and be ready to take up a Task with one or more active processes fail". Zookeeper provides an excellent service which can be used to co ordinate among P processes and using the mechanism of locking we can ensure that there is always T processes active. Without a central co-ordinating service generally there will be 2T processes[ i.e atleast one back up for each process]. With Zookeeper we can decide P based on the failure rate.The assumption here areThis was developed for a different project S4 which is also open sourced
Document process for client recipe contributions.
Per Doug's suggestion I'll use a link instead of copy/paste:
provide an option for the WriteLock to also watch the locks own znode, so that if someone else deletes it then it is equivalent to calling WriteLock.unlock()
Most clients probably wont need this, but it could be a handy system management feature to allow the WriteLock to watch its own znode so that if someone else deletes it, it then relinquishes the lock and tries to get it back again
implement java.util.concurrent.locks.Lock.
we should implement the lock.
unnecesssarily complex reentrant zookeeper_close() logic.
While working on a wrapper for the C API I puzzled over the problem of how to determine when the multi-threaded adaptor's IO and completion threads had exited. Looking at the code in api_epilog() and adaptor_finish() it seemed clear that any thread could be the "last one out the door", and whichever was last would "turn out the lights" by calling zookeeper_close().However, on further examination I found that in fact, the close_requested flag guards entry to zookeeper_close() in api_epilog(), and close_requested can only be set non-zero within zookeeper_close(). Thus, only the user's main thread can invoke zookeeper_close() and kick off the shutdown process. When that happens, zookeeper_close() then invokes adaptor_finish() and returns ZOK immediately afterward.Since adaptor_finish() is only called in this one context, it means all the code in that function to check pthread_self() and call pthread_detach() if the current thread is the IO or completion thread is redundant. The adaptor_finish() function always signals and then waits to join with the IO and completion threads because it can only be called by the user's main thread.After joining with the two internal threads, adaptor_finish() calls api_epilog(), which might seem like a trivial final action. However, this is actually where all the work gets done, because in this one case, api_epilog() sees a non-zero close_requested flag value and invokes zookeeper_close(). Note that zookeeper_close() is already on the stack; this is a re-entrant invocation.This time around, zookeeper_close() skips the call to adaptor_finish() #8211; assuming the reference count has been properly decremented to zero! #8211; and does the actual final cleanup steps, including deallocating the zh structure. Fortunately, none of the callers on the stack (api_epilog(), adaptor_finish(), and the first zookeeper_close()) touches zh after this.This all works OK, and in particular, the fact that I can be certain that the IO and completion threads have exited after zookeeper_close() returns is great. So too is the fact that those threads can't invoke zookeeper_close() without my knowing about it.However, the actual mechanics of the shutdown seem unnecessarily complex. I'd be worried a bit about a new maintainer looking at adaptor_finish() and reasonably concluding that it can be called by any thread, including the IO and completion ones. Or thinking that the zh handle can still be used after that innocuous-looking call to adaptor_finish() in zookeeper_close() #8211; the one that actually causes all the work to be done and the handle to be deallocated!I'll attach a patch which I think simplifies the code a bit and makes the shutdown mechanics a little more clear, and might prevent unintentional errors in the future.
Bug in WriteLock recipe implementation?Not sure, but there seem to be two issues in the example WriteLock:(1) ZNodeName is sorted according to session ID first, and then according to znode sequence number. This might cause starvation as lower session IDs always get priority. WriteLock is not thread-safe in the first place, so having session ID involved in compare operation does not seem to make sense.(2) if findPrefixInChildren finds previous ID, it should add dir in front of the ID
Concurrent primitives library - shared lock. I create this jira to add sharedock function. The function follows recipes at
Findbugs/ClientCnxn: Bug type JLM_JSR166_UTILCONCURRENT_MONITORENTERJLM Synchronization performed on java.util.concurrent.LinkedBlockingQueue in org.apache.zookeeper.ClientCnxn$EventThread.queuePacket(ClientCnxn$Packet)Bug type JLM_JSR166_UTILCONCURRENT_MONITORENTER (click for details)Bug type JLM_JSR166_UTILCONCURRENT_MONITORENTER (click for details)The respective code:409 public void queuePacket(Packet packet) {415 } else 418 }423  else 435 if (wasKilled)441 }
Coding error in lock. In fuction child_floor(), strcmp() is used to compare the whole string.
improve zkpython synchronous api implementation.Improves the following items in zkpython which are related to the Zookeeper synchronous API:I will attach the patch shortly.
C API makes it difficult to implement a timed wait_until_connected method correctly.
When using the C API, one might feel inclined to create a zookeeper_wait_until_connected method which waits for some amount for a connected state event to occur. The code might look like the following (didn't actually compile this)://------int zookeeper_wait_until_connected(zhandle_t* zk, const struct timespec* timeout) pthread_mutex_lock(amp;kConnectedMutex); pthread_cond_timedwait(amp;kConnectedCondvar, amp;kConnectedMutex, amp;abstime);void zookeeper_session_callback(zhandle_t* zh, int type, int state, const char* path, void* arg)}That would work fine (assuming I didn't screw anything up), except that pthread_cond_timedwait can spuriously wakeup, making you not actually wait the desired timeout. The solution to this is to loop until the condition is met, which might look like the following://---//---That would work fine, except the state might be valid and connecting, yet not ZOO_CONNECTING_STATE or ZOO_ASSOCIATING_STATE, it might be 0 or, as implemented recently courtesy of zookeeper-1108, 999. Checking for those states causes your code to rely upon an implementation detail of zookeeper, a problem highlighted by that implementation detail changing recently. Is there any way this behavior can become documented (via a ZOO_INITIALIZED_STATE or something like that), or is there any way this behavior can be supported by the library itself?
Add Ruby bindings.Add Ruby bindings to the ZooKeeper distribution.Ruby presents special threading difficulties for asynchronous ZK calls (aget, watchers, etc). It looks like the simplest workaround is to patch the ZK C API.Proposed approach will be described in comment.Please use this ticket for discussion and suggestions.