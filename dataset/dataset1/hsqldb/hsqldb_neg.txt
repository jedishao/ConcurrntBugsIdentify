1
Error in Build
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
When i try to execute the script build (build the jar
archive) i can't because the javac give me the
following error :
I can't find the class org.hsql.Map (used in class
jdbcconnect)
My version of JVM is 1.1.8
Please send me the correct source
Best Regards
Joao Luis
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

2
Please Support multiple ResultSets!
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Currently, the jdbcStatement class does NOT support
multiple ResultSets. (This is clearly documented in
the javadocs, and is also obvious from looking at the
source code.)
However, this makes this class USELESS for executing
general stored procedures (which OFTEN do mutiple
queries -- doing complex stuff like this is usually
the whole point behind using stored procedures).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

3
bug in the getMoreResults() method
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The getMoreResults() method of org.hsql.jdbcStatement
class fails to follow the Statement interface specs
exactly. This causes problems with code that should
work.
The basic problem is that getMoreResults() fails to
advance the current ResultSet. (It also fails to close
the current ResultSet, which is also a violation of
the spec.)
Since HypersonicSQL currently does not support
multiple ResultSets, what SHOULD happen if the specs
are to be exactly followed is that the next ResultSet
should be advanced to null after closing the initial
one when getMoreResults() is called.
Below is the current (and buggy) implementation of
getMoreResults():

To illustrate the current bug, and also to test that
the above patch actually solves the probelm, consider
the following code which works with any properly
written JDBC Statement implementation (but fails in
HypersonicSQL):
From the Javadoc on Statment:
The above code fails with updates (but not queries)
because the loop never ends. This happens because
fails to return -1 the second time that it is called.
If the above patch to getMoreResults()
were in place, then getUpdateCount() WOULD recognize
that the current RS is null and then would return -1.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

4
DatabaseMetaData.getExportedKeys broken
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am writting a java program that analizes the
relations among the different tables in the database.
When I create a table using FOREIGN KEY, Hypersonic
classes don't write to the SYSTEM_EXPORTEDKEYS table.
Then, when I call to the getExportedKeys method, I
obtain no data.
Thanks.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

5
DatabaseMetaData.getExportedKeys() inop
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am writting a java program that analizes the
relations among the different tables in the database.
When I create a table using FOREIGN KEY, Hypersonic
classes don't write to the SYSTEM_EXPORTEDKEYS table.
Then, when I call to the getExportedKeys method, I
obtain no data.
Thanks.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

6
LONGVARBINARY error using setObject
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
using hsql version 1.42
When using setObject to set the value of a binary
column a class cast error occurs.
I have made the following changes that solved the
problem:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

7
Fix for &amp;amp;quot;SELECT x, SUM(y)&amp;amp;quot; &amp;amp;amp; &amp;amp;quot;GROUP BY x
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following sql does not work correctly:
It returns a single null row.
The bug is in Select.java, getResults() method
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

8
Fix for &amp;amp;quot;SELECT x, SUM(y)&amp;amp;quot; &amp;amp;amp; &amp;amp;quot;GROUP BY x
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following sql does not work correctly:
It returns a single null row.
The bug is in Select.java, getResults() method
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

9
Code transition and standards
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
On 2001-04-04 nobody@nowhere screamed in frustrationa
and said:
I'd like to supply some fixes we made to HSQL over
the time. However, we need to be able to reconcile
those changes with whatever you did to HSQL.
This is _impossible_ to do, since all files changed
due to your formatting changes. There's no way to find
out what changed between 1.43 and 1.60RC2.
Do you have any kind of change history documenting the
changes you made?
I can't even just grab the newest sources - I added
some testing framework to HSQL that I'd hate to lose.
Is there any way to merge stuff back?
This is an ongoing issue and will be cross-posted as
an open bug as we transition our customers to the new
code base.
For the future of this project, NOTHING IS MORE
IMPORTANT THAN THIS! At least until we get the CVS
tree in place. Mark has offered to do this on an
individual basis but we need two things from the
developers:
a) someone to volunteer to be trained by Mark to do
this for customers/users (could be - SHOULD BE -
multiple someones (ideally 3))
b) someone to ride herd on this in the future (again,
should be more than one) and assist me in this
specifically, keeping me aware and honest - THIS KIND
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

10
No docs on DB transition
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Need to add documentation on transition issues from
other RDBMS's, the strengths and weaknesses of HSQLDB
in various use-cases and other new-user documentation
to next point release
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

11
PreparedStatement Timestamps
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I'm using hsqldb-v1.6 for unit testing my code, and I've lately run into trouble
with java.sql.Timestamp in PreparedStatement. Our code will be using PreparedStatement
all the time, and there will be lots of timestamp columns, so I really hope this works out.
The problem is basically that I create a PreparedStatement from a hsqldb driver connection,
then call setTimestamp(int,Timestamp) on it, and when I call executeUpdate, I get the following
error:
It appears from the error message, as well as from the hsqldb source, that the Timestamp
object gets converted into a string (&quot;2000-01-01 05:00:00.0&quot;), and then the stringified SQL is
processed by the hsqldb engine. But apparently it doesn't parse the string. I'm guessing it's the
nanoseconds, because in the self-tests included in hsqldb (which make good examples), there are
things like '2000-02-29 10:00:00', but nothing with nanoseconds. I don't really need nanoseconds,
it's always going to be 0, but I at least need JDBC compliance: if I pass a Timestamp value, the
driver must support it.
Please let me know what the status of this is. I'll gladly post this elsewhere on the SourceForge
site if there's a more appropriate place, and I'll also send source code if it helps.
Thanks.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

12
single quote in statements
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If string field contains single quote it throws
exception !
Can anyone here help me patching this problem ?
I am using HypersonicSQL which is being closed i guess.
TIA
Nitin
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

13
Infinite loop while executing DELETE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a sample tiny hsqldb 1.60 database on which the following statement:
causes an infinite loop in method TableFilter.findFirst.
The database uses tables created with the CACHED modifier (leaving out this modifier seems to fix the problem, but is not an acceptable work-around).
As the database is taken from a closed-source project, I would prefer not to upload the file publicly. Please e-mail me for the test data + code which demonstrates the bug (jpljpl@gmx.de)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

14
not null fails on primary key
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I believe without looking this is ansi 89 to have
even though the not null is implied by being a primary
key. It is acceptable by oracle 8i, MS SQL Server, MS
Access, and hsql 1.43.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

15
canonizing column names
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I can understand why you may take this shortcut to
capitalize all column names internally to the
database, but it sucks if you want to use reflection
to instantiate records off of the database.
You may not consider this a bug since oracle does
this, but you won't find it in hsql 1.43, mysql, ms
sql, or access.
The mysql database appears to rely on the application
programmer being case sensitive in all cases instead
of hashing mixed and upper case names. This is the
other extreme, but I still prefer it to just
capitalizing everything.
Please reconsider.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

16
#16 bit types only take 'true' and 'false'
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
You should be able to change a bit value by setting it
to 0 or 1. Don't know if this is ansi sql or not, but
it is accepted in oracle, ms sql, ms access, mysql.
I can only change bit values by setting it to 'true'
or 'false'.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

17
#17 followups don't work on sourceforge
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
btw, ms sql server does not take 'true' and 'false' in
update statements for bit types. And oracle does not
have a bit type, closest type was smallint.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

18
#18 now() getting parsed as timestamp
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am using the now() function to put timestamps on records. I have an insert that has a value for each column in the record, no nulls that works just fine, and includes one formatted date, and one call to now().
Later I do an update with a record that has many nulls, so the columns do not appear, and two calls to the now() function. For some reason it is dropping
Sounds like alternate paths through the sql parser..
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

19
#19 problem with 'SHUTDOWN COMPACT'
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm running HSQLDB v1.60 and am having no luck getting
the &quot;SHUTDOWN COMPACT&quot; query working. No matter what I
do, the JVM never exits properly if I use this
query... my application is always left &quot;hanging open&quot;
at the end. I finally stripped the code down to it's
bare bones... please take a look at the following code
and let me know what's wrong.
It doesn't matter whether the 'file.script' database
is brand new, has tables in it, is 'script mode'
or 'cached mode'... the above app never exits. Notice
the commented out 'normal' SHUTDOWN line... it works
properly when used.
It appears that the 'SHUTDOWN COMPACT' query executes
properly and that no exception is thrown. Any ideas?
I also tried the &quot;CHECKPOINT&quot; query, which seems like
the way to go, but it didn't compact the database like
I thought it would... actually, I don't know what it
did. I ran &quot;CHECKPOINT&quot; on a cached database with
deleted rows that were still &quot;hanging around&quot; after
deletion... but the rows were still around after
the &quot;CHECKPOINT&quot; was issued and the database was
closed. It didn't seem to do anything. Is 'CHECKPOINT'
also a problem?
I'm running VA Java Enterprise, patched to v3.5.3.
under Win2k. I imported the HSQLDB v1.60 java files
(not the class files) into it's own VAJ project. This
HSQLDB project is the only thing included in
the 'project path' of the above class. The workspace
classpath is empty. The code doesn't work properly
under the IDE, nor does the VAJ compiled version work
properly from the command line under SUN JDK v1.3.1.
Thanks.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

20
#20 SA: User not found
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
Occassionally, I get this exception saying that SA
is not a valid user(something along those lines).
Since, sa is the default user and I haven't created
any other users, the only choice I have while
accessing this database is to drop and recreate the
whole database since it doesnt let me login. Any
suggestions? Any one reported this problem before?
Thanks for the great product.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

21
#21 Subselect fails when using functions
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Consider following situation:
This fails reporting:
No progress when trying to use test._id in all
places... It seems it doesn;'t find the column for
the &quot;select max(_id) from test&quot; part.
&quot;select max(_id) from test&quot; alone works GREAT!
Hope I'll see this fixed soon.
returns the last inserted row :(
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

22
#22 Insert statements misbehave in this case
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
No exception is thrown when I do an insert using the
wrong statement:
Create table varchar_test(id varchar(100) null, tstamp
bigint null);
insert into varchar_test values(tstamp);
Values are silently inserted into id column and tstamp
is 0.
regards,
Xtrimity
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

23
#23 like &amp;quot;text%&amp;quot; does not use index
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
in hsqldb 1.6 the like statement does not use the
index so queries take a lot of time. It would be good
if statements:
would use the index.
For the first one there is a workaround which can be
used:
instead of:
..which is with index 100 times faster!!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

24
#24 issues with storing objects
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I am not very sure if the following is bug or not.
I have attached a java file. Please execute it and u
will find the bug or error. I will try to explain the
four cases of my program
My motive is to store a string as an object type
of the hsql database.
Case 1:
1.) create table temp (name varchar(25), data object)
2.) stored a string in first column of table and
hashtable in the second column.
3.) works fine
Case 2:
1.) create table temp (name varchar(25), data varchar
(20))
2.) store string in first column of table and store
another string using setString() of preparedstatement
class.
3.) works fine.
Case 3:
1.) create table temp (name varchar(25), data object)
2.) store string in first column of table and store
another string using setString() of preparedstatement
class.
3.) Behaviour not understood. (Please see output to
understand)
Case 4:
1.) create table temp (name varchar(25), data object)
2.) store string in first column of table and store
another string using setObject() of preparedstatement
class.
3.) I get an error.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

25
#25 multiply doesn't seem to work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I'm migrating my MySQL application to use HSQLDB. I
already fixed most of the problems but one still
remains. I try to multiply values in a simple SQL
statement 'select *,(count*price) from purchase'. This
statement however throws me :
or This function is not supported: 12 in statement
or a NumberFormatException
Anyway, keep up the VERY good work guys
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

26
#26 Several Date/Time Functions Fail
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Most of the date/time methods -- year, month,
dayofyear, dayofweek, hour, minute, and second -- fail
with java.lang.NumberFormatException when used with a
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

27
#27 Long.MIN_VALUE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a row of type BIGINT and I submit a SQL
statement putting Long.toString(Long.MIN_VALUE) into
that row.
Then the following exception happens.
I suspect, that the Tokenizer steals the values sign
to apply it after parsing, so that
java.lang.Long.parseLong ends up parsing the negative
of Long.MIN_VALUE, which is just out of range for long
(Long.MAX_VALUE+1).
Sorry, I have no idea, where to look for the version.
I just have the hsql.jar here, and ther is nor version in the MANIFEST
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

28
#28 Double.NaN
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We use a row of type DOUBLE and put in the value
Double.NaN, via the JDBC interface. This results in an
exception complaining that NaN is not a valid row
name. We think that Double.NaN should mapped to SQL
value NULL and vice versa.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

29
#29 ResultSet incomplete - lying ?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
i was verry happy when I found HSQLDB, looked through
the documentation and was even more happy.
I tested it and requested a Updateable, srollable
Result Set and got no error.
Then I tried to used the promided functionality and
got something like &quot;Sorry not implemented yet!&quot; for
features like lastRow(), firstRow() or setRow().
In my opinion that's essential for a scrollable Result
Set and it felt like lying to me when HSQLDB excepted
the request for an Srollable ResultSet and didn't
provide the elementary functionality.
First I thought I'm doing something wrong but after a
while I looked into the Source code and all the
functions where Documented with what they should do
but implemented as a simple Return of &quot;Not Yet !&quot;.
I took a deeper look and I don't know how to implement
the functionality without reading a lot of the sources.
So the question is:
When will the implementation of ResultSet keep the
Prommise to be Scrollable maybe Updateable ?
It would make things a lot easyier for me ... .
Sugestion: Till then you should shange the Code so
that it gives an error if someone is requesting a
ResultSet with not implemented Features (Scrolable,
Updateable). That's the way other Databases handle
that ... .
You should mention it in the Docu to, that would have
saved me a lot of time ;-).
Anyway: Good work, seams to be quiet complete and the
docu is really good (Ok, not always if it comes to the
Javadocs ...).
Knut Pape
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

30
#30 Droping tables drops the database
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The sequence of the database CREATE TABLE statements
determine the order of INSERT statatements. This is
dependant on Referential Integrity within the created
tables.
I have a table which is in the heart of the database
(holds the core PK) and desire to add an additional
column at a later date (which has arrived), the
existing DB has been populated so the procedure would
be to:
1. create a tmp table that includes the new column
2. do an INSERT SELECT to copy the existing data over
to the tmp table, but adding the default value for the
new column at the same time
3. DROP the old table
4. CREATE a new table with the old table name but with
the additional column
5. do an INSERT SELECT to copy the tmp data over to
the new table
6. DROP the tmp table
Right, now the bug - when re-creating the new table
(point 4) the statement is inserted at the end of all
the other CREATE TABLE statments; this means that the
And because of that sequence HSQL DB thinks the DB is
currupt because the INSERT statements prior to newly
inserted ones a refering to something that does not
come into existances until it reaches the bottom of
the script. The result is a NullPointerException.
Do you have a way to re-order the script file while it
is in memory?
You help is much appreciated.
Keith
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

31
#31 [TIMESTAMP]second function
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
fields and throws an IllegalArgumentException from
TIME fields.
Try:
fix for bug 446415 else return NumberFormatException)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

32
#32 Error in the jdbc documentation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In the 'operating modes' documentation page (/hsqldb_v.1.61/doc/internet/hSql.html), the URL to connect to the hsql server in &quot;Server&quot; mode is written as
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

33
#33 TINYINT: ClassCastException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I ran into a problem when converting a mySQL databse
to hsqldb with the Transfer tool. The mySQL database
has columns of type TINYINT, which cause a
ClassCastException. I tracked down the problem to
(what appears to me) an inconsistency between the
jdbcPreparedStatement class and the Column class. The
Column class uses a Short for both a TINYINT and
SMALLINT, while the jdbcPreparedStatement class uses a
Byte for TINYINT and a Short for SMALLINT. The
jdbcPreparedStatement.setObjectIntype(..) then
receives a Short where it expects a Byte, causing the
ClassCastException.
For now, I patched the jdbcPreparedStatement to also
use a Short for both TINYINT and SMALLINT, however I
am not sure if this is the right procedure. I could
also patch the Column class to use a Byte for TINYINT.
If anyone could give me advice on the way to go, I
will submit a patch.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

34
#34 Problem with CONSTRAINT PRIMARY KEY
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have been testing the 1.6.1 version of the HSQL
database and have come across a problem. The steps to
replicate the problem are:
1. Run the DBManager and select a database URL
something like &quot;jdbc:hsqldb:message&quot;
2. Execute the following SQL statements in order:
3. Check the inserted record:
4. Close the DBManager
5. Open the DBManager again selecting the same JDBC
URL as before
6. At this point, a stack trace should appear in the
DBManager window, complaining about a string index out
of bounds exception.
7. Check for the previously inserted record:
SELECT * FROM MESSAGE
No records will be returned from this query.
I have found a work-around, which is to specify a
primary key using the syntax
instead of using the CONSTRAINT PRIMARY KEY syntax.
I assume that what I have found is actually 2 bugs,
because if you repeat the process and create the
message table only, no stack trace appears, but the
previously inserted data is still lost.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

35
#35 Minor typo in build.bat
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Build.bat contains following line:
It should be:
This may sound like a minor detail, but it caused me
some gray hair as the (Finnish?) saying goes... :)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

36
#36 Java 1.1 error in Log.java
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Log.java contains a non-conditional call of
setProperty function, which doesn't compile on JDK
Is should be:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

37
#37 Recovery commits broken transactions
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When HSQL restarted after a system crash, it will made
the changes made by the last *uncommited* -- thus
possibly inclompete -- transaction permanent. HSQL
should drop all changes after the last succesfull
commit from the log (providing that auto commit was
set to off).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

38
#38 SOUNDEX
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
Soundex function is not complaint with Soundex Algorithem.
According to the Algorithem,
The vowels are not used.
If two or more adjacent (not separated by a vowel) letters have the same numeric value, only one
is used.
But in your implementation it is using same numeric value.
If there are not three digits after the consonants are convert, the code is filled out with zeros. The
name Lee has no consonants after the L, so the soundex code would be L000.
But in your implementatiion it will give only &quot;L&quot;, basically is not padding with Zero's.
It can be easily fixed. I modified the soundex function and attached to it. you can check in to
the repository.
you are doing good work.
Regards
Aravilli Srinivasa Rao
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

39
#39 Soundex
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
i,
Soundex function is not complaint with Soundex Algorithem.
According to the Algorithem,
The vowels are not used.
If two or more adjacent (not separated by a vowel) letters have the same numeric value, only one
is used.
But in your implementation it is using same numeric value.
If there are not three digits after the consonants are convert, the code is filled out with zeros.
The
name Lee has no consonants after the L, so the soundex code would be L000.
But in your implementatiion it will give only &quot;L&quot;, basically is not padding with Zero's.
It can be easily fixed. I modified the soundex function and attached to it. you can check in to
the repository.
Regards
Aravilli Srinivasa Rao
I had problem with attachment i.e why i am attaching code function here)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

40
#40 JDBC URL for server documented incorrect
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In the 1.61 release, the file doc/internet/hSql.html
that the URL for a Server connection is:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

41
#41 Group by
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I have a table with two column that pile data (serAccess, month). i am triyng to count the number of serAccess by month by doing the following SQL command:
When i run it using the demo application it run ok but
when i run from my JSP application, i only get 1,0 as
result!
Please help!
Nicaud Bourgault
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

42
#42 GROUP BY in 1.6.1 still broken
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following forum thread indicates that the GROUP BY
with aggregate functions bug was fixed:
Based on the results of some of my queries, it looks
like it's still not working correctly.
My db table is as follows:
The query I'm running is:
What appears to be happening is that the grouping
logic is done last in the order of events, but done
first in mysql.
Has anyone else noticed this problem in the 1.6.1
codebase?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

43
#43 Applet is incompatible with Java Plug In
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I just got this mail from a contractor of Sun:
Dear Thomas,
We are about to release a new version of Java Plug In
which supports all Java applets, including those based
on older versions of Java. We expect this version to
be widely distributed and to replace all older
versions of Java. After extensive testing of existing
applets, we have done our best to fix all
compatibility issues. However, we have found the
following problem in running your applet, which cannot
easily be fixed on our end. Whenever available, we
suggest a solution that will allow your applet to run
with this new version of Java Plug-in.
You can find the early-access version of Java Plug-in
at
in order for you to do your own testing.
Thank you,
The Java Plug-in Team
Sun Microsystems
URL
Problem
access denied
Suggested solution
Broken because of security model changes in Java 2.
Migrate the applets to Java 2 security model.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

44
#44 GROUP BY FAILS WITH ALIAS
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Version 143
GROUP BY on a column select alias fails but works if
the actual column name is used (order by works using
an alias).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

45
#45 Connection is broken for objects &amp;gt; 32k
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After creating a prepared statement and using it's setObject() method with an Object (size&gt; 32k) I receive a &quot;SQLException: Connection is broken&quot; error
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

46
#46 poor INSERT INTO implementation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When processing:
will select all 1,000,000 rows from the existing table
named TEST, create a new table called TEST, insert the
1,000,000 rows in the new table, and only then attempt
to link the table into the database, at which point the
operation will fail due to the restriction on duplicate
table names that Database.linkTable() imposes.
With a simple change to the code, that is: attempting
to link the new table into the database immediately
after it is created and before the selection result is
retriveived, the example statement will return
immediately with a &quot;TABLE ALREADY EXISTS&quot; SQL
exception, saving much time and overhead.
-- Will be provding patch, including support for
equivalent CREATE TABLE AS syntax extention.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

47
#47 StringIndexOutOfBoundsException: -10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Running Standalone mode from JBoss I get this from
time to time. After this has occurred the database
needs to be deleted to avoid hanging on further use.
How do you properly shutdown in standalone mode? I
assume that the DB file are being left corrupted by
not closing down properly with a shutdown hook?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

48
#48 Self Referencing Table bug
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Initial Comment:
This create table statement should be legal. I need to
create a hierarchical set of entities. As a workaround
I suppose I could modify the table after creation but
this shouldn't be necessary! Is this possible even?
How do I add a foreign key as an afterthought?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

49
#49 supporting &amp;quot;default&amp;quot; in create table?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
in 1.6.0, the following sql command fails because of
possible to support?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

50
#50 LIMIT limitations
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In 1.61 LIMIT does not work together with DISTINCT.
Has now been fixed.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

51
#51 SELECT .. IN from same table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In 1.61 queries in the form of SELECT .... FROM atable
work, reporting &quot;Column y not found&quot;. Has now been
fixed, pending more thorough testing.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

52
#52 Java MIN VALUE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I get a NumberFormatException when trying to insert a
Java Long.MIN_VALUE into the normal sql mapping column.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

53
#53 join/condition bug
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
following problem:
with the hsql sample data (Database manager), execute
the following statement:
the problem arises if you add another condition like
(OR 1=0) which should not change anything, but
unfortunatly the statement
will now product a different resultset with less
recordsets (exactly all records without NULL values in
addressid)
This definitly seems like a bug, or?
Any ideas?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

54
#54 Extraneous quote corrupts database
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If you include an extraneous single quote (') in an
insert or update statement value, the entire database
is corrupted. Every record is overwritten with the
value up to the quote.
Every record is set to 'It isn'.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

55
#55 Server crash after logging DISCONNECT
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I use hsqldb 1.61
In the situation where the dbengine is running as a
server, I open a connection via jdbc and close it.
At that moment, the database engine logs a DISCONNECT
statement in the script file.
If I close the database at that point and restart it,
I get a NullPointerException on the screen:
And the server won't ever respond again normally to
connections.
This crash can be recovered manually by removing the
DISCONNECT statement from the .script file.
Of course that will be a big problem in production
environments. :)
For myself I have solved the problem in Log.java
by adjusting the
void write(Channel c,String s) throws SQLException {
method, so that it doesn't write a DISCONNECT to the
script file.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

56
#56 double column names are allowed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Due to an error in my create table script I created a
table with 2 columns with the same name.
I don't know what the effects of this are, but I think
it should not be allowed.
Only imagine select .. from .. where a=x
with a the double column name
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

57
#57 combined conditions don't work correctly
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Okay, here is an example:
Create some tables (I stripped off everything you do
not need to see the problem).
Now insert some test data.
or
there is no result!
If you use just one of the conditions
or
it works. If you don't create the foreign key FK_USER
or the UNIQUE clause, everything is just fine.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

58
#58 Version 1.62 patch breaks selftest
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The suggested patches for version 1.62 includes a
patch for bug 471711 by fredt. This patch generate a
column not found error when running the selftest
program.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

59
#59 EOFException executing UPDATE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
(HSQLDB v1.61)
I was running an application making lots of inserts,
selects and updates and got the following stack dump.
I know this is not a lot to go on, but I have only seen
it once. This happened while executing a query like
the following:
The number of ids in the query varies.
I will post again if I get any more information.
Thanks,
Stack dump:
at
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

60
#60 Inserting Objects
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm having problems storing java.lang.Integer and
java.lang.String objects in hsqldb 1.60.
Here is the table I have created:
I am inserting Objects using a PreparedStatment. All
other objects I insert (user defined objects that is)
seem to work fine. No exceptions are being thrown
while inserting. When I open the database in
DatabaseManager, The preference name has been written,
but the serialized object field is blank.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

61
#61 Null Pointer thrown from jdbcResultSet
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The method getBinaryStream() throws a NullPointer
exception if the data in the database is null. This
is because it tries instantiate a ByteArrayInputStream
with a null value (the result from getBytes())
This should be changed so that the method retuns null
and does not throw the exception.
See line 438 of jdbcResultSet.
Maybe something like this...
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

62
NPE on connect.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
v1.61
We got the following stack trace once while our
application was trying to connect to HSQL.
We only saw this once, so I was not able to narrow it
down at all.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

64
Unprotected system tables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Here's a good chuckle for anyone who's interested:
database (you don't even have to be an admin user for
this)
Ha, ha, ha...wimper... (:-(
Here's another good one:
Hey! Where did test go? I just created it, but it
doesn't show up in the treeeeeeee.
The fix is simple:
hsql(db) needs to disable creation of user tables with
the same names as system tables. A really naive
implementation only needs to do a
getSystemTable(create_table_name) != null to check
this, although it is a waste to do it that way.
BTW
inserts, updates, deletes are also allowed against
system tables, but since the tables are regenerated on
each use, the DML is completely without observablke
effect (but does consume processing time). These
actions should be disabled too, if not too expensive...
Campbell
PS
yes: I know that nobody with even a little hsqldb
knowlege is going to do this (on purpose or
otherwise). But what about the case where someone is
migrating to hsqldb, they are fairly green, and their
previous naming conventions were *unfortunate*? What
about automated environments, where software chooses
the names for table creation?
PPS
I've known about this one for ~6 months now (basically,
since I first downloaded the product and took my first
skim through the source). Sorry about not posting
earlier
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

65
 #65 Exception while shutdown compact
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello we use HSQL in a production system.
The usage is very heavy msg traffic.
When we try to shutdown immediately from the Admin
console
We observed the following exception:
Then the server does not quit.
We have to hit ctrl C and then edit the properties
file to modified = NO and restart the server to
run.
How does one restore from a previous backup file.
Thank you
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

66
#66 problem inserting data &gt;30k
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a problem when I want to insert data &gt;30kb into
a longvarbinary field. For inserting I use a
preparedStatement and the setBytes() method. There is
no error code, the program &quot;stops&quot; only...
The error occurs when hsql runs in &quot;server&quot;-mode. I
tried the same insert of large data with the hsql
standalone version and it works!
Please help me, if you know a solution or hint.
Peter
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

67
#67 NumberFormatException DECIMAL
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Is this a bug ?
I am using hsqldb_v.1.61
Boerries
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

68
#68 commit does not work with multiple con.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We are not sure if this is a limitation of hsqldb or a bug. Please clarify:
We use 3 connections:
If we reread the TABLE after finishing this test
program, the new Row is not in the TABLE anymore!
If we do one of the following changes, all works as
Can you please clarify this issue?
Thanks a lot!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

69
#69 foreign key problem
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Suppose:
table B has a foreign key on table A field nr 1 (both
tables are cached)
If table A is dropped, it is still possible do to selects in table B, but when the database is stopped and restarted table B is deleted.
In my opinion here is something wrong. I have 3 options :
1. It should not be possible to drop table A. At the moment it is not possible to delete records in table A which are referenced in table B.
2. If you can drop table A, table B should be impacted
immediatly, not after a restart of the database.
3. In my opinion the best would be to put table B in a
mode not accessible to the users, (but it should not be
deleted, as it happens now), until table A is recreated
with the correct foreign keys. If table A is recreated
table b should be again accessible for select/update by
the users.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

70
#70 {fn hour ()} Returns 0 through 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The hour function returns zero through eleven, instead
of zero through twenty-three, because the hour method
in Library.java used Calendar.HOUR instead of
Calendar.HOUR_OF_DAY. I've made this mistake many
times myself. Source fix:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

71
#71 {fn month()} returns 0 through 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The month function returns zero through eleven instead
of one through twelve. It's always seemed odd to me
that the Calendar.MONTH returns zero through eleven
instead of one through twelve. Here's a source fix to
Library.java:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

72
#72 Memory problems with cached tables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We've been using HSQL for a little while for internal
testing and such. However, we've had some trouble
loading large quantities of data to cached tables.
Since the HSQL code uses integer values to perform its
seeks into the data file, theoretically you should be
able to store a gig or more of data. In practice,
however, it seems that all the data you load remains
in memory, even when using cached tables.
I've attached a Java class written by one of my co-
workers that demonstrates this. No matter how large
or small the row size, it always craps out after
consuming the available memory. It appears that the
Cache class is supposed to maintain a limited number
of rows in memory and persist the rest to disk, but I
don't think it really performs that function. I've
tried changing the value governing the size of the
cache (even setting it as low as 20 rows), but the
amount of memory consumed remains the same. In
addition, running an analyzer reveals that Row objects
are never garbage collected.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

73
#73 Shutdown
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Im trying to use HSQL. I've configured it and it seems to meet my needs in all but one respect.
I'd like to create a JRun-style admin tool which allows the user to start and stop the database
server at the click of a button. I've done this with McKoi, another Java database.
The problem I'm having with HSQL is restarting the database server after the user has shut it
down. The SQL &quot;Shutdown&quot; command appears to mean something other than shutdown for HSQL.
After issuing the shutdown command on it, attempting to restart it causes it to issue the
following...
... and kill whatever process it's in.
I'm no expert in this area but I think this is possibly an indication that the server port is still in use.
I think it might mean that the database never fully shuts down. This hurts me in the admin tool
scenario because I'm running the database server as a thread within the admin tool process. When
the above exception is thrown the entire admin tool process is killed. The user just sees his admin
tool disappear. The exception is flagged, the user hits OK, the database dies and says I'm going to
kill everybody near me. AAAAAAAAARRRRRRRRRRRGGGGGGGGHHHHH!!!!!!!!!
Any thoughts on this would be greatly appreciated.
Thanks.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

74
#74 *must* fix logging w.r.t. object names
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It has come to my attention that there are several
outstanding issues w.r.t. case preservation,
extraneous quotes corupting the log, etc.
My latest experiment shows that yet another case is not handled correctly
Say one issues the statement:
which is perfectly legal and reults in a table named:
then the statement is logged as:
When the database is restarted, all statments
pretaining to this table, of course, fail.
My suggestion is that (if we continue with the current
log format) we should double-quote all object names in
the log and escape internal double-quotes in the
standard manner by doubling them, as in:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

75
#75 StringIndexOutOfBound Exception
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
I got the following Exception with HSQL v1.61:
To reproduce the exception:
1- Launch database Manager and execute all the CREATE
SQL commands to create the database
2 - Quit Database Manager
3 - Launch Database Manager again and connect to the
database just created.
Could you help me ?
Thanks a lot.
Rene Levantinh
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

76
#76 SHUTDOWN fails under JVM 1.4.0. beta 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Issuing a SHUTDOWN command to the database does not
shut it down under version 1.4.0 beta 3 of the JVM.
The server just hangs in some sort of limbo state
where the server is not accepting queries but is not
actually shut down either. SHUTDOWN works fine under
JVMs 1.3.0 and 1.3.1.
Sometimes, SHUTDOWN will work after repeated attempts
to shutdown/connect to the server from a client.
When this eventually happens, (if run with &quot;-silent
false&quot;) the message &quot;The database is shutdown&quot; is
issued once for each time you attempted to shutdown
the server.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

77
#77 ABS function unknown
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
unknown function: ABS
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

78
#78 right join throws SQLException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
SEE:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

79
#79 Parser error in nested query
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
version : HSQLdb1.61
The parser has some problems with nested queries that
are combined via 'where in'
try the following :
Gives as result :
SQL
Error
and
work both fine.
By the way,
I could work around the problem by rewriting the query.
Alexander
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

80
#80 Compatability Issues in J2SDK1.4.0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
assert is now a keyword within Java2 v1.4.0 and so
the program will no longer compile.
Thanks
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

81
#81 Updates not being applied
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I send an SQL Update command using executeUpdate
(), I do not get a JDBC error, but the updates are not
applied. I am issing a commit after the SQL Update.
If close the JDBC connection, or issue a CHECKPOINT,
the update is applied.
Any ideas?
Thanks,
Dave Connerth
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

82
#82 Literals in SELECT, not enough results
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Version: 1.61
I'm trying to insert data using a INSERT INTO ...
SELECT ... GROUP BY ... statement. As part of the
statement, I need literal values. If the literals are
placed before a SUM() expression, I only get a single
row returned. If after the expression, I get how every
many the SUM() returns. MSSQL returns the same number
in either case.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

83
#83 GROUP BY returns row of NULLs
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using a INSERT INTO ... SELECT ... GROUP BY ...
statement. Getting 'Cannot insert NULL' when SELECT
should return nothing. It appears that the GROUP BY
clause causes the SELECT return a row of NULLs when
nothing is found.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

84
#84 Index names not local to table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This problem did not exist in 1.61!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

85
#85 probs with CodeSwitcher
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I tried to run CodeSwitcher to switch to JDK11 on (I am not sure whether it is still allowed to use CodeSwitcher. I havent installed Ants yet.)
I got the following errors:
And there is a double //#endif JAVA2 in line 1263 of
Regards,
Ulrich
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

86
#86 multiple null values violate unique cons
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
As far as I know, NULL values should not be considered
when checking if an insert satisfies the unique
constraints defined on a table. For instance:
The following should be allowed:
However, with hsqldb (v 1.61), the second insert fails
because of the unique constraint on y.
Will this be corrected in a future version?
Kind regards,
Tijmen
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

87
#87 NullPointerException in getBinaryStream
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I try to access a column that has been set to NULL with getBinaryStream, I get a NullPointerException. Obviously HSQLDB tries to create a ByteArrayInputStream
from a null-Pointer.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

88
#88 compile errors with J2SDK 1.4.0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
with hsqldb 1.61 and J2SDK 1.4.0, I did the following
steps:
Ant version 1.4.1 compiled on October 11 2001
Same error messages appear when I did in advance
ant switchtojdk12
Regards,
Ulrich
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

89
#89 no compile for JDK11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I tried with hsqldb 1.61 and JDK 1.3
ant switchtojdk11
and got the following error messages:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

90
#90 setProperty() in Log.java
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
just as a reminder:
There is still the quick hack for version 1.61 in the
code of 1.70 at line 182: setProperty() which is not
defined for JDK 1.1
Regards,
Ulrich
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

91
#91 IF EXISTS in hsqldb 1.70
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Trying to submit
leads to
or without a defined table foo to
Regards,
Ulrich
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

92
#92 SUM() + GROUP BY give unwanted NULL rows
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Similar to previously raised bug re: GROUP returning a single row of NULL(s) when its corresponding SELECT returns nothing.
Although 1.7 RC 2 fixes that bug, the following causes
the same problem:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

93
#93 Transfer corrupted / Error Code: -19 / State:40001
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
v1.7 RC1 RC2 both have the following bug:
Operating in client/server mode on localhost
After executing the select statement
I get an exception:
You can reproduce the error by executing the table
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

94
#94 delete problem
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
i tried to use HSQLDB 1.7.0..
there was a delete problem.
when i restart the HSQLDB Server after i delete rows,
all the rows which have the same data were deleted.
for example,
there were rows like below: but when i reatsrted the server ,
the rows were changed like below: two rows were deleted which have the same data.
colud you please consider of this problem...?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

95
#95 V1.70RC3: build error with jdk 1.1.8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I downloaded v1.70RC3 and found the new build.xml great(thanx to Fred).
The build runs without any problems with
But, I got an error when building for/with JDK 1.1.8
Complete build sequence is [init, javaversion, prepare,
Regards,
Ulrich
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

96
#96 GROUP BY with ORDER BY
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
after inserting the test data in DatabaseManager, I
submitted the from position, product
But, the result set is NOT ordered by name.
Regards,
Ulrich
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

97
#97 Int &amp; Double problem
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Version: hsqldb_v.1.61.
Here is script
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

98
#98 ASC / DESC  with DISTINCT does not work.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The ASC / DESC functions when used with DISTINCT have no effect.
To reproduce..
This bug seems to have been present for quite some
time, and hopefully will be fixed with 1.7.0.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

99
#99 server.properties file not accessed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
server.properties file is not loaded.
I downloaded 1.7 release candidate to work on long var
strings. Database would not read our current script
file because the server.properties file was being ignored.
Recommend modifying Server.java to load properties
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

100
Win2000 - Multi-byte char. issue
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HSQL latest stable version is garbling the multi-byte
characters in the following scenario.
Platform:
This is reproducible always and is happening
continuosly. Please contact me at
sanjayag@india.hp.com for more details.
Thanks
Sanjay
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

101
#101 jdbcConnection.getAutoCommit broken
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
jdbcConnection.getAutoCommit fails to take into
account SET AUTOCOMMIT statments, reflecting only the
last value passed to jdbcConnection.setAutoCommit.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

102
#102 Wrong update decomposition in script
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I recently installed the hsqldb 1.7.0RC3. After some time of testing/using it I found the following bug (?):
In some cases (couldn't find out under which circumstances exactly) when hsqldb writes the UPDATE command into the script file and decomposes it into
DELETE, INSERT sequence, it distorts the WHERE clause.
This happens in the following way: it compares the
correct column, but with incorrect value, ie. with
value of the first updated column (or the first column
in the table definition?). As the value is of
different type, the next time hsqldb is started, it
throws the &quot;java.sql.SQLException: Unexpected token:
37000 Unexpected token: 08&quot; exception.
Here is some example:
This command is called upon hsqldb server (hsqldb
console printout):
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

103
#103 Connection reset by peer
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In client/server mode of RC4:
When my client closes the connection (and then
terminates) I get the following exception on the
server side console:
This is new in RC4, the previous RC's did not have
this trace.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

104
#104 bug in reading LONGVARCHAR
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
JavaDocs in jdbcResultSet.getAsciiStream says:
&quot;This method is particularly suitable for retrieving
large LONGVARCHAR values.&quot;
getAsciiStream makes internal call to getUnicodeStream
and here is the definition of that method:
Now the source of getBytes method:
Did you saw the problem?
When I try to invoke getAsciiStream it will throw SQLException because I'll try to field, defined as
LONGVARCHAR but I'm actually reading it as BINARY.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

105
#105 SELECT -- an invisible value?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I've encountered a strange behavior of hsqldb, first
in the RC3, but I didn't pay attention to it then. Now
with the hsqldb 1.7.0RC4a, I have to face it again and
I believe, it's a bug.
I have a table Hierarchy with a column named Path. One
of the rows in this table has Path='/intranet'. When I
execute the following query:
I get an empty result set.
However, when I use the LIKE clause:
I get really all the paths beginning with '/intranet', including the '/intranet' one.
It seems like the string '/intranet' would have added
some invisible character(s) in the end, but this can't
be the case, bacause of the result of the following
query: which is 9 for '/intranet'.
Please, consider the possible sources of this. I can
send the database on demand (I don't wanna to send it
here to public).
Thanks
Martin Stepanek
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

106
#106 Subselect error with same column names
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This was sent to me by David Moles and applies to 1.61
and to all RC releases of 1.7.0 up to RC4. (has not yet been fixed):
I discovered another interesting quirk. Say I have the
following two tables:
And say further that I have the following records:
If I do the following query:
It works, and I get 2 records, same as if I just id or even
Apparently, the problem is some kind of collision
between fruits.name and trees.name.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

107
#107 Duplicate index names rejected
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have just downloaded hsqldb version 1.7rc4a.
I notice that I may not create indexes with duplicate
names. This behaviour is different to the previous
version of hsqldb that I was using (1.6) where I could
do just that. I am attempting to do this through the
database manager application supplied.
So, the following will work in 1.6, but not in 1.7rc4a:
The error message is:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

108
#108 Trouble with Jakarta Avalon Apps
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I've tried to simply upgrade the jakarta avalon apps
from the 1.6 to the last release (1.7RC5pre2) and I'm
getting some troubles with the
HsqlSocketRequestHandler.handleConnection(socket);
It seems that the SocketHandler doesnt handle the
connection...
Here is a short synospis of the code:
When I use a client with server jdbc connection, the
connection process get thru the 'handleConnection' but
I get a &quot;Connection is broken&quot; in the demo/runManager.
Also when I use the demo/runServer with the runManager
its works.
Regards
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

109
#109 Server mode &amp;  Access denied
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Good evening from France,
My web app opens a connection to an existing Hsqldb
database (server mode, hsql protocol).
But when a query is executed, I get the
because I've kept &quot;sa&quot; as username and &quot;&quot; as password
bertrand
(rougierb@users)
NB : another class creates and fills the Hsql DB from
Oracle but its execution is terminated before (and
without error!)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

110
&quot;SELECT x.y AS z&quot; DOESNT WORK
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
Is there any reason why statements such as the
following dont work properly?
The alias is completely ignored and the result is an
error stating 'column CNAME not found'.
I cant find anything anywhere regarding this but i apolagise if i have missed something.
Regards,
Mark Raynes
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

111
#111 select count(distinct field) ....
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following request doesn't work :
What I get :
bertrand
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

112
#112 Join-syntax documentation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am using 1.7rc4a.
1 The syntax documentation reads that the pattern of a tablelist in a SELECT statement is tableList:
I may be reading it wrong, but that seems to imply
that the 'OUTER' is optional in 'LEFT OUTER JOIN', but
that is not accepted when I try it. I find that
'INNER JOIN' and 'LEFT OUTER JOIN' only are accepted.
Also, the error message reads: but using OUTER on its own is also rejected, so that
is a little misleading.
2/ I also got the impression (from the source) that
brackets could be used around the expression , so that
either could were acceptable. In fact only the second
(without brackets) is accepted.
3/ This is just a question - am I correct in thinking
that hsqldb intentionally only supports expressions of
the type 'col_a = col_b' when specifying the join
expression? When I try 'col_a &gt; col_b', for instance,
that is rejected. Also, it seems that only ONE join
expression of the form: 'col_a = col_b' can be made,
and that 'col_a = col_b AND col_c = col_d' is not
allowed. Is that intentional?
Thanks for your help
Chris Cockrell
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

113
#113 compile errors for jdk1.1.8 with v170rc5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
when compiling v170rc5pre2, I got an error concerning
SwingCommon.java which should not be compiled for jdk
So, I changed line 118 from
to
to exclude SwingCommon.java from being compiled.
But I got some more errors later, see the messages
below.
Regards,
Ulrich
Because it is used outside of its source file
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

114
#114 DELETE fails ArrayIndexOutOfBoundsEx
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

115
#115 ALTER TABLE ADD COLUMN modifies indexes
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The ALTER TABLE ADD COLUMN command seems to be
modifying the definition of existing indexes on the
table.
For example, create a table and an index:
Then, add a column:
Now, the definition of the index includes both
columns F1 and PK.
If another column is added, PK is added to the index
again:
Now, the index definition is TEST(F1, PK, PK). This
results in an invalid index definition and the
database cannot be opened.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

116
#116 Swing - DatabaseManager &amp; Transfer Tool
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I could not get either the DataBaseManagerSwing or
TransferSwing to work from the 1.70RC5 distribution
jar file. I traced it down to the loading of the icon
image in both classes. After commenting out the
getIcon method call and rebuilding the jar, they both
worked. The gif image in the distribution is also no
good. I couldn't load it into any graphics program.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

117
#117 Text Tables - 1.70 RC5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I've spent several hours debugging text table support
in the 1.70 RC5 distribution. I was trying to create
a temporary text table with the following statements:
create temp text table ImportTemp (f1 varchar, f2,
varchar, f3 varchar)
When the set table statement was executed, it returned
the following SLQException:
not found in statement [set table ImportTemp
By removing &quot;temp&quot; from the create staement, all
worked well.
I also noticed that after dropping the table and doing
a shutdown compact, the &quot;csv&quot; file was deleted from
it's directory. I'm not sure what the rationale for
this behavior might be. If this is the planned
default behavior, I would like to see a database
property where it could be turned off, much like the
property for setting the absolute path to the text
files (i.e. not in the same folder as the database).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

118
#118 Error in creating text table with PK
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
With hsqldb-rc5, I tried the following statement
(trying the text table functionality)
CREATE TEXT TABLE B (id integer not null primary key,
name varchar)
And this gave the following error :
Attempt to define a second primary key: S0011 Attempt
to define a second primary key in statement [CREATE
TEXT TABLE B (id integer not null primary key, name
varchar)] / Error Code: -24 / State: S0011
When I omit the 'TEXT' keyword it works fine.
Alexander
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

119
#119 ByteArray needs to be Serializable
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Example:
This fails because ByteArray does not implement java.io.Serializable. Making ByteArray implement
Serializable does seem to solve the problem, at least
for the example above.
Thank you.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

120
#120 backslash not escaped in script file
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Try this: Write this string to any table with varchar
Note the backslash and the icelandic  character
This is what gets written in the script file:
Stop the program and restart it
The script file now contains:
And everytime you start the program another u005c
gets appended.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

121
#121 functions, operators fail in create view
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
You probably know, but ...
your new implementation of views (RC5C) does
not support caluclated column values.
That is (assuming persons) ...
results in error:
SQL Error
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

122
#122 LEFT OUTER JOIN
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
Following is a bug report for a bug that happens on
hsqldb 1.7.0 RC5c but no problem on hsql1.6.1
I have two tables defined as folows:
When I did a qury as
SELECT
I got an error message as
Anyway You have done a greate job on the hsqldb
project. Thanks a lot.
hl
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

123
#123 org.hsqldb.DatabaseInformation.getSystem
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm working on migrating my MS access database to hsql with JBuilder 5 but when I get this error when I try to run the code hsql-1.7.0.rc5:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

124
#124 Total Loss of Data
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
There appears to be a spontaneous re-intitialization of
the .script file Still occuring:
I'm getting a exception on an increasingly regular basis.
When I check the .script file, the data itself is gone,
leaving the CREAT ALIAS stuff.
I haven't been able to force this error to occur; either
locally or on mycgiserver.
Anybody else had this problem again lately?
Any comments most welcome
Cheers - Stan
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

125
#125 SELECT LIMIT 0 10 * INTO ....
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When you do a statement like &quot;SELECT LIMIT 0 10 *
INTO newtable FROM oldtable where blah blah&quot; it
ignores the limit clause and adds all records that meet
the blah blah caluse instead of limiting them like it
should.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

126
#126 &quot;SELECT * INTO&quot; not writing CREATE TABLE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I just noticed that in the newest version it doesn't write
the CREATE TABLE entry on a SELECT * INTO
Here's the code that it wrote in the .script file
I drop the table using:
drop table ESG_CONCEPTS
Then I create the new table using
and it write the above code, but it should write:
because it freaks out when ESG_CONCEPTS doesn't
exist.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

127
#127 NullPointerException when using IN
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
You guys should love me for this. I made a nice
simple test case and everything.
-mike
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

128
#128 jdbc primary key query failed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I use the version hsqldb_1_7_0_RC5c
The error arises, if I query over jdbc the primarykeys of a
table;
The same if i try it with the dababase manager
The origin is the line 515 in the file DababaseInformation
it returns a null
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

129
#129 Errror when creating table with UNIQUE a
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following schema raise 37000 Unexpected
token...error:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

130
#130 quoting problems
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Quoting in SQL statements is somewhat
buggy. I believe that the user should be able to single-quote
any table name or file name to escape munging/normalizing,
but several times I
have had to settle with the munged entity name because
hsqldb can't find out where a quoted string begins and
ends. Examples:
Unexpected token: 37000 Unexpectred token: 't3 in
statement...
set table t2 source 't2.csv'
Notice that in the first sample I did not type 't3, I
typed 't3'; and in the second, the database created the
file &quot;'t2.csv&quot; instead of &quot;t2.csv&quot;.
I do not know if this is an engine problem. That's just my
first guess. If nobody else jumps in, I'll troubleshoot it
farther and/or make a fix once I make some headway with
my other hsqldb work.
Problem experience with 1.7.0rc5.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

131
#131 Ungraceful In-Memory Text Table File err
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
With in-memory database:
patchThread.java:138)
I do realize that trying to set a file path with an in-memory
DB engine is a bad thing to do. I'm just suggesting
that IMO some checking should be done to prevent
misleading
runtime errors like this.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

132
#132 doc errors in TextTables.html
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I come to these opinions empirically. I've verified
that WRT 1.7.0rc5, the following statements are wrong.
The table name must appear between &quot;TABLE&quot; and
The command fails if you use the keyword &quot;TABLE&quot;.
In addition, a SET command specifies the file and the
separator character that the Text table uses:
This is not &quot;wrong&quot;, just incomplete. I GUESS that &quot;DESC&quot;
stands for description, but I have no clue how to set the
delimiter character(s).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

133
#133 minor doc error in hSqlSyntax.html
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The file says
The types on the same line are equivalent and VARCHAR_IGNORECASE and VARCHAR are on the same line. A few lines latter the document says VARCHAR_IGNORECASE is a special case-insensitive
Are VARCHAR and LONGVARCHAR equivalent in hsqldb?
(According to hSqlSyntax.html they are different).
In any case, for hsqldb-specific stuff (like custom
datatype
such as VARCHAR_IGNORECASE, and size limitations) it
would be good to give the users enough information to make
use of the types.
I notice that VARCHAR_IGNORECASE does not store case-
insensitive data, as one could imply by the name, but that
the behavior of some test expressions (at least) is
altered.
No problem with that, as long as it is disclosed.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

134
#134 Tools throw null ptr ex if disconnected
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Both DatabaseManager and QueryTool will give a null
pointer trace stack upon exiting the programs if the user
1.7.0rc5. Standalone engine.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

135
#135 Buggy getImportedKeys
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
First, thanks for a great product!
I saw that there is now finally better metadata support in
hsqldb_1_7_0_RC6c:
However, it seems the implementation is buggy. I have
three tables:
When I call getImportedKeys on the reservations table, I
get an empty resultset. If I call getImportedKeys on the
flights table, I get a resultset with one row. (the same
happens for flights).
I'm quite sure this is wrong. A correct implementation of
getImportedKeys would do the opposite. There should
be a resultset with two rows for reservations and an
empty resultset for persons and flights. (At least that's
what all other drivers I've seen do).
Aslak
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

136
#136 cannot create table with DEFAULT values
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The create script below worked with
Wrong data type or data too long in DEFAULT
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

137
#137 script corrupt: negative DEFAULT values
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HSQL 1.7.0 RC5, RC6c
(this bug is related to bug 565189)
A table column with a negative default value
will cause the .script file to be unusable:
negative integer or double values are written to
the .script file *without* quotes. After a
shutdown of the database, the .script file cannot
be parsed anymore.
eg. This will corrupt the .script file:
I believe the root cause lies with the Tokenizer
class. Instead of treating every single '-' character
as a SPECIAL, maybe only a sequence of two '-'
characters are SPECIAL (a line comment)?
Or a '-' character followed by a numeric character
could be parsed as being part of a numeric value?
I am not sure if this would be correct (I'm not sure
if line comments are the only reason why
'-' characters are SPECIALS)
Instead of fixing the Tokenizer class, how about
patching the DatabaseScript class to surround
*all* DEFAULT values with quotes? (See below)
Or would that cause other problems?
I've attached a JUnit test that identifies the
problem. The test fails with RC5 and RC6c.
The test works after applying the patch.
You can run the test inside JUnit or standalone with
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

138
#138 cannot compile the db
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When compiling with jdk1.4 the compiler gives the
following error:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

139
#139 SELECT MAX(ID) not working
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi !
I tested the latest version: HypersonicSQL 1.7.0 RC6
with this piece of code (which works with hsqldb_v.1.61 )
It gives -1 all the time (it returns &gt;0 with the other version)
Thanks.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

140
#140 Problems under Linux and different JDK's
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
I had this exception on linux only (it's ok on windows os):
I think that there is a limit for the instruction size,
because there is no problems with instruction
Cheers,
Philippe
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

141
#141 JSDK 1.4 Compliance
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
As some may have noticed, HSQLDB wont compile on
JSDK1.4. This should be easy to fix as only a few
methods are added to driver. I am also getting lots of
deprecation warnings - the methods with deprecated
tags are used in the code itself!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

142
#142 calculated values for substr etc
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
In 170rc6g ... Statemets like:
produce:
This function is not supported: IM001 This function is not
This seems to stem from the fact that the substring
and other functions are looking for Integer and
that was not passed ... or top level did not reduce
to Integer.
Thanks,
Joe
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

143
#143 Missing things for views
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi again,
During our testing we noted:
Views show in SYSTEM_TABLES as tables
and there is no ALTER VIEW xxx RENAME to yyy
Joe
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

144
#144 timestamp bug
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
It seems that timestamp columns still have
problems.
In version 1.7.1RC6 I did the following:
-&gt; generates a date but not the correct one
as I saw, the parser does not recognize that this
is a date format
-&gt; throws an exception
This problem was reported earlier in
he also fixed the source! But it still is not in the
current version :(((
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

145
#145 NT -Database is already in use
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I have more than one process accessing the
database I get the following error.
The stackTrace is
I spent some type patching isAlreadyOpen and
distributing updates .jar files to
my users.
Here is what I learned.
I think the problem is the way that the file is opened.
Java states that on
some systems an open file is exclusive.
I think the problem is that on Windows, file operations in
Java under Windows
are exclusive.
Maybe we should rewrite (make sense) Log.java to
make sure that we don't try to
load the properties file while we are trying to save it.
Looking at the current
code, it appears that someone could call open in one
thread (DBConnection) while
isAlreadyOpen is trying to be called in another thread.
Under UNIX/Linux... these are not exclusive and at least
from Java, there is no
way to make them operate so.
So... my thinking (no suggestion yet) is that we just use
a static synchronizer
so that the properties file is not written to during another
write/read
operation.
Does that make sense? It looks like we only have to
modify isAlreadyOpen,
loadProperties and saveProperties...
As an aside. Could a DB connection pool raise this
issue? It seems that it
might. I am using Turbine's DB connection pool with 20
connections. This
seems to work fine under Linux... obviously not find
under Windows :)
Kevin
I have downloaded the latest 1.7 candidate and the
problem still exists. I have had a look for the bug on the
HSQLDB bug list but can't find it, but I would have
thought that it would have been raised previously as it is
discussed on the HSQLDB developers mail list.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

146
#146 Simple SQL query equaling two columns fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Create a table
But should work!
Workaround:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

147
#147 Can not store objects &gt; 64 Kb
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi!
I'm using JBoss 3.0 application server with embedded
HSQL database (I do not know, what version of hsql
JBoss uses).
I have a troubles when I want to store CMP entity bean
that have a field &gt; 64 Kb.
Thats, I have no troubles, when I'm storing StringBuffer
object with capacity = 15000 (~62Kb), or empty
ImageIcon, but I receive such message, when want to
store StringBuffer with capacity = 20000 (or ImageIcon
with image data):
This bug prevents me to store ImageIcons with picture
data and a large objects as a table fields :(( Is there a
way to win this bug?
Kimerinn
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

148
#148 Transfer Tool - no target in v1.61
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
RAM: 512MB (in case it matters)
I noticed that if you launch the Transfer Tool directly it
will only give you a window to select the Source, you do
NOT get one to select the Target.
If you launch this from the DB Manager, you will get
both windows in the proper order.
Any questions: drop me a line at paul.m.boos@saic.com
Paul
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

149
#149  powerbuilder connect to HSQL
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
using powerbuilder connect to HSQL Database by
jdbc,arises &quot;memory can't read &quot;err,why?
Do you try powerbuilder connect to HSQL?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

150
#150 Types with functions
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This function is not supported: IM001 This function is not
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

151
#151 limit on size of sql for view
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It looks like the select statement for a view can only be
about 840 characters --- runs ok on create but stored
truncated. Tried something like:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

152
#152 Exception reconnecting / foreign key
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Everytime I reconnect to a database created with the
following statements, I get an exception. It seems to
be due to the foreign key
constraint originating from a non-primary key-attribute.
I'm using the binary version from the web-site of
HSQLDB 1.7.0 RC6 Revision i with JDK1.4-01 under linux.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

153
#153 getColumnName (continued)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello all!
I had once outlined that getColumnName() in the
jdbcResultSet class returned the column name instead
of the alias of the column (which is the way most jdbc
drivers use).
A patch has been done for that (thanks, by the way), but
it doesn't work in a client/server environment.
The patch has been applied at the Jdbc client level.
On the other hand, it uses a *database* property named
jdbc.get_column_name.
In a client/server environment, the client and the
database are in two separate JVMs (and possibly on
two different machines), so the client doesn't see the
database property (which is on the server).
So, as a result, even if you set the property to false on
the server, the getColumnName on the client still returns
the real column name, and not the alias. And you
cannot set the property on the client because there is
no server and no database there (and you can set the
property only if you start a database).
Fix:
I have two main ways of fixing this.
It involves a bit of thinking about the design. Should the
*database* be patched to behave in the way we want to
(i.e. return the label instead of the column name), or
should the *jdbc client* only be patched to behave like
that?
These are completely different designs. With the second
one, each client might behave differently on the same
database.
So let's have a look at the first solution (database patch):
the jdbc.get_column_name property should affect the
getColumnName() method in class Expression. To
make that work, the &quot;get_column_name&quot; boolean in
jdbcResultSet should be moved to Expression. Then,
the getColumnName() method in Expression should be
patched to return the label instead of the column name if
the get_column_name boolean is false.
Right, let's have a look at the second solution (jdbc
client patch): we need the client to get properties
somewhere. I don't mind where these properties should
come from (another configuration file, or simply in the
url), but the setGetColumnName(boolean) method
should be called within the jdbc driver itself (while
initializing the driver, or while connecting with the URL).
Note that the best fix might be a combination of both
solutions: have a parameter on the database AND
another parameter on the jdbc client.
I let you see what's best from your point of view. As for
me, I think that the patch should be applied at the
database level (and not at the jdbc level). Anyway, this
affects only people who set the property to false.
Thanks!
JY.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

154
#154 Bugs in test suites
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The classes in org.hsqldb.test (v1.7.0RC6i) contain a
couple of bugs.
The main methods of the classes
org.hsqldb.test.TestSql are useless since they don't report
the failures. If junit.*ui.TestRunner is used instead of the
main methods, then class org.hsqldb.test.TestSqlPersistent
should additionally define a suite() method, since otherwise
the TestRunner detects the test*() methods inherited from
the superclass, which I think is not intended.
The last statement of org.hsqldb.test.TestSql.testMetaData()
should read assertEquals(result3, result4); instead of
assertEquals(result2, result4);
Method org.hsqldb.test.TestSqlPersistent.testInsertObject():
the arrays arrayValue and arrayValueResult cannot be
compared with arrayValue.equals(arrayValueResult). Must
iterate over their elements or use java.util.Arrays.equals
((Double[]) arrayValue, (Double[]) arrayValueResult).
Class org.hsqldb.test.TestSubselect: Resource dataset-
subselect.xml is missing.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

155
Oracle chokes on &quot;;&quot; at end of select
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using the transfer tool with 1.7 RC6i and oracle 8i as
the source DB.
The semi-colon added to the select statement on line
126 of TransferTable.java cause Oracle to throw an
exception with an &quot;Invalid character&quot; message. Removing
the semi-colon solves the problem.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

156
Missing servlet.jar in classpath
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
in buildJDK14.bat you seem to be missing the servlet.jar
file in the classpath even though that jar is included in
the distribution. Here's my corrected version.
This is from 1.70RC6. Perhaps other scripts have this
issue also?
@echo HSQLDB build file for jdk 1.4
@echo *** we recommend the use of the ANT build.xml
instead of this method
@echo for all jdk's include the path to jdk1.x.x\bin in
your system path statement
pause
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

157
#157 Typo in getIndexInfo (ORDINAL_POSITON)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
getIndexInfo returns ORDINAL_POSITON not
ORDINAL_POSITION (the third 'I' is missing).
P.S. This bug was found in the project LDBC, see also
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

158
#158 Missing hsqldb.gif
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The batch commands for building are missing
the ...../util/hsqldb.gif for the swing DatabaseManager
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

159
#159 Primary key should not allow null values
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The standard says (at least, most if not all other
databases do that), the primary keys columns don't
allow null values. HSQLDB allows null values. I think this
behaviour should be changed.
This bug was found in the LDBC project.
Thomas
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

160
#160 DELETE FROM does not update database
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After a DELETE FROM order on a table, records stay visible (and selectable) until database is closed.
When re-openning database, records are correctly deleted.
Here my code :
Any idea ?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

161
#161 SELECT COUNT(...) with no rows fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If you select the count of column in a table that has no
rows, it throws a &quot;No data available&quot; SQLException
rather than returning 0.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

162
#162 Error processing SET INDEX on script
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Some of the SET INDEX clauses that are generated by the
engine in the [DB]script file are throwing a
NumberFormatException in the setIndexRoot method of the
Table class when the engine is started. It seems that
there is an inconsistency between the internal index
count for a table, and the entries generated for those
indexes in the aforementioned clause.
For example if the internal index count is 1 and the
set index clause goes like &quot;SET TABLE [table] INDEX '-1
-1 -1 0'&quot;, an attempt is made to parse &quot;-1 -1 0&quot; to get
the identity index root, throwing the indicated Exception.
I just try-catched the problematic line, and set the
identity index root to 0 (as far as I have tested, no
problems are generated for this patch, but I don't know
if the solution is entirely correct).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

163
#163 when shutdown compact is issued server
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
when shutdown compact is issued server throws an
exception as shown above.
Sometimes script is flushed and is in good
condition sometimes it is not.
Sometimes it works properly.
version 1.7.1
DB hsqldb
Thank you very much
Raman Kannan
rkannan@govpx.com
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

165
#165 ClassCastException in preparedStatement
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I'm using v1.7.0 , ( v 1.7.1ALPHA also bugged)
I found a bug in the method
static Object convertObject(Object o, int type)
in Column class, when sqltype is SMALLINT or TINYINT ,
this method return a Integer object.
And when, I call a preparedStatement.setObject(int
parameterIndex, Object x, int targetSqlType)
with 'x' intanceof java.lang.Byte and targetSqlType
equals to java.sql.Types.TINYINT ( or 'x' intanceof
java.lang.Short and targetSqlType equals to
java.sql.Types.SMALLINT ), I get the error :
I modify Column.java to return the correct type ( just
put a line for converting Integer to Byte / Short )
and my program runs fine ( driver workes, but I don't
know if this patch add bugs to server )
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

166
#166 Win98 crash can corrupt script file
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Windows 98, HSQLDB v1.7.0, Java 1.3.1
1. Start database server
2. Perform a minimum of activity, it appears not to
matter what exactly
3. Pull the power plug from the machine
4. Database refuses to start on reboot
We have not had this problem on Win 2000 which struck
us as odd, but it happens every time on 98.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

167
#167 LENGTH(NULL) should return NULL
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It currently returns 0, that is not ANSI standard.
The bug is in Library.java:
This bug was found in the LDBC project
Thomas
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

168
#168 IN fails with BIGINT literal values
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
For a table defined as follows:
The workaround is to enclose the BIGINT literals within
single quotes, but this compensates my cross-platform
SQL.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

169
#169 poolman cannot connect
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When i try to connect with poolman to the 1.7.0 version
of hsqldb i get the error:
SQLException occurred in JDBCPool:
Regards,
Edwin Eversidjk
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

170
#170 Insert Not Working Consistently
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
We are using HSQL 1.61. Through SQL scripts we first
create a schema. Then with another script we attempt
to populate the database. When I run the full population
script it fails with the message &quot;Try to insert into a non-
nullable column in statement ...&quot; on the very first insert.
If I run insert statements in isolation they execute just
fine. Thanks.
Andy
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

171
#171 Trigger classes not loading
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Bug in HSQL:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

172
#172 server.dbname or server.database key?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After lookin at the sourcecode and noticed the the
following use pattern on database property.
Should HsqlServerFactory use &quot;server.database&quot;
property name as well?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

173
#173 getScale returns wrong value
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This bug refers to release 1.7.0-RC3.
Suppose you have a table with a field like this:
quantity decimal(10,4)
If you do a select on this table and look into the
ResultSetMetadata you can find that the getScale
method returns always 0 instead of 4 (the getColumns
method in DatabaseMetadata reports the correct value of
4)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

174
#174 ArrayIndexOutOfBoundsException in RC2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
JBuilders Database Pilot came up with this exception
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

175
#175 jdbc.get_column_name=false in 1.7.1RC3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
Don't think jdbc.get_column_name=false is
working in rc3, did work in 1.7.0
Joe
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

176
#176 org.hsqldb.Library.left
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This is using 1.7.1 RC1. Is this still an issue in
RC3?
I assume this is a collision between LEFT JOIN and
the left(s,count) library method.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

177
#177 Wrong results in LEFT OUTER JOIN
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
A LEFT OUTER JOIN does deliver wrong results when
filtering for IS NULL. The example is using the following
simple ER and data:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

178
#178 RC3 and &quot;Statement Unreachable&quot;
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
classes14:
JDK 1.4.1 FCS, ANT 141.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

179
#179 Forte 4: SQLState exception in 1.7.1RC3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I get this exception when Forte 4 starts up. I don't know
what I can do to get some more detailed information about
the exception.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

180
#180 Including null in select with where &gt;or&lt;
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
hsqldb seems to include rows with null value for attributes with a WHERE equal/smaller/bigger operation as in this example:
Both Oracle as well as mySQL return 1 (the -1 value)
and do not include the &quot;null&quot; value, which &quot;seems
logical&quot; as well. However, hsqldb returns 2. I believe
this is a bug.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

181
#181 default_pk name &quot;SYSTEM_ID&quot; problem
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
i have do test hsqldb version 1.7.0 with eXist (xml db)
but when eXist use RDBMS as storage for itself,
eXists create a table named &quot;documents&quot;.
In this &quot;documents&quot; table has &quot;SYSTEM_ID&quot; column for XML Document's doc-type id.
followings are the DDL for this table.
you can see system_id column of this....
can i ask to change org.hsqldb.Table.DEFAULT_PK 's
value with other value can not have chance to conflict...
next version of hsqldb...?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

182
#182 hsqlServlet not working with Resin
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I had some problems using the hsqlServlet in the Resin
application server.
See:
It seems that the servlet code is not correct. I already
included the code that fixes the problem.
I hope it can be fixed.
regards,
Dennis
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

183
#183 1.7.1 RC 4: type conversion no longer works
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

184
#184 shutdown compact destroys database
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
During a SHUTDOWN COMPACT; I get the following
The error occurs in very particular circumstances, when
a ALTER TABLE is used to add a CONSTRAINT to an
existing table, and something (?) with the order of table
generation is not as HSQLDB might expect.
It took me some time to deduce a reproducible example.
So the following test, creating TABLE vertices, then
They both sould work as expected.
I am using 1.7.1rc5, but I met this bug already before in
version 1.7.0, but was not able to deduce it at that time.
Thanks
Daniel Frey
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

185
#185 Function invocation error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The invocation on stored procedures -
Function.getValue() - convert its data members using
the the Column.convertObject(Object, int) method.
There are two problems with this invocation.
First: All numeric SQL types gets passed into the
reflection as Integers. Every attempt to use a stored
procedure that has byte, short or float arguments will
fail with a 'Function not supported' error.
Secondly: Byte arrays gets passed passed to the
invocation as org.hsqldb.ByteArray objects instead of
byte[] instances. (This error can easily be
circumvented with a type check immediately before
invocation in the Function.getValue() method.)
Thanks for your good work
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

186
#186 issue with system id column visibility
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi Fred. Not sure if this should be considered a
bug or a patch suggestion. I provide no code, but a
discussion of the motivation, so here it is on Bugs:
Was just thinking a tiny bit about what you said re:
hiding SYSTEM_ID column by renaming it to: &quot;&quot;
Here's my little test:
Column already exists: in statement [create table
So, its still not completely hidden: do we want to
take the next step to unsure that it really is fully
hidden from SQL?
I realize that it is now pretty unlikely that anyone in
their right mind will &quot;discover&quot; the above
undocumented &quot;features&quot; (unless they read this ;-).
But, I presume the column still has all the quirks it
had when it was named SYSTEM_ID and visible, so,
in the interest of attaining true closure, I am
wondering if there is not some very brief and
simple additional code we could add to truly make
it innaccessible from SQL. My initial thought on
the matter is that one could assign the column a
null HsqlName.name. I'm reasonably sure there is
no way to assign or get a match to a null column
name from SQL, so this would permanently hide it
as well as allowing &quot;&quot; as a column name on tables
with a system id column in the same way that it is
allowed on tables without a system id column (yes:
I know that is ridiculous, but at least it is
consistent behaviour), without having to add some
silly crutch rules to the column expression
resolution mechanism. Are there caveats to this
approach? If not, this is not much more than a one-
liner patch that shuts the door permanently on the
issue, so I think we should do it.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

187
#187 wrong value from HOUR( date )
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Have table with create_dt DATE NOT NULL column.
Note: The time is around 10:50am machine time.
First:
The return of 19 from the HOUR( CREATE_DT ) looks
like a bug.
HsqlSyntax.html documentation shows HOUR(time) but
has no indication that function is overloaded as
HOUR(date). I would think it should be.
For select, I was using HSQL Database Manager as SA
user, autocommit: true, readonly: false
Was hsqldb version, hsqldb_1_7_1_RC5 inside of hsql-
avalon application.
Machine Windows 2000 2.0 ghz
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

188
#188 Text or mem. tables, read only db files
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using text or memory tables if *.data file is missing
and database files are read only causes errors even if
tables are small and are not modified.
In Standalone mode,
If &quot;readonly&quot; property is false, error message pops up:
File input/output error: tests/database1.properties
java.io.FileNotFoundException:
tests\database1.properties (Access is denied)
If &quot;readonly&quot; property is true, error message pops up
(depending on table type):
Workaround is to add 0 length *.data file and set
&quot;readonly&quot; to true
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

189
#189 ? trace in CachedRow
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
While testing 1.7.1 release ...
init would fail on inputstream error for
was commented out and tests ran o.k. ...
Did not try to figure out what the trace was for ...
Please advise.
Thanks,
Joe
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

190
#190 Outer join syntax fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following SQL statement works fine in HSQL 1.6.1
but it fails in HSQL 1.7.x.
The statement:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

191
#191 Incorrect resultsetmetadata
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
All numeric types have all scale factors and displaywidths
set to 0 when retrieved as their types. Work fine when
retrieved as string.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

192
#192 values in date fields change values
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I compiled hsqldb v. 1.7.1 on linux with Sun's jdk
1.4.1_01 and javac. I created 3 tables with one column
of type Date and insert 2 rows.
On MS Windows 98 with Sun's jre 1.4.1_01 and produced
database when i change time zone stored dates changes
values.
On linux GMT+2
On Windows GMT+1 (or GMT+3, but not the same as on linux)
31-12-2002
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

193
#193 Text Table Bug
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have been testing the CREATE Text Table. When I enter
all INTEGER colums and shutdown the database and again
restart it , the values of the last column keeps
increasing by 7 in every row.
My Jdk is 1.4.1
OS Windows 2000 professional
SQL Statements were generated using the Database Manager
Hsql was started in Server mode.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

194
#194 cannot select from Text Table in1.7.1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I've used Text Tables with versin 1.7.0 in a small project.
All works fine (except outer join bug).
After switching to current 1.7.1 using Text Tables no
more
possible. I've experimented with several different settings
without better results.
I can create an new Text Table using &quot;select * into text
xxx from yyy&quot;
and select data from new table. (I think all doing is in
memory until here.)
But after disconnect/connet receiving this Error
&quot;File input/output error: reading: java.sql.SQLException:
InputStream error in statement [SET MAXROWS
when I use sthe same select.
On the other hand the meta- data of the table can be
shown.
My Jdk is 1.3.1
OS Windows NT
SQL Statements were generated using the Database
Manager
Hsql was started in Standalone mode.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

195
#195 Referential integrity check fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The referential integrity check fails when a column with a
FK is updated!
Create these tables:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

196
#196 implicit type conversion ?!
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I've expected that the result of following statements
in all cases will return 3.3
Im using v1.7.1 in standard or memory mode
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

197
#197 transaction not rolled back
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm using Hsqldb 1.7.1 (latest stable). The database is
in multi-user mode with the following flags:
-trace &quot;true&quot; -silent &quot;false&quot;
If a JDBC client is interrupted abruptly then his
undergoing transaction(autocommit = false) is not
aborted (rolled back), although the server displays the
fact that it has disconnected this interrupted client!
So, the server is aware of the transaction voidness but
doesn't roll it back.
There's another thing to add here. The DatabaseMetaData
for hsql says that it supports ONLY the READ_UNCOMMITED
level of transaction isolation.
Isn't it a pitty? I personally like Hsql for it's light
load in terms of memory and cpu time.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

198
#198 features webpage needs update
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
One of your online documentation pages -
states that a range of _current_ restrictions _will_ be
solved by summer _2002_ (I put that paragraph at the
end of this posting).
By now, it is December-2002.
Some visitors might draw false conclusions from that
with regard to the health of this project. One might
think, that work stopped, or that work proceeds much
slower now, or that documentation is out of date.
Eitherway, this doesn't add to the visitors (especially
hasty ones) trust into this project positively.
BTW, from looking at various indicators I can see that
this project is proceeding very well indeed.
Additionally some visitors might be left
confused/insecure as to what restrictions there
_really_ are currently.
Thanks for your works.
Best regards.
&lt;snip&gt;
Current restrictions are:
* GROUP BY is limited (solved by June, 2002)
* HAVING, ANY, ALL are currently not implemented
(solved by August, 2002)
* No triggers and views (some functionality here
for v .1.7 of hsqldb, coming within days)
* The size of Binary data is limited to about 32 KB
(because UTF is used) (solved by June, 2002)
* No server side cursors (here is a workaround for
selecting big results) (may already be solved)
* Empty space in the database file is not always
reused (here are the details and a workaround) (solved
by June, 2002)
* Does not yet conform to ACID or true SQL-92 or
JDBC 2 or 3 (targeted for June, 2002)
&lt;/snip&gt;
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

199
#199 Column.java  BINARY type bug
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This is wrong:
it should be:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

200
#200 Strange problem with text table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I try to create a text table with following code:
Table is created ok and value foo/bar is added to the
table. After code is executed, size of testAccounts.db is
5 bytes, it contains only text foo and line change after it.
Password column's value bar is not added to the file. If I
select * from account after that code is executed, it
returns only foo for username and 0 for password. If I
use memory table (remove &quot;text&quot; from create table
command) everything works ok.
I have tested this with hsqldb version 1.7.1 and 1.7.2
alpha H. Same problem with both versions. Is this just a
stupid user error?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

201
#201 Text file sql infinitely slow response
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I was excited about the text file sql capabilities. so I decided
to try v 1.7.1 on a 28000 record file and noted the response
time for a select count(*) at about 4 secs on a 384MB
850Mhz NT machine; HOWEVER, on a 350000 record file
the response time was ghastly - the query never returned
after 5 minutes.
Anybody have similar problems?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

202
#202 VIEWS and aggregate function no results (172_I)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have discovered this fast/small SQL engine yesterday -
this is exactly I would like to use in home projects. This
will prevent me using Access for which I have to log with
ODBC. An other good alternative would be to choose
mySQL but I think it too big for such a small project !
Great product - I immediately thought trying my small
application over it transfering data (it is easy with
transfer even if there is no Views support).
Unfortunately I was using &quot;Having&quot; and &quot;aggregate&quot;
functions (SELECT X1,sum(X2) as Y FROM Z HAVING
sum(X2) &gt; 0) - in 1.7.2_I you can define views with those
functions but they don't show anything when run !
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

203
#203 Exception on insert with select from..
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Unexpected token: ( in statement [insert
test case:
and result is
SQL
Error
Serg
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

204
#204 dup idx name on diff. tbls not allowed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If the following 4 SQL commands are issued via JDBC the
last one will result in an SQLException. Since the 2nd
index is on the table bar it should not conflict with
one of the same name on foo.
Details:
Server was started out of the box using the
runServer.bat on a Win2K machine. 2 tests were
conducted. Both the in memory database and the on disk
database. For the on dist test a database name of
&quot;test&quot; was used.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

205
#205 previous() does't work when afterlast
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When you do this:
previous previous previous
It doesn't work because previous returns false when
afterLast condition ocurrs.
I think the problem is the order of condition evaluation at
jdbcResultSet.java.
If you move up afterlast condition evaluation (before
Empty resultset or no valid row) and recompile the
output is OK. I don't know why.
HSQLDB version is 1.7.1
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

206
#206 GROUP BY with SUM() returns fantom row
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am using ALPHA-J on Win2K, Sun JDK 1.4.1_01
When the GROUP BY and SUM() are used on an empty table
a row of nulls is returned.
create table foo (word varchar(20), fooi integer);
select word, sum(fooi) from foo group by word;
returns... (null,null)
select word from foo group by word;
returns an empty result set.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

207
#207 Boolean object failed to be serialized
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hey I guess Fred,
the following tests failed with the exception down.
The coloumn 4 is from type Other.
The exception is:
If I use a
It works fine.
Alex
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

208
#208 GRANT before CREATE VIEW + ISNULL
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In my test application, I have re-written some fine views -
and noticed these 2 points :
1-In script file, GRANT SELECT ON view_XY TO
PUBLIC is done before CREATE VIEW view_XY AS ...
this means, I got often errors and I've to correct the
script file before starting database server (I am running in
server mode).
2. How is working ISNULL(x,y) - as I didn't make it - I've
used CASEWHEN(x,y,z) ! What do you think ? Is this
command broken or I should exercice a bit more ?
Yours
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

209
#209 Slow query
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have provided a DB and query. The query executes
very slowly. However, if I break it up into several smaller
queries that do the same thing, they execute much
faster (also included). Both queries create only 448
rows. My assumption is that the query optimizer isn't
working correctly.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

210
#210 Incorrect number of rows in GROUP BY query
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
A query with a MIN() aggregate function and a GROUP BY
clause, where there are no rows that qualify for the
WHERE clause, returns one row with only null values.
According to ISO 9075:1999, part 2, subclause 7.11,
&quot;&lt;query specification&gt;&quot;, general rule 1bI, such a query
should return no rows:
&quot;If T has 0 (zero) groups, then the result of the &lt;query
specification&gt; is an empty table&quot;.
Example:
SELECT id, MIN(date_of_birth)
FROM person
GROUP BY id
This query returns a row with two null values, when table
&quot;person&quot; is empty.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

211
#211 Unhappy with group by
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Technically this is a bug, because of the documentation, but
it may be an oversight:
our SQL generator uses &quot;...group by x...&quot; where x is a
column number containing an expression.
Under HSQLDB if the expression is a date operator (e.g
quarter(&lt;date&gt;) ) this does not work (SQL error).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

212
#212 Index not dropped when table dropped.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have an application that will periodically drop and
recreate a table. I find that in one particular case
this seem to fail regularly as an index gets left behind.
When I execute the following script it will
consistantly fail.
I get the following when the above is executed...
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

213
#213 foreign-key constraint test failed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following foreign-key constraint test failed:
Can you please solve this issue?
(btw: other tests concerning foreign keys I tried, were
successful ! )
Thanks!
Regards,
Albert
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

214
#214 Binary data and 'in' operator
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Actually, I've found two bugs. Please, see the following
The following statements must produce the same
results (each statement must produce two rows):
Set 2 (doesn't work. Problem with binary data and using
Set 3 (Trying to work arround set 2. Works horrible.
I think rawtohex confuses the inner select &quot;oid&quot; with the
outer &quot;oid&quot; and allways returns true. BUG 2!)
Set 4 (trying to work around Set 3 + Set 1 problem)
Ok, that's all. I hope you solve this soon as possible.
Cheers.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

215
#215 Text Table Problems
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When using text tables, the last column doesn't (on my
system) get persisted to the files. When the server is
shutdown and restarted, the missing column causes an
exception.
My System:
To Reproduce Bug: Start attached DB in Server Mode
Open Manager and send CHECKPOINT cmd
Examine the CSV and note the absence of the last column.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

216
#216 Text Table Field Concatenation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Zero-length characters fields when concatenated with non-null fields yield result in null or zero-length string.
For example:
If any of the concatenantion fields are zero-length then
Descr will be zero-length.
Using the parameter: empty_is_null=false
Thank You
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

217
#217 shutdown compact corrupts data
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After doing a &quot;shutdown compact&quot; in Version 1.72aL I
read the lines
General error in statement [CREATE CACHED TABLE
ARTIST ...
at System.out.
After that all data files are gone and the unprocessed
tables are not in the script any more.
It was possible to track it somewhat down; it seems an
index gets inserted which wasn't removed in
DDL to reproduce:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

218
#218 dates are not being normalized in CURDATE()
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Currently, the DATE SQL type is being treated exactly
like the TIMESTAMP type. Instead, the DATE data
should be &quot;normalized&quot;, as per the java.sql.Date class
documentation:
&quot;To conform with the definition of SQL DATE, the
millisecond values wrapped by a java.sql.Date
instance must be 'normalized' by setting the hours,
minutes, seconds, and milliseconds to zero in the
particular time zone with which the instance is
associated.&quot;
The function curdate() is not normalizing the dates it
returns. This allows for an &quot;invalid&quot; date to be created.
Here is an example of this bug:
-- manually specifying todays date should yield one
row, works ok
Hope this helps. If you need further information,
contact me at sieira@usa.net.
Thanks for the great program.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

219
#219 Group By returning null values
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If you create a simple table like this:
create table test (a varchar(100),b numeric)
and then you execute this:
you will get a single row with null values. The expected
result is no rows.
If you execute
you will get a single row with null values. The expected
result is a SQL error like this &quot;not a single-group group
function&quot;.
Cheers.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

220
#220 LEFT JOIN regression from Alpha J =&gt; L
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In Alpha J, I had built several queries using LEFT JOIN
to detect inequality in table1.id = table2.id such a way
table2.id is returned null !
On application testing with (Alpha L), I got bad results
(null are not shown) : Alpha K, Alpha L and even latest
Alpha M !
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

221
#221 now() doesn't work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Whats happened with now()? It doesn't work in alpha_m.
Some changes at Tokenizer.wasValue() are in conflict
with &quot;now()&quot;. If you execute:
will get:
To solve this problem I removed sToken.equals(&quot;NOW&quot;)
from return (at line 253 of Tokenizer.java). It works but I
don't know the effects of this change.
Why &quot;NOW&quot; is there? Future features like Oracle's
SYSDATE?
Cheers.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

222
#222 Log-Thread not exiting if any connect fails before
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
Using the henplus JDBC Shell
I noticed, that the Logger thread in hsqldb is not exiting
properly, if any of the connect attempts to a database
failed because of user/password failure for instance.
Why I noticed this with henplus is, because henplus first
tries to connect to the database just with the JDBC-URL
alone and if this fails, prompts for the password.
This means, that the first connect will always fail, while
the second will succeed, if the user/password is right.
The problem is, that hsqldb starts the Logger thread with
the first attempt to connect to the database and
increments the usage count .. however that connection
never gets used, because connecting fails and throws
an SQL-Exception (see
jdbcConnection::openStandalone()). This means, that the
usage count is always the number of all _attempted_
connects not real connects.
I did a simple fix, see attached patch. This patch
initializes the usage count with zero and _after_ the
connect is successful (i.e. if no Exception has been
thrown), the usage count is incremented. This will make
sure, that the last active connection, that is close()ed,
will shut down the Logger thread correctly.
Note, however, that this is not a complete fix to the
problem. If we get _any_ connection that connection that
connects correcly, then this solution will work, since the
database is removed and the Logger thread is closed in
the close() of that very connection if the usage count
turns out to be zero. However, if _no_ connection will
succeed, then we will still have one database in the
hashtable since no close() will run through (this is
another problem: in finalize(), close() is called; however if
this is a jdbcConnection that failed to connect, then the
session is 'null' and executing a shutdown on that
session will fail as well -- so the finalizer thread throws a
NullPointerException. The finalaizer-Thread better should
catch any Throwable instead of only a SQL-Exception).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

223
#223 LEFT JOIN AND Null Values (ALPHA J)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
You should have
but instead you got :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

224
#224 Error with aggregate functions
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The statement
..works if field1 and field2 is of type FLOAT, but if it is of
type BIGINT, it does not work. The I get this error:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

225
#225 Strange Exceptions with Alpha M
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hey,
I'm using the 1.7.2 since a couple of days in my test
enviorment. So far everything fine.
Since yesterday I'm getting strange exceptions:
If I do the constraints into the create table definition the
whole create won't work.
These general Exception seemed not to be very
important becauseI can still work with the database and
be happy. But these exception above occoured after an
insert of 32.000 records and that's was strange because
all records are in the database (for my luck)
Alex
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

226
#226 Driver.connect not verifying user/password
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Build: 1.7.2
If I connect to the Server with an invalid user or
invalid password (with a good username) I get a
connection, but the next db access via the connection
fails with a:
This exception should be thrown from the connect method.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

227
#227 Transaction, UK constraint, Rolback produce lost of data
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I found some problems while using last stable version of hsqldb (1.7.1.).
Transaction and Unique Constraint violation and
Rollback could produce unreliable db, and lost of data.
I try this (from Database Manager):After that depend of the amount and type of data in
destination table (in this case TUKR), some data would
be lost, additonaly many other commands produce
same S1000 error (including SHTUDOWN).
From JDBC connection behaviour is exactly the same.
PK violation lads to similar results.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

228
#228 DatabaseManagerSwing
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I recently downloaded/installed hsqldb(1_7_1). I
tried running the DatabaseManagerSwing class and
get the following:
It apppears&quot;hsqldb.gif&quot; is not in the jar. Where did I
go wrong or was it mistakenly left out of the package?
Thanks,
Frank
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

229
#229 Foreign key constraint error with trees
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The foreign key constraint does not work correctely
when used within one table.
Example: The table A below is used to create a tree
structure in a database table.
It should be possible to fill the table with the nodes
of the tree:
Though the first insert would create a legal table contents, it creates an SQL Error:
insert the first line directely into the hsqldb.script
file, restart hsqldb. This creates the root node, all
other inserts work.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

230
#230 error 23000 after alter table with PK
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi!
I am using HSQLDB version 1.7.1.
I am running the following: alter table a add column amount integer default 0 not
null;
The last insert to table b fails with error 23000 - Integrity constrains Violation.
Only stopping HSQLDB, and starting it again will allow
that insert to work properly.
It looks like a BUG to me.
Any suggestions?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

231
#231 IDENTITY skips numbers when table updated
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I think I found a bug in ALPHA M:
I have noticed in my application that a table with an
IDENTITY column skips numbers when any row of the
table is updated. Here is an example:
Here is the final contents of the table:
Contact me at sieira@usa.net if further clarifications
are needed.
Thanks.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

232
#232 Foreign-key constraint fails on update
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have encountered a problem with foreign keys. Ive
read that others have mentioned this problem before me
and that it has been fixed in later versions.
I first used hsqldb_1_7_1 and then tried the same in
hsqldb_1_7_2_alpha but with the same result.
Ive created to tables:
I then add a team and some users (on inserts-statements the constraint works fine), when I try to
update a user and changing his team to one that does
not exist, it still works. That is the constraint has no
effect.
Questions:
Is there any way I can work around this problem, or
write the syntax in another way?
Is this problem supposed to be solved in the
Best regards / Philip
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

233
#233 DatabaseManagerSwing misses Icon
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When starting org.hsqldb.util.DatabaseManager I get a
NullPointerException:
The reason is quite simple: There is no hsqldb.gif
under org/hsqldb/util in the JAR file (or anywhere else in the 1.7.1 release distribution).
Could you please add that image file (and maybe even
add that icon to the AWT Database Manager)?
Otherwise your database is great. I currently use it to
replace a big fat Oracle database during development
work and regression testing :-)
Hendrik Wrdehoff
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

234
#234 NullPointerException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello
we are using Hsqldb 1.7.2. The database is
in multi-user mode with the following flags:
-trace &quot;true&quot; -silent &quot;false&quot;
We are getting NullPointerException in database
window.The following test condition is observed:
To reproduce the bug, make sure HSQL database is
running for very long time and DB is quite big.ie size of
the database is
Logged in to DB as Administrator
We are using Connection pool to update 3 hsqldb
database using type 4 driver of hsqldb.
Is there any workaround to solve this.
Thanks in advance
Neppo/Srini
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

236
#236 PowerBuilder 8.01 crashed when trying to connect to Hsqldb
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The issue could be PowerBuilder itself, but I post it here
to see if it rings any belt.
From PowerBuilder 8 IDE, I can connect to Oracle 8,
Informix 7, and SqlAnywhere (ASA) 6 JDBC drivers.
But, when I connect to Hsqldb 1.72 rev M, then PB will
crash.
Is it because Hsqldb does not implement
getColumnDisplaySize nor schema? I don't know.
Ben
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

237
#237 Problems Running on NetWare 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
JVM: JVM Version 1.3.1_06 patched to level noted below.
First confirmed as a JVM (*see below*) problem by
original submitter, Dick Hildreth, in the Open
discussion forum and currently under study. Mr.
Hildreth's detailed report of the problem follows,
followed by further posts from Mr. Hildreth and
Maintainer. Original Forum thread is at:
Problem report follows:
Background
Rows were disappearing from the hSQLdb 1.7.1 run
database.  When a record was updated, all records with
key values (char type key) lexically before the
selected one would disappear. No deletes were found in
the .script file yet after doing a SELECT query, the
records would be missing. Also, running a script
command in the DatabaseManager and looking at the
resultant file, the records
would also be missing. This only occurred when the
hSQLdb client was running on the NetWare server (either
a calling program or hSQLdb's DataBaseManager program
running in the NetWare GUI). A client running on a
Windows box could run the update without exhibiting the
problem.
Running System
The above occurred on a Netware 6.0 server patched to
service pack level 2. Two post sp2 patches had been
applied: the NSS patch dated 02-01-2003/11:37AM and the
TCP patch version 6.15o. The JVM was version 1.3.1
service pack level 3 which showed the following
information under the Netware command module
The problem was replicated on another server with the same
configuration with the exception of the two post sp2
patches.
Fix
Applying a different JVM has removed this problem from
the server. The new 1.4.0 JVM (and the required new
LIBC from March 2003) has the following attributes
displayed under the Netware command module java:
With this sole change, the loss of records ceased.
Bug report filed by dedmike.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

238
#238 InputStream error with unique indexes
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Version 1.7.1 with Sun JDK 1.4.0.03 on Windows 2000:
I'm getting inputStream errors and sometimes
numberformat exceptions on startup of a database.
To reproduce create a new database run the attached
create.sql script in the manager and restart the
database. I get this error :
TABLE IMAGE INDEX '-1 0 -1 0' contains negative
numbers.
If i leave out the unique index it is running fine.
Achim
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

239
#239 Thread SQL Scripts in Help Forum
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Enclosed is the SQL Script that I do not manage to have
run by the ScriptTool. It is the same that produces the
&quot;Wrong data type or data too long in DEFAULT clause&quot;
error from my DatabaseManager thread.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

240
#240 Wrong math
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am using hsqldb 1.71.
Following is an example that the engine can produce
wrong math. This may be the result of data type (or
data precision/scale) conversion error:
Ben
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

241
#241 Problem with &quot;UPDATE ...&quot;
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I haven't had time to investigate this fully, so apologies
for the lack of detail.
I have some code which uses the HSQLDB in stand-alone
mode. I find that when I use UPDATE to modify an entry
in a table and subsequently COMMIT it, the modification
is visible only for the life time of the run time. Closing
the application and reloading it causes the pre-UPDATE
data to be retrieved. I have tried switching to CACHED
tables which had no effect. Looking at the .script file
produced when the tables were uncached seemed to
imply that the behaviour for UPDATE was implemented
through the use of DELETE followed by INSERT and that
the SQL that did this was not produced correctly
(DELETEd all records in table then INSERTed the same
record three times!).
By changing my code to use DELETE and INSERT instead
of UPDATE. I can now get data to remain modified. I
may have missed something here.
I look forward to seeing the inclusion of
CONCUR_UPDATABLE. This would have made the coding
much easier. Not complaining though. I currently have a
RDMS for an application and it saves me from the
nightmare of using JDBC-ODBC -&gt; Microsoft JET... which
didn't really appeal. Keep up the good work guys!
Jon S
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

242
#242 echo_ scripts not found
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I don't know what system you use to develop hsqldb but
echo_failure, echo_warning and similar commands are not
present on my GNU/Linux Debian system. I'm using stable
hsqldb version.
Regards,
Max
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

243
#243 BIGINT corrupts very big negative long values.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi all:
I can't store very big negative long values
(generated by a hash function) into bigint column
data type.
Please test it:
Very big positive longs seem to be work OK.
I solve my problem storing long values as String,
but long stored in native form will be better.
Best regards, Marcelo.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

244
#244 SQL SELECT IN operator does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
IN operator used as part of expression in SELECT sql
statement does not work with 1.7.1.a version.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

245
#245 standalone is not exclusive
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello!
I'm using hsqldb 1.7.1 and don't figure out what's
wrong with the standalone database url.
Why is it possible to connect with DatabaseManagerSwing
to a hsql database in STANDALONE (IN-PROCES) mode
several times simultaneously??? And I can also start a
server for the same database, simultaneously , too?
Looks like a bug (the database is the same for sure).
Thanks.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

246
#246 database script file order error 1.7.1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
i don't know if there's any special reason why in the
class DatabaseScript, more exactly in the method
getScript(), the first thing you do is to iterate the vector
tTable, ignoring temps and views....
then you dump users, and their grants to the script file
and finally you iterate one more time on tTable, to dump
the views creation script
the problem is:
suppose sa creates tables, views, and a user, then
grants permission for the new user on a view
if you connect as the new user you can access the
view, but if you shutdown the engine, the next time you
start it up it finds the grant .... on &lt;view&gt; statement
before it finds the create view statement, so an error
appears saying that the view is an unknown table
i suppose there's a reason about not dumping the views,
when you dump the tables (there's a line saying &quot;//
fredt@users 20020221 - patch 513005 by
sqlbob@users (RMP)&quot;)
hope you fix it soon... actually i'm changing the order in
the script manually
thanks in advance
Alejandro Gomez
agomez@cibermatica.net
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

247
#247 ResultSetMetaData getColumnType error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
ResultSetMetaData.getColumnType() returns null
pointer exception when called, but however returns
correct value sometimes (in another test program).
The application program was checked using jdb to
ensure that the correct parameter was passed to
getColumnType. The metadata was used to implement
the Java Swing AbstractTableModel. Other metadata
functions also failed to return a proper value except
getColumnCount() which returned the correct count.
Another short program was written to test
ResultSetMetaData.getColumnType(i) shows that the
function worked perfectly in this case.
lks@webpres.com.my
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

248
#248 Column size ignored
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I created a table with a char(1) column and the DB had
no problem to insert values of much greater length.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

249
#249 out of memory
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Also I created the tabled as &quot;cached&quot; the database ran
out of memory (test.data ~25MB)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

250
#250 count distinct rolls back when no records counted
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
executing a count distinct query, when there are no
records counted, fails. a rollback is executed an null is
returned.
trace when there are records:
iBanx
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

251
#251 Failure to fire trigger on table insert
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Operating System: Windows 2000
Service Pack: Service Pack 3
HSQL 1.7.1
IDE: Oracles JDeveloper
JDK 1.3
Jave Compiler: Oracles's
Hi,
I have created a database that has 2 triggers that
should be fired on deletes and inserts operations on a
table called FILE. The trigger for deletions works fine,
however the trigger for insertion fails to be fired. I have
tried various combinations i.e. be fired before and after
insertion - but the trigger for insertion never gets fired.
I have enclosed a zip file that have the following
directories:
database files. The *.bat are what is used to startup the
database.
java source files for commicating to the database.
Any further information or help - please email me
Cheers
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

252
#252 cannot have foreign key constraint on same table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Is it possible to have cascading records? Each record has
a parent record. The root record references itself.
Consider the create table statement below (which
works), however you cannot insert the first record
because cnt_parentid is defined not null.
This does not work:
The foreign key is violated, because record id -1 does not
exist.
Thanks
tomsk
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

253
#253 cannot use self join on unique column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Found in production 1.7.1 and in ALPHA_M.
This bug is related to #722442
To reproduce do the following:
I have cascading records in several tables - each record
has a parent record in the same table - the root has
itself as parent.
I cannot define the parent id as foreign key of the id from
the same table.
Thanks
tomsk
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

254
#254 CHECKPOINT DEFRAG fails on Alpha_J
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If there was no prior data activity in the script file as in:
Unfortunately this is a tough one to figure a workaround for.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

255
#255 Delete not commited
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm using hsqldb version 1.7.1 as standalone database and
JDBC driver to access. Java application is single-user, so only
one connection is created at program start and its shared (no
threads) until the end of application. After deleting more than one
record from some table, only one is deleted, others can be
visible in next select. Database is in (default) auto commit mode
and to making me sure, I have added conn.commit() after delete
operation. But &quot;delete problem&quot; appears again.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

256
#256 CREATE ALIAS issues for java.lang.Math.(min,max)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
see next comment
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

257
#257 HAVING exception using TAG: latest pre ALPHA_N updates
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi Fred.
I ran into a defect that I thought initially I had
caused with the code tagged &quot;pre ALPHA_N
refactored code&quot;
The incident does NOT manifest in the ALPHA M
files download.
However, after some frustrating work and not being
able to find anything in my added/refactored code
that would cause this defect, I decided to do a
checkout that consists of a snapshot of what was
in the hsqldb-dev CVS, just prior to the &quot;pre
ALPHA_N refactored code&quot; tagged updates. Lo and
behold, the same defect manifests as in my local
working checkout.
there are a few related defects, but the following
illustrates the biggest part of the problem, which I
believe is related to Expression.setTrue() or
possibly an incorrectly calculated iHavingIndex and
some unwanted interaction with aggregating value:
Say, after performing the database manager &quot;insert
test data,&quot; I do a select like:
select firstname, count(*) as &quot;count(*)&quot; from
Customer group by firstname
This is fine. I get the expected result:
However, say I issue the same query but with
a &quot;having&quot; clause involving the &quot;count(*)&quot; term (any
operator and value will do...perhaps any aggregate
expression as well...I haven't tested that far yet).
Then I get:
So this proves that the defect was in the pre Alpha
N CVS *BEFORE* the last update.
I would list the stack trace using the latest CVS, so
that you don't have to translate the line numbers,
except that I can't build the latest hsqldb-dev CVS
due to its missing the Token.java file in
org/hsqldb. Would you please commit this to the CVS?
Thanks,
Campbell
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

258
#258 Error on using full qualified column names in insert command
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Trying to update a table by using full qualified column
names like &lt;table name&gt;.&lt;column name&gt; we get
a &quot;Column not found&quot; error. Example code:
Is this a bug? Do you think this syntax could be
supported in future?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

259
#259 Serialization failure
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi.
Here is bug scenario:
0) I'm starting application with in-process hsqldb server.
1) I'm creating sample table (REGISTRATIONS).
2) I'm inserting sample object into it.
3) I'm shutdowning hsqldb server using &quot;SHUTDOWN&quot;
command and exciting application;
4) Second time, when I trying to start application with
existing database, a &quot;java.sql.SQLException:
Serialization failure&quot; exception occurs while creating
a connection to database.
I have debugged hsqldb code and checked that method
&quot;readLongVarString()&quot; in TextDatabaseRowInput.readOther
probably returns incorrect value. It reads object's
binary data only until first carriage return in binary
data occurs.
I've created demo which is packed into errorSample.zip.
Here is full error stacktrace:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

260
#260 PreparedStatement.execute(sql) works but should not
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
According to the specs, &quot;If any of the
PreparedStatement execute methods is called with an SQL
string as a parameter, an SQLException is thrown.&quot; See
jdbc-3_0-fr-spec.pdf, page 101, 13.2.4 Executing a
PreparedStatement Object. (-fr- stands for Final
Release, not France). You can get this document here:
Currently, HSQLDB executes the statement, but it should
throws an Exception. This bug was found using LDBC
(ldbc.sourceforge.net).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

261
#261 OutOfMemoryError when inserting into LONGVARBINARY column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I create a table that has a LONGVARBINARY
column and try to insert data from a file in the file
system, the java.lang.OutOfMemoryError exception
occurs.
The documentation says that the max size of a
LONGVARBINARY column is Integer.MAXVALUE, but
the problem is happening for me with a Word document
that is about 2.8 MB.
The attached Java program shows how to reproduce the
bug. When the JFileChooser dialog shows up, just
make sure you pick a large Word document or
Powerpoint presentation file.
Unfortunately, the OutOfMemoryError exception is not
propogated properly, meaning, I am unable to &quot;catch&quot; it.
I simply see the error reported in the Java console.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

262
#262 Problem with &quot;NOT NULL&quot; constraints on UPDATE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Version : HSQLDB 1.7.2 Alpha M
The &quot;NOT NULL&quot; constraints don't work in update
statements.
Test :
This problem was not in 1.7.1 version of HSQLDB.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

263
#263 LEFT JOIN not work with 2 primary keys in a table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
For example:
will return
I tried with only one key and it works.I am using 1.7.2
ALPHA M
-andy santosa
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

264
#264 Connection.close() does not release TCP/IP port
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When using Connection.close() to close a connection
with a HSQLDB server, the TCP/IP connection is not
closed until the client JVM is terminated.
This can easily be checked using netstat under Windows.
Open a connection (Keep the client running)
-&gt; connection to port 9001 is visible in netstat /a
Close the connection (keep the client JVM running!)
-&gt; connection to port 9001 is still visible in netstat /a
Any attempt to issue a statement using the Connection
inside the Java client will result in an error
close client JVM
-&gt; connection disappears
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

265
#265 error selecting primary key column with itself
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Create the following table and get the following error
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

266
#266 CASEWHEN bug (with hibernate)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
With Hibernate &quot;joined-subclass&quot;, a statement like this
one is generated:
The problem comes with the 3rd argument of the inner
casewhen function (null): the data type returned is
always set to 0 and the switch in
This error occur in HSQL mode (not in STANDALONE mode).
I have patched Expression.resolve(TableFilter f) this way:
instead of,
Everything works fine with this modification (but they
are maybe some side effects I cannot figure out).
Regards,
F. Wolff.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

267
#267 assert is a keyword
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
warning: as of release 1.4, assert is a keyword, and
may not be used as an identifier
Example:
A simple solution is to change the 'assert' to
'cassert' in all files. That is what I did to solve
the problem.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

268
#268 Count Aggregate function  is giving wrong results sometimes
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I am using HSQL database in my project.
I am using following SQL query to retrieve count
of packets in database.
The database contains two tables RSPPdu0 and
RSPSegment0.
I need to retrieve the number of pdus( pduKey) per
output port
The above query works fine in most of the cases. But
for two different samples it is giving incorrect count of packet
In one case it gives
68002 instead of giving 68000
In other case
12802 instead of 12800
Also, this behavior is inconsistent on Solaris/Linux
and window platform. Sometimes the same query gives
correct results , sometimes incorrect.
It is highly unpredictible when it will give correct
result
Can u please help me in this regard ? Is this a bug
related to HSQL database or am I wrong somewhere in
forming SQL query.
An immediate response will be helpful.
Contact id : riti@agere.com
Thanks
Riti
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

269
#269 Error with UPPER() in ORDER BY
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I was using 1.7.2 Alpha M and I did this SQL:
And it gave me this error even though it's valid SQL and
works in version 1.7.1:
Here's the stacktrace:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

270
#270 1.7.1 Engine's LEFT method does not work.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The use of the &quot;LEFT&quot; alias doesn't work:
SQL Error
Unexpected token: LEFT in statement [select left('mike',
3) from test] / Error Code: -11 / State: 37000
1 row(s) in 0 ms
If we create a new alias w/ a different name:
it works just fine:
select leftstr('mike', 3) from test
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

271
#271 Group By and Binary columns
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
HSQL has a problem comparing binary columns in
grouped queries.
I use binary columns to represent object IDs. Check this:
select &quot;oid&quot;,sum(&quot;amount&quot;) as &quot;result&quot;
from &quot;test&quot;
group by &quot;oid&quot;
If you have 30 rows with te same oid (a binary column)
you will get a result set with 30 rows.
I think the problem is related to HsqlHashMap. It uses a
HashTable in order to store groups. Unfortunately two
byte arrays with the same content will have a difrent
hashcode. HsqlHashMap assumes different groups.
Thanks.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

272
#272 Incorrect SQL execution
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This bug has been described in the help forum -- please
see
Attached is a .tgz file containing the database in
question. Its user/password is sa/admin.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

273
#273 negative BIGINT are wrong
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi I tried the following:
the value which I gave to id is a valid java.lang.Long
value. Is there a fix to that already ?
Thanks
Michele Laghi
laghi@swissinfo.org
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

274
#274 erroneous fk constraint violation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm using hsqldb 1.7.1.
Attached to this message is a db that exhibits
erroneous (as far as I can
tell) behaviour. It's a little difficult for me to
strip out all of the
superfluous (for this problem) tables, so please just
consider the
PX_IDENTITY, PX_MEMBERSHIP and PX_CONTACT tables. There
are only 3 rows total
in the entire db, so it's fairly easy to see the issue.
PX_MEMBERSHIP is a simple many-to-many join table that
has fk constraint from
I get:
How can this be, when PX_MEMBERSHIP is empty??
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

275
#275 BIG script works on standalone but server
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I tried to insert some data by using a BIG script.
It was working for standalone database but failed in
server mode database.
HSQLDB : 1.7.1, virgin, no patches
Java : IBM J2SDK 1.4.0
Linux : redhat 8.0
How I did it:
* Run demo/runManager.sh
* Open connection, standalone or server mode
accordingly
* Made a new table.
* Open the script (attached) and execute it.
* Try to execute select * from kuisikon.
Submitted by:
PS: the database information is in Indonesian language,
I'm sorry for not translating it, but I hope it does not
matter.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

276
#276 Connection Problems in standalone mode
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hypersonic DB when running in standalone(In-process -
not in memory) mode does not allow the Process which
created the DB to use nested transcation(nested
connection) or transcation across threads and throws
the below Error
ERROR: The database is already in use by another
process
ERROR: Cannot open connection
java.sql.SQLException: The database is already in
use by another process
Let me explain the scenario.
1. Gets the DB connection with security user
(Username=security1, pwd=&quot;pwd&quot;,
url=jdbc:hsqldb:myDB&quot;). Get all the necessary data.
close the connection.
2. Now get the DB connection with real user
(Username=real, pwd=&quot;123&quot;, url=jdbc:hsqldb:myDB&quot;). Do
a lot of initialization.
3. Now I have this part of code which executes a nested
transaction. And I get the above error when the inner
So when i tried to debug hsqldb, I found the following
bug with the implementation.
1. org.hsqldb.jdbcConnection holds a static instance of
HashTable-tdatabase(url,org.hsqldb.Database Objects).
When closing connection the corresponding entry is
removed and the Database objects goes out of scope.
2. Now when i reconnect to the same DB, it creates an
entry in tdatabase. And if the finalizer for the previously
freed Database objects is called now the entry in the
tdatabase(jdbcConnection class) is removed and if you
try to get any more connection it reports with the above
bug as it tries to create a new instance of DB instead of
creating a new connection(openStandalone(...) method
in jdbcConnection Class).
Solution.
1. We must not depend on the finalizer method as we
have no controll over it.
2. never call close to a connection(will be a bad
solution).
3. As a temporary solution i have commented method
lines in removeDatabase(...) in jdbcConnection Class and
everything appears to work fine for me.
Other Details.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

277
#277 Column not found
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Executing the below query and when getting the result
the below error occurs.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

278
#278 ResultSet down not move backwards!
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I was able to move in both directions when using
resultset in hsqldb1.7.1, but with the code from hsqldb-
dev throws the below exception when attempted to
move backwards... I am using hibernate 2.0 to access
hypersonic.
jdbcResultSet.rsType can be set to
jdbcStatement.rsType when creating the ResultSet.
This will enable movement in both direction of the ResultSet.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

279
#279 NPE on SHUTDOWN when created by HsqlServerFactory
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If the HSQLDB Server is created using the
HsqlServerFactory then its internal socket instance
variable is never set. If a &quot;SHUTDOWN&quot; command is then
issued, a NPE will be thrown within the Server.notify
method on the &quot;socket.close()&quot; command. The code just
needs to check to see whether or not socket is set
before attempting to close it. See the patch to fix
this below. This is in the current release version 1.7.1
Cheers,
Leif
Index: Server.java
RCS file:
retrieving revision 1.8
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

280
#280 SQLEx closing Connection that is closed by server.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have been working on getting HSQLDB running within my
application in server mode. The database is then
connected to using a pool of JDBC Connections.
Everything works great until I call SHUTDOWN as the
server is shutting down.
As the server shuts down, one if its tasks is to go
through and close all of the Connections in the JDBC
pool. The problem is that the HSQLDB JDBC driver
throws and SQLException with the message, &quot;Connection
is broken&quot; for each connection as it is closed. This
is because the socket has already been closed by HSQLDB
on the server side. It makes sense to throw this
exception if a query is being made. I know this is a
matter of opinion. But if the client is
attempting to close the connection and the JDBC driver
detects that the connection is already closed. It
seems like it should just fall through gracefully as
the desired outcome was achieved.
The following patch modifies jdbcConnection.java to do
this.
This is with the current released version 1.7.1
Cheers,
Leif
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

281
#281 Strange results in UNION
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi, UNION seems to work fine with not null values:
The strangest thing: [Query 2] breaks server
connections!!!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

282
#282 CREATE VIEW + GRANT failes
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Found in 1.7.1
If you execute the attached script and then restart the database, you get an error and the database cannot be used anymore, because the CREATE VIEW will be after the
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

283
#283 Max Aggregate Causes null row to be returned
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In the query ... the max aggregate causes a single row of nulls to be
returned when Table1 and Table2 are empty.
nathanila@hotmail.com
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

284
#284 Some problems with last alpha
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I had some problems with Timestamps in alpha_M (you
save 00:00 and retrieve 03:00 -- I'm at GMT-3 --). I've
downloaded last cvs tree and that problem has been
solved.
However, I've found new big problems:
- I can't start a server with &quot;-database &lt;dbname&gt;&quot;
command line argument. (The server always uses &quot;test&quot;)
- I can't load my old database files (created by alpha_M).
- Prepared statements can't execute DDL statements
- Problems with Binary columns: I use binary(20)
columns as Object ID. The error is:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

285
#285 Cannot use IDENTITY and PRIMARY KEY together
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When specifying a column as an IDENTITY column, it
leads to an exception when the same column is also
specified as a PRIMARY KEY:
primary key in statement [CREATE TABLE A ( ID integer IDENTITY,
PRIMARY KEY(ID) )]
This is especially troublesome when using Torque to
generate the sql code as Torque does generate both
IDENTITY and PRIMARY KEY (which is the usual
combination with other databases, e.g. MySQL).
Since in this case the column is only overspecified, at
most a warning (if at all) should be issued, but no error.
This error occurs for both the 1.7.1 release and the
1.7.2 alpha (M)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

286
#286 Error when storing a String object in a LONGVARBINARY column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When trying to store a String object in a LONGVARBINARY
column while using a prepared statement, a exception
java.sql.SQLException: invalid character encoding
is issued (1.7.2 Alpha M; a similar exception is issued
in 1.7.1). A quick look in the code revealed that it is
tried to convert a string that is assumed to contain
hex-encoded byte s into a byte array (Column:
convertObject -&gt; convertString -&gt; hexToByteArray).
However there is no hex string in the object, just a
normal string, so the conversion is not successful.
This exception is probably raised for BINARY and
VARBINARY, as well (judging from Column.convertObject).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

287
#287 nulls in prepared statements (alpha_n)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I use hsql to store Tomcat passwords. JDBCRealm seems
to prepare a statement setting a null value when the
user tries to access to a protected resource the first
time.
The cause of this bug (I'm trying to guess) is at
ValuePool:127 (ValuePool.getString(String val)). This
method is used to set Strings in
Rewritting getString like this:
seems to work.
I don't know if this problem will ocurr with all
ValuePool.getXXX() methods (I've not tried yet).
Previous versions of hsql accept null values.
Please include this bug. (I don't want to rewrite Tomcat
code.)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

288
#288 SAVEPOINT and ROLLBACK
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
and
Are not document in hsqlSyntax.html but exists in
javadoc class org.hsqldb.jdbcConnection.
Affected versions 1.7, 1.7.1, 1.7.2-alpha M.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

289
#289 NullPointerException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This happens in 1.7.1 and can be seen in DBManager or
through JDBC
--- now set auto commit off and execute step by step
--- Just to make sure I set the autocommit off before
each step since the menu doesn't show it
rollback
You get NullPointerException here
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

290
#290 Group By and Binary columns II
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
Group by is not working with binary columns in alpha_n.
Please, test this:
This was fixed previously but the bug is here again.
Cheers.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

291
#291 saveSorted ... negative seek offset
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
I'm using HSQL 1.7.1 with JRE1.3.1_03 in a Windows
2000 environment. My database contains only one table
with 8,000,000 rows which used to work fine (database
size is 1.3 GB). But when I try to create a new index or
drop an existing index I get the following error message
after some minutes:
Negative seek offset in statement [...]&quot;
where statement can be
Is there any limitation in database size, table size, row
count, or index size?
Greetings
Christian
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

292
#292 implicit DATETIME value change
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
I'm using HSQL 1.7.1 with JRE1.3.1_03 in a Windows
2000 environment. My database contains only one table
with 8,000,000 rows (database size is 1.3 GB) and an
indexed DATETIME column. After months of working fine
the following strange phenomenon occured:
Sometimes all values in the DATETIME column were
normalized to a value between 0:00 and 1:00, that is
the time was divided by 24. The date part was kept
correctly, new values were inserted correctly, too. It
seems that this occured during recovery procedure after
killing and restarting the database process.
More greetings
Christian
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

293
#293 On delete cascade
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
There is a problem with foreign key constraints with 'on
delete cascade' option.
Please try this:
The self reference of tableB works fine without the
reference to tableA.
The reference to tableA works fine without the self
reference of table B.
If both references are present, the script doesn't work.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

294
#294 PreparedStatements
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
Try this script:
(Please note there are no rows).
Why? Is something wrong?
Regards.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

295
#295 rs.getString(&quot;name&quot;) fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The often used first version works alright, however 2nd version, which is commented out, throws an
SQLException (Column name not found).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

296
#296 select avg(column) - wrong results
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When using select avg(col1) from test, null values are
incorrect counted as zero.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

297
#297 Reading database from inside jar fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
I can't get hsqldb to connect to a read-only database
inside a jar.
According to the description I created a test.jar with a
testdata directory containing
test.hsqldb.script,test.hsqldb.log and
test.hsqldb.properties.
The test jar also contains a TestInJar class which tries
to connect to the database.
I Use the url &quot;jdbc:hsqldb:testdata/test.hsqldb&quot; and call
System.setProperty(&quot;hsqldb.files_in_jar&quot;,&quot;true&quot;).
Start the test with java -cp hsqldb1.7.2N.jar;test.jar
TestInJar, hsqldb didn't connect to the database inside
the jar, but created a new database in the current
directory.
The same happened when I added all the testfiles
directly to the hsqldb.jar.
Thanks for your help
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

298
#298 Insert into Table with self FK
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I was working with a table which has FK for self.
I found bug with this kind of table managment
when inserting data with FK to self
Here is example of used table creation script:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

299
#299 Delete from table with self FK
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I was working with a table which has FK for self.
I found bug with deleting data with FK to self
Here is example of used table creation script
and insert, update row queries
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

300
#300 Delete with FK on delete cascade
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I was working with a table which has FK for self on
delete cascade.
I found bug with not correct returned number of updated
rows when deleting.
Here is example of used table creation script and
insert queries for test data:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

301
#301 short form of group by doesn't work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
after creating a little test db:
select month, year,sum(value) from costs group by 1,2
work in 1.7.1, but not in the current cvs.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

302
#302 Table not found !
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
hi all,
I am using Java,JSP and HSQLDB. I ahve used HSQLDB
as Server and given the url as
With the above connection it takes as the default
database as test, how do i mention by own database
mail to viswa@infonents.com or viswajit2@yahoo.com
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

303
#303 compute after aggregation ex) select sum(1)+sum(2)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
general error occurs when this kind of sql is executed.
and nagative function gives another error
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

304
#304 computing  within argument of aggregation function  ex) sele
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
computing within argument of aggregation function ex)
select sum(a+b) ,
S1000 general error java.lang.ClassCastException occurs
when this kind of sql is executed.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

305
#305 Error with Default Value with VARBINARY datatype
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using Alpha N of 1.7.2:
This is my table declaration:
This is the error:
error in script file line: 133 Wrong data type or data
too long in DEFAULT clause: FFFFFFFFFFFF in statement
If I put quotes around the 'FFFFFFFFFFFF' like so, then
it works, but then when I close the database it removes
them and then prints this error the next time I open it.
Dave Johansen
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

306
#306 SQLException: Column not found
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using Alpha N of 1.7.2:
When I call any of the get methods on a ResultSet
(getString(), getInt() and so on) with anything but the
column name in all uppercase it throws an SQLException.
Is this a change in the way it works or a bug? Because
lower case column names worked in previous versions of
Hypersonic.
Dave Johansen
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

307
#307 Prepared Statement param is turned to NULL
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Timestamp with TimeZone processed incorrect
hsqldb 172-alpha-n
Try test case
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

308
#308 COALESCE caused the client to hang and stack trace in server
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Version: 1.7.2 alpha N
At the client, enter:
at java.lang.Thread.run(Thread.java:536)
Ben
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

309
#309 Server died on grant select ...
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Version 1.7.2 Alpha N
The server will always die if my test.script has
grant select statements. See the attached file.
Error:
In order to circumvent this, I have to remove by hand
all of the grant select statement, and then restart
the server.
Ben
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

310
#310 PowerBuilder 9.0 can not list any user or system tables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Below is the trace from the server:
I think the problem is that table_schem and
procedure_schem are being null. I wonder if it hurts
anywhere else if hsqldb associates the table_schem
and procedure_schem with whomever created those
tables and procedures.
For example, if I connect as sa, then whatever
table/procedure I create should be stored under the
schema sa.
Ben
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

311
#311 Parametrized SQL with IN in doesn't work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Tested with alpha N.
This used to work on alpha M and before, e.g.
now I get this exception
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

312
#312 SET TABLE ADDR_TMP SOURCE &quot;tbl_addr.txt;fs=,;&quot;
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I migrated 1.7.1's data to alpha n, the server
crashed because of lines like the following:
Notice that there is a trailing &quot;;&quot;. After I manually
removed &quot;;&quot; then the server is happy.
Ben
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

313
#313 PreparedStatement failure
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm using hsqldb_1_7_2_ALPHA_N and have found a problem with PreparedStatement.
I have attached a JUnit test that exemplifies the problem. Basically the PreparedStatement is failing a query that a Statement is successfully executing.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

314
#314 ResultSetMetaData.isAutoIncrement returns false for IDENTITY
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
ResultSetMetaData.isAutoIncrement returns false for
IDENTITY columns, also in version 1.7.2_N.
From the code I understand that isAutoIncrement is a
supported feature from version 1.7.2, so I thought I
should let you know...
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

315
#315 Problem with left join returning null
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
select p.propertytextid, p.internalname,
ptc.textcontent as displayname, pe.expression,
p.defaultexpression, dt.datatypeid, dt.javaclassname from
(select descriptiontextid, propertytextid,
internalname, datatypeid, defaultexpression from
property where appliestoperson=1) p
left join
(select propertytextid, expression from
propertyexpression where objectid = 1 and objecttypeid
= 7) pe
on p.propertytextid = pe.propertytextid
left join
(select javaclassname, datatypeid from datatype) dt
on p.datatypeid = dt.datatypeid
left join
(select textcontent, textid from textcontent where
languageid = 1) ptc
on p.propertytextid = ptc.textid
This is the entire query that I want to use and it
works just fine with 1.7.1, but for some reason the
DISPLAYNAME column (ptc.textcontent) is null whenever I
do the left join with PROPERTYEXPRESSION. If I remove
the first left join (The one with PROPERTYEXPRESSION),
then it returns the values just fine.
Here's the DDLs for all of the tables:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

316
#316 Confusing error message with &quot;left join&quot;
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If you have a typo in &quot;left join&quot; then it chops off the
SQL and gives an error message that is very confusing.
I did the following:
select p.internalname, tc.textcontent from
(select propertytextid, internalname from property) p
LEJT JOIN
And it threw an SQLException with an error about
TEXTCONTENT being an invalid column. I think it would
help if it told me that &quot;LEJT JOIN&quot; was invalid.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

317
#317 In XP can't do a jdbc connect with Manager UI
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
CAN do it programmatically in XP but not by Manager
UI. I try to connect to HypersonicSQL Server with
org.hsql.jdbcDriver and
jdbc:HypersonicSQL:hsql://localhost from the Manager
UI BUT I get the following java.sql.SQL. Exception
connection is broken, network subsystem has failed.
Is it just that the version of hsql (circa 2001) just
doesn't do XP with the UI? My email is dfheinz@aol.com
thanks
Daryl Heinz
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

318
#318 Server start of 6MB database out of memory
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I tried to start a server with a 6MB database, but it
failed.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

319
#319 Metadata ignorant about new tables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I ported an hsql1.7.1 database to 1.7.2O and have some
hava code look inside the metadata for the tablenames:
The old tables show up, but when I create new tables in
the 1.7.2O DatabaseManager, they aren't listed in the
metadata in server as well as in-process mode.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

320
#320 Server shutdown failed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I can't reproduce it, but when issuing shutdown to a
Hsqldb1.7.2O server, I got the following exception:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

321
#321 1.7.2 ALPHA_O cannot read binary script file
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a database where I converted the .script file to
a binary representation (using 1.7.2 ALPHA M) using set
When I try to open this DB (standalone) with ALPHA_O I
get an exception:
When I open the DB with ALPHA_M, set logtype 0 then I
can open the DB with ALPHA_O as well.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

322
#322 Database works but stops writing to file
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am using hsqldb 1.7.1 with Java 1.3.0 on a Solaris
For some 10 days the database has obviously worked
correctely but did not write any data to the
hsqldb.script file:
We used our Java database application during this time.
The application does not hold any data outside hsqldb
and I could add and remove entries. However, the
database file date has not changed for 10 days and the
last entry is 10 days back.
Consequentely, on a restart of the database, I lost all
the data.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

323
#323 Left join - no result
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I used standard left join query.
But this was not working if in ON clause is
another condition
Example:
No result !!!
Of course I expect that there will be result like:
I also found similar reported bug which
is already closed:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

324
#324 Cannot specify a column type + IDENTITY together
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The documentation for CREATE TABLE states:
columnDefinition:
with the following error message:
Wrong data type: ID in statement [...]
The following statement works, despite that it does not follow the
grammar rule:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

325
#325 NIO IllegalArgumentException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The database using NIO would try to access
a file beyond the length of the buffer causing
an IllegalArgumentException. The code missed
the corner case when the seek position was
equal to the file length. Attached is a modified file
that fixes this. I no longer get the
IllegalArgumentException.
Also, I took the liberty of expanding the data file
linearly instead of exponentially. This seems to work
in the testing I've done, and it cuts down on disk usage.
Thanks for a great product!

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

326
#326 1.7.2 ALPHA N+O: Query on SYSTEM_SESSIONS throws SQLExc.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I guess theres a problem with the SYSTEM_SESSIONS
table.
Heres a short description of what I did:
1. User 1 connects to a database (served by
org.hsqldb.Server).
2. User 1 disconnects.
3. User 2 connects to the same database.
4. User 2 posts statement &quot;SELECT * FROM
SYSTEM_SESSIONS&quot; which results in the following
(copied from DatabaseManagerSwing)
Udo
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

327
#327 possible DatabaseManager confusion
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I noticed a special case which can confuse the
DatabaseManager (1.7.2P) and possibly also destroy a
database.
When copying a database from one directory to another,
I only copied test.script, but forgot about the
test.properties file.
When I tried to open the database with the
DatabaseManager no tables were shown, the existing
test.script was ignored and the command &quot;checkpoint&quot;
or &quot;shutdown&quot; would even overwrite the existing file.
This behaviour is new to 1.7.2P (wasn't in 1.7.2N).
Before I also used to copy .script files around and rely
on the DatabaseManager to recreate a
default .properties file as needed. So I guess some users
might get very confused about this changed behaviour.
I suggest improved exception information (e.g.
test.script found but no test.properties). (would also be
good if a properties file is missing is missing inside a jar
file - see other bug report).
I guess a question like : &quot;Shall I create a default 1.7.2
properties file&quot; would be ideal.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

328
#328 maxlength of varchar is ignored
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

330
#330 NullPointerException using IN where NULL values in column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Version:
How to reproduce bug:
First create the table and insert some integer values:
The following select runs fine:
Now insert a null value:
The following select fails:
So I guess you have to do a check for null before
comparing the data in the table with the values in the IN
statement.
I know this worked before in 1.7.1, as my code failed
trying to switch to the latest alpha.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

331
#331 NullPointerException with VARCHAR and PS
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I do the following PreparedStatement:
select TEXTID from TEXTCONTENT where TEXTCONTENT = ?
and LANGUAGEID = ?
I get this NullPointerException when I call
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

332
#332 rs.isBeforeFirst() should return false
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
rs.isBeforeFirst() should return false when ResultSet is
empty.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

333
#333 Transactions and closing server from dos bug!
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

334
#334 wrong results with join in ALPHA_Q version
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This is how to reproduce the problem:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

335
#335 select with where on unique indexed column doesn't work well
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm working on 1.7.2 alpha_M (I have problems with
version alpha_R - I wrote about it in the forum)
The test case is:
I have server.properties file with:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

336
#336 create user with jdbc
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
With 1.7.2 R the following jdbc prepared statement
&quot;create user ? password ? admin&quot;
throws SQLException &quot;parameter index out of range: 1&quot;
when trying to substitute the first ? with name using
setString(1, &quot;name&quot;)
This was warking correctly with 1.7.2M and before
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

337
#337 select date/time with jdbc
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
our table is
Do following in java
now try to select the date back using following code
The result set returned is empty
If I would set the exact time I set when I inserted the
date, the result set would contain the record.
The same happens if I want to insert time and then
select it with different date into a TIME column
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

338
#338 comparison with LIKE and Pstmts fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
hi there,
(see below sample code). it seems that &quot;col LIKE ?&quot; is
broken in the current release. the below code normally
should return a row, but if I use LIKE the it does not.
the result of the comparison is always false. if i simply
use col = ?, it works as expected. seems to be a bug.!?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

339
#339 where column in (select
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Given a table
with content
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

340
#340 hsqldbmin.jar looking for HSQLClientConnection
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Trying to run embedded hsqldbmin.jar, and getting:
This comes from src.org.hsqldb.HSQLClientConnection
HSQLClientConnection is not included in hsqldbmin.jar - it
does implement SessionInterface which is included. A fix
for embedded mode is:
I don't have the background to know if this might have
any bad side effect in server modes.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

341
#341 Deletion of .backup file causes db corruption w/JDBC
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I deleted a .backup file (isn't this permitted?). Then I
opened the db using JDBC (DriverManager.getConnection
()). This results in a &quot;database in use&quot; error, and the db
is corrupted, having 0 bytes.
This sounds like a serious bug, if it is a bug. At any
rate, the system should never blow away the entire db?
Thanks,
Dave C.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

342
#342 New bug in Select/Like on primary key
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Working with HSQLDB 1.7.2 - S
Following DDL:
If i'm doing the following query:
select * from additionaldata where function like 'G%'
the ResultSet is empty. This is definitly wrong!
Using only
select * from additionaldata where function like '%'
the whole table will be shown (this is ok)
Removing the &quot;Primary Key&quot; constraint from the DDL in
the &quot;function&quot;-column causes the SQL-queries to
perform properly.
Any ideas about that? I'm quite sure, that the version of
HSQLDB I tried before does not behave like this.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

343
#343 LIKE error in v. 1.7.2_S
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi!
I created a simple cached test table:
and an index to DESC column.
I execute a simple Java program to fill the table with
random numbers (200,000 records). No problem.
I execute this select:
I try also with LIKE '-57966%07' and don't works again.
HSQLDB is unable to find the record.
Thanks,
Francesc Ross
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

344
#344 (ALPHA_S) Can't compile HsqlDateTime with earlier JDKs
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
One can't compile HsqlDateTime.java with JDK1.3 or earlier
because it uses Calendar's setTimeInMillis(long) and
getTimeInMillis() methods, which were protected rather than
public until JDK1.4 (I think it was). Equivalents, though less
efficient, in earlier JDKs would be Calendar.setTime(new
java.util.Date(long)) and
Calendar.getTime().getTime().
FWIW, I've attached an
updated HsqlDateTime.java and build.xml which use
codeswitcher and a wrapper method to use the more efficient
methods when JDBC3 is defined, the less efficient ones otherwise.
Using a wrapper makes the code simpler (though not so pretty) and
the optimizer should toss the overhead. Used the 'tempDate' static
member that was lying around seemingly unused (made it private).
Haven't done regression on it, no warrantee.
Obviously,
I'm not a developer on the project and throw it away if you like. It's
just FWIW.
Regards,
T.J. Crowder
tjcrowder
at users
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

345
#345 Errors in Text Tables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
There is a problem with writing and then reading text
tables.
The problems are related to the field &quot;SystemId&quot; feature.
Seems like the text tables were written without the
last column, and when reading,
the extra column &quot;SystemId&quot; was created.
When the last column type was not the same type of
SystemId (long, i guess),
a ClassCastException was thrown, and the server hanged
(as the client).
I studied and fixed this problem in 2 methods:
Without this (or other fix) to resolve this issue,
using text tables with the
full life cycle (attaching, updating/inserting, closing
bd, reopening, reading)
will be impossible.
The connection hangup was due to the fact that we were
not expecting a
ClassCastException in class ServerConnection, method run().
I think a catch(Exception e) statement should be add
after the existing
one for IOException and SQLException, to avoid this
problem.
Best Regards,
Carlos Silva
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

346
#346 1.7.1 zip missing hsqldb.gif in src/org/hsqldb/util
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After using ant to build the standard jar or any jar
including Swing, the DatabaseManagerSwing utility will
fail to load, on a getIcon statement in CommonSwing.
Upon examing the source, I see the .gif mentioned is
missing from the distro. I found one on the web, put it
in hsqldb/src/org/hsqldb/util, and reran ant (the ant file
does mention this gif explicitly for inclusion). This fixes
the problem (still a bug though, until the distro zip is
fixed).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

347
#347 .lck file created for read-only database
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Each connection to the resulting database still creates a
&lt;database name&gt;.lck file, preventing it to be put in a
CD-ROM, for example.
This has been tested on 1.7.2 alpha N, R and S versions.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

348
#348 number of records
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello!
I'm writing from Palermo(Sicilia).
Thanks for your hsqldb database that i'm using for a web
application with servlet,jsp e javaBean.
The version that i use is 1.7.1 but i have found a bug, i
think.
I use this method to find the number of records of a
ResultSet :
if records are 4 this method return 4 and in ResultSet
there are 4 records but if i use while loop to print
records, it prints 3 records if i use before this method,
and 4 records if i don't use this method before
I use this ResultSet :
Associazioni.Key order by key desc&quot;);
What can i do to find number of records without to use
this method ?
Spero di essere stato utile
Ciao e grazie.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

349
#349 LIKE bug
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If you create a simple table with version 1.7.2 alpha R :
then this first SELECT statement returns this last
But this one don't, though it should !
This seems to happen when you use any string function,
... returns this row.
However when you include another string function the
problem reappears:
There was not any problem when I used the 1.7.2 alpha
N version. Alpha T still have this bug.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

350
#350 alter table... add default not working
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm running the following command in the DatabaseManager:
alter table k alter column vc add default 'dfltval'
It returns &quot;Unexpected token: ADD in statement
version A T.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

351
#351 saveSorted ... negative seek offset (2)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
I'm using HSQL 1.7.1 with JRE1.3.1_03 in a Windows
2000 environment. My database contains only one table
with 8,000,000 rows (database size is about 1 GB) and
three indexes on single columns. I had a former problem
with creating resp. dropping an index which didn't work
because of an
&quot;File input/output error: saveSorted java.io.IOException:
Negative seek offset in statement [...]&quot;
error (see bug submission 780397 from 2003-07-30). I
solved this problem by creating a new database and
copying the data row by row.
Now I have a more serious problem: The database can't
start anymore because of the same error (though I used
a backup which has been successfully compacted
before). When I remove the &quot;CREATE INDEX&quot; statements
from the script file I get InputStream-errors (because
the index roots aren't valid anymore), when I
remove &quot;CREATE INDEX&quot; and &quot;SET TABLE INDEX&quot;
commands I get no errors but have no access to the
data anymore (though the database size stays the
same).
No my questions:
Is there another workaround for this situation?
How can I manipulate the &quot;SET TABLE INDEX&quot; command
manually so it doesn't lead to the errors described above?
Will it be possible to use an index spanning more rows in
1.7.2?
Greetings,
Christian
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

352
#352 Create View fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This sequence
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

353
#353 .new files not removed when using relative path
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If I create a connection to a stand alone database using
a relative path, when I close the connection the
backup.new and script.new files do not get removed.
The next time I try to connect to the database and then
disconnect I get the following exception.
Example:
if I use the following URL: &quot;jdbc:hsqldb:db/da_database&quot;
the .new files don't get removed.
But if I used: &quot;jdbc:hsqldb:C:/javaProj/Draft Assist
2.00/build/db/da_database&quot; it seems to work fine.
using hsqldb 1.7.1 on Win2000
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

354
#354 't' should be interpreted as 'true' for bit columns
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
inserting the value 't' into a bit column is
interpreted by hsqldb (1.7.2 alpha T) as 'false'.
Example :
create table test (isOk bit);
insert into table test (isOk) values ('t');
The row will now have the value 'false'.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

355
#355 Corrupted .script file using DatabaseManager
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Generating a database with about 300 tables using
ScriptTool
generates the database files .properties and .log. After
using
the DatabaseManager and closing it the data went
from .log to
the .script file but seems to be corrupted.
The DatabaseManager changes the IDENTITY attribute of
the primary key columns to
&quot;GENERATED BY DEFAULT AS IDENTITY(START WITH 0).
Using this database in my application leads to the
following error:
Questions:
Is the &quot;GENERATED BY DEFAULT ...&quot; entry of
DatabaseManager actually
intended / needed?
How can I avoid this DatabaseManager behaviour? Note
that the data in the .log file seems to match exactly my SQL statement
without &quot;GENERATED ...&quot;?
I'm using version 1.72 alpha m.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

356
#356 JDBC methode &quot;getScale&quot; always returns zero
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
There is a bug in connecting of hsqldb and Druid
(druid.sf.net).
Because of the &quot;not correct/not complete&quot;
implementation of the &quot;getScale&quot; methode
of &quot;jdbcResultSetMetaData&quot;. Durid use this methode and
handle the colum as an Integer and not a decimal.
For more information about the druid bug see:
&quot;[ 838413 ] Exception with JDBC HSQLDB&quot;
or:
So please correct the implementation.
Thanks a lot!
Jochen
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

357
#357 TestSelfQueries fails on 1.7.2_ALPHA_T
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I got 3 failures (2 errors) when running the Self test,
The first one is:
This is mentioned in the file as:
but it seems like the fix didn't take.
I built hsql Windows 2000, JDK 1.4.2_02. Did I do
something incorrectly when I built? (I have included
the entire test output)
Thanks.
Chris
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

358
#358 Does a Constraint have an Impact Over the Returned Rows
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
There are two tables - TABLE_A AND TABLE_B and their
definitions are:
it returns 1 (for identifier=uidtns) row instead of 2 (
doesn't return for identifier=procedure)
if you delete all rows from table_a where process_id != 9
(leaving two rows) it still returns one row
If you shutdown the server and restart it only then it
return both rows, when there are just 2 rows. It does
not work when there are all 20 rows.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

359
#359 Problem with clob
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We use hsqldb 1_7_2_ALPHA with Hibernate.
In jdbcPreparedStatement.java in method
setCharacterStream.
At this line.
Sometimes for some reasons. The reader is already at the
end of the stream.
MY PATCH
Thank you if you add this line in the next version. Now
we use your jar, we apply the patch and give the
patched jar to the developpers.
Jean-Franois Nadeau
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

360
#360 CASEWHEN(SUM(some_filed),some_value) -&gt; Exception
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Any CASEWHEN-type function with any aggregating
function as parameter causes to General error.
The reason is that in this case
Expression.getAggregatedValue() requires special
algorithm for value retrieving, but it missed. Possible
solution is to add the code below to the first switch{} in
getAggregatedValue() method:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

361
#361 1.7.2 alpha T is not compatible with JDK1.3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I ran a simple test using a 1.3 JDK, but the JDBC driver
will not load because of a dependency on:
Which was introduced as of JDK 1.4.
Here is the exception:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

362
#362 DatabaseMetadata.storesUpperCaseIdentifiers() returns wrong
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Method storesUpperCaseIdentifiers() on the
DatabaseMetaData class returns true. But when I am
searching tables with DatabaseMetaData.getTables I have
to use lowercase in the table name.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

363
#363 ResultSet.isBeforeFirst() does not work as specified
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following javadoc code snippet is from JSDK 1.4.2:
before the first row;
position or the
AFAK, this works as the specification for the JDBC API.
Thus, when I have an empty ResultSet, the first call of
the method should return false, but obviously the
return value is initialized to true without checking
the ResultSet, if it contains any rows.
The ResultSet was generated with getTables(...) from
the database's metadata, and I wanted to test if a
given table already exists or has to be generated.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

364
#364 Error in jdbcDriver / getPropertyInfo
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I've found an error in jdbcDriver.java, while I was
looking in the source for some information about the
method getPropertyInfo (what I can expect from it):
The properties 1..3 are all assigned to pinfo[1],
overwriting the values 1..2 by value 3.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

365
#365 Warnings
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Compiling the latest alpha release iin Eclipse gave the
attached file of warnings. Mostly uneeded imports and
static methods/fields access.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

366
#366 Assert Failed inside of Hypersonic
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I get this error when issuing these queries on the
latest alpha version of 1.7.2 and I'm not sure why.
com.tallgroup.framework.ocs.listeners.FileListener$1.run(FileListener.java:329)
If you need any more info or some test data just let me
know.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

367
#367 Unexpected token
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Sorry to submit again, but I forgot to login in the
preceeding submission. Hope you don't mind.
I got unexpected token when I tried to connect to the
attaches database. You can launch runManager to
reproduce the error ( stand alone engine)
Can you help me ? Thanks a lot.
visual
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

368
#368 bad cache logic
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Bug scenario:
I have obtained <TurnDetails> object (result) from
database using something like:
conn is database connection that is created on program
init and persists all program lifetime: static Connection
First time object <result> is obtained correctly, all
works fine. I am changes several object's fields during
program lifetime and, after all, I want to get the
original unchanged object, that is stored in database.
Hovewer, code described above returns me OLD object
with CHANGED fields (in fact, there is no difference
between object that was manipulated during program
lifetime and returned object).
Here is workaround for this problem: if I close and
reopen database connection, i'll get fresh unchanged
object (but only first time. Each time I want to get
FRESH UNCHANGED object, I should close/reopen database
connection, and this is too slow). Seems, that
connection caches objects and simply returns me old
object in spite of the fact that it was changed in memory.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

369
#369 Null-value update causes unrelated contraint violation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a schema that works well in hsqldb 1.7.1 but fails
miserably in 1.7.2RC1.
I execute the following statements
The last statement causes SQLException shown below.
Notice that the values updated on the last statement is
identical to the values being inserted!
Now if I execute
It works!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

370
#370 1.7.2 is 25% slower than 1.7.1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In my test of schema consisting a few tables and
references (cascade delete), the performance of 1.7.2
is about 25-30% slower than 1.7.1
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

371
#371 In  .log file, some command lines are corrupt
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Here are the first few lines of the log file.
SET AUTOCOMMIT FALSE
Is there a bug in the line printer? Seems similar to
the problem I saw awhile ago on the forums (seeing "T"
or some other character) at the beginning of a log file
line. This was from a database that was not cleanly
shut down.
This was on a two CPU machine, Solaris, Java 1.4.0_06.
I saw the same problem on Linux on 1.4.1. Perhaps
there is a synchronization issue?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

372
File input/output error: *.backup in version 1.7.1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm currently using the HSQL database engine 1.7.1 on
and an embedded XP environment using Sun's 1.4.1 Java VM.
On some occasions when a hard boot has occured (while the HsqlDB (running as a server) is running with a couple of connections possibly still open to it, but idle), I've found the database files to be corrupt upon reboot (not only the *.backup file as the captured event below indicates, but also *.script and *.data files)
The *.properties file always seems to indicate 'modified=yes' after boot up.
After reboot, the following exception is thrown when the
DB is started up again after a hard boot -
Use SHUTDOWN to close normally.
Use [Ctrl]+[C] to abort abruptly
Is there anything I could do to prevent this from occuring?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

373
#373 RC1: ROUND makes an exception
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In construction like
the query returns correct result if sum() returns some
value. If sum() returns null then "General error:
If round() removed and sum() is null, the query returns 0
as expected
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

374
Error in Server.start()? (1.7.2 RC1)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello
This little example code does not work as expected in 1.7.2 RC1
The server does not stop. It seem the start() returns
before the server is fully started. The javadoc for the
start method clearly states:
it blocks only until the server's background thread notifies the calling thread that the server has either started successfully of failed to do so.
IMHO the start method should block until the server is completly started.
Futhermore I would have expected the start() method to throw an exception if unable to start.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

375
#375 .script file contains wrong CREATE TRIGGER statement
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have run the following statements on a newly created
database:
The trigger was create successfully (no error, and it
was visible in SYSTEM_TRIGGERS)
Shutdown and re-connect to the database gives an error.
A quick look at the .script file shows that the
following script is generated:
Note the missing space in "CREATE TRIGGERMY_TRIGGER"
which fails thus the database cannot be opened.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

376
#376 Weird exception when column not specified
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This weird exception happens when I don't specify the
column FREQUENCYOFUSE.
Here's my DDL:
If I specify the column FREQUENCYOFUSE then it works
just fine.
Let me know if you need any more information or if I'm
doing something wrong.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

377
#377 Indexes are mixed up when adding index
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When a new index is added to a table WHICH ALREADY
CONTAINS DATA, the data structures that implement
the different indexes are mixed up with each other
(cross-linked). In our case, the result was that data
slowly disappeared from the table because the indexes
became corrupted when data was updated. In one lucky
case, we got an assertion error after several updates to
the table.
The attachment contains a fix in the form of a diff which
can be applied to revision 1.72 of
org/hsqldb/Table.java . It seems to be good also for the
(at this time) most current version 1.87.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

378
#378 Internal Error : Invalid Compiled Statement ID
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
My testsuite of about 600 tests encounters this error
several times.
Invalid Compiled Statement ID:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

379
#379 Embedded Interserct doesn't work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
THis didn't work on 1.7.2 alpha M and on RC1, works on
sapdb
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

380
#380 Embedded Union doesn't work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
THis didn't work on 1.7.2 alpha M and on RC1, works on
sapdb
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

381
#381 Bug with Clob functions
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
guillaume.nodet@deliasystems.com
jdbcClob's functions getSubString() and position()
handles index badly. The spec says the index should
begin at 1, but they are coded as if they are handled at
0
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

382
#382 getTimestampString(Timestamp x, Calendar cal)  bug
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
The getTimestampString(Timestamp x, Calendar cal)
method in org.hsqldb.HsqlDateTime class have x's
milliseconds part added twice. See the following code
for current implementation and the error place.
sdfts.format(new java.util.Date(x.getTime() +
x.getNanos() / 1000000));
It is not necessary to add x.getNanos() /1000000.
x.getTime() already includes the millisecond part. I
think the author thought x.getTime() just return the
integral seconds part.
For example,
take timestamp 2004-01-19 23:59:59.999 (from UTC+8) as
x and calendar with UTC as cal, the return value is
2004-01-19 16:00:00. The correct returned value should
be 2004-01-19 15:59:59:999
Regards,
Rice
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

383
#383 OutOfMemoryError when telnetting to the server.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello
If I start the server at telnet to it using "telnet
localhost 9001" and write "aa<enter><enter>" [1] in
the telnet prompt. Then the server will throw an
OutOfMemoryError.
Granted this is not ordinary usage. But I think a
server should be able to handle malformed requests
without running out of memory. The server should
somehow detect that the client is bogus and ignore it.
[1] do not type <enter> but hit enter...
Btw this is on 1.7.2RC@jdk 1.4.1_03
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

384
#384 setMaxRows on preparedStatement
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
The setMaxRows does not work for a prepredStatement. I
had a look at the code for jdbcPreparedStatement and
saw that the executeQuery() method never updates the
maxRows on the resultOut(Result) instance. The same
goes for execute().
Cheers Jaco
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

385
#385 res: urls for JWS do not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In the file HsqlProperties.java, there is a method:
public static boolean checkFileExists(String
fileName, boolean resource,
the problem is, cla.getResource translates package
names into directories, so trying to open:
jdbc:hsqldb:res:dir/mydatabase
Would result in this class trying to check if
dir/mydatabase.properties
exists, which in turn is translated as
dir/mydatabase/properties
Which obviously does not exist. Note that this problem
does not happen if the URL starts with / (something
that is not correct, since the classpath is relative,
but is needed to disable the package translation
mecanism). The proper solution would be to call
cla.getClassLoader().getResource(). You don't even have
to pass cla as an argument, since
Thread.currentThread().getContextClassLoader() also works.
Another two points:
1.- In the example jnlp file that you provided, there
are some properties especified in doc/databaseinjar.txt
to be put in a jnlp file as an example of use
<property name="jdbc.drivers"
value="org.hsqldb.jdbcDriver"/>
problem is, JWS does not allow (without signing the
jar) to System.getProperty() anything that does not
start with "jnlp." or "javaws.", as
<property name="jnlp.jdbc.drivers"
value="org.hsqldb.jdbcDriver"/>
2.- Didn't check that too much, but it looked as if
checkFileExists is called too often. IIRC even calling
to Connection.close() does trigger a call to this one.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

386
#386 Inserting binary datas fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a problem trying to insert a File in a
LONGVARBINARY field.
Here's the code I use to insert:
I have the same problem when using the
setBinaryStream methos.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

387
#387 getMetaData() and getColumns(null, null, tableName, null)...
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
There is a case sensitivity issue with
the code works, but nothing happens...
(using 1.7.2 RC1).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

388
#388 2 now generates 2 different times
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have not tried the latest version, just up to alpha
M, but when I have statement like
insert (......) into ... values (.... now, now)
(the statement contains multiple now calls) in certain
situations (probably depends on timing) the times
returned by now are different and therefore the insert
puts two different values into the database even though
the intent is to put two
of the same values there.
Not sure if this is a bug, but it is definitely little
unpexpected so I am bringing it up to hear your opinion.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

389
#389 PreparedStatement.getParameterMetaData()
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HSQLDB 1.7.2 RC1
Hi,
PreparedStatement.getParameterMetaData() is not
implemented.
Could you provide a dummy implementation to avoid:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

390
#390 Delete query doesn't allow conditional statements
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I've attempted in every possible way to issue a delete
query that looks for <statement> and <statement> and
it always gives java exception errors. The same
conditional statement works fine in the search or update
queries.
Perhaps I dont have the right syntax? But i've tried
almost every SQL variation and then some.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

391
#391 correlated update results change
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi
In the 1.7.1 the following worked fine:
update waits set operation_seq = (
select min(o.operation_seq)
from db_operations o
where o.line_no > line_no
and o.cursor_no = cursor_no )
With 1.7.2 operation_seq is not updated, but the update
count is correct.
Has the syntax for the correlated update changed?
What is the correct syntax?
The SQLServer and Oracle formats that i tried caused
sytax errors.
thanks
trevor
test case:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

392
#392 UPDATE failure
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
version 1.7.1
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

394
#394 float precision lost &lt;k@kylemiller.com&gt;
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This is a problem in 1.7.1 and release candidate 1.7.2.
if you have a float like 9.3, and call set float on a
prepared statement the number saved in the db is
9.300000190734863. I looked in the source and the
setFloat method on prepared statement is just a pass
through method to setDouble. The problem is when you
cast a float to a double the precision changes, as in the
problem above.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

395
#395 null value not accepted in IN (..) clause
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
null values are not accepted in IN (..) clause.
This would be very helpful for me, because I
programmatically create long IN-clauses in which null
might occur. Having to change it all to
where xyz IN (....) or xyz=null would require some
programming effort.
create temp table testtable (xx integer);
insert into testtable values (1);insert into testtable
values (3);insert into testtable values (null);
select count(*) from testtable where xx in (1,3,null)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

396
#396 CURTIME() problems
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm having a bunch of problems when trying to use the
CURTIME() function in the 1.7.1 release.
This always returns 'bad' for me.
Since the BETWEEN expression covers the entire 24 hour
period, I believe any time zone differences, if they existed,
would be irrelevant and this would always return 'good'.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

397
#397 CURTIME() not normalized
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This is related to #897591
version 1.7.2
Since this doesn't normalize the time to Jan 1, 1970 whereas
other time values are normalized, CURTIME cannot be used
reliably in comparison operators.
I believe curtime() should look more like:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

398
#398 different results in 1.7.2 rc1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
hi,
i just migrated an older cocoon application with hsqldb in
server mode from cocoon 2.0 to 2.1 rc. i also decided to
try hsqldb 1.7.2 because it's new features look very
promising to me. the first problem i a came across was a
very curious EOFException when trying to acces one
special table of the database (i simply took the old
database files which worked very well until then). but
there is nothing special about the stucture of this table
nor it's content. looked very random to me. after playing
around with the transfer tool and various different hsql
versions i was able to use a version 1.6.2 sql dump of
my database and (after modifying table creation to get
cached tables again) open it without any hassle with
version 1.7.2 hsql server.
now the dbms seemed to do it's job but i realized an
enormous performance drop compared to the older
database system. it looks like a communication problem
because the db server starts outputting my sql
commands (silent = false) seconds to minutes after my
servlet is activated.
but the biggest problem is, i'm now getting different
results compared to the old system. i didn't check the
results of single queries yet but i can clearly see that
the application (kind of multi dimensional scientific
search engine) is just producing crap with hsqldb 1.7.2.
to sum it up it seems like additional constraints i throw in
are not reducing the amount of results i get but enlarge
the result set a lot. the app itself is used for almost 2
years now without any modification to the sql
statements or code so i consider it stable.
sorry for staying so vague but i didn't find time to take a
closer look at the problem yet. i did all of the system
design and programming years ago and the whole thing
is very complex so i don't know exactly where to start
when it comes to debuging. if you could give me a hint
where (i mean for what kind of query) you changed code
that could possibly couse different results i would try to
check the corresponding statements and results in detail.
just a last point - i went back to version 1.7.1 as
shipped with cocoon 2 and everything's working perfect
again (including import of my old database files).
kind regards
momo
p.s. i'm using jdk 1.4 for all java systems including hsqldb
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

399
#399 CHECK CONSTRAINT column not found
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Version 1.7.2RC1
Following SQL generates error:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

400
#400 CHECK CONSTRAINT follow-up
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I was in such I hurry to create the example in 900350 I
messed up the SQL; let me try again. The error I was
trying to report was:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

401
#401 Conflict with Turkish Regional Settings
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In windows environment with Turkish regional settings
and character set, there occurs some errors.
For example,, whereas the column ISIM exists:
The cause of the error is the difference between turkish
and english character set. In Turkish the capital of 'i' is
not 'I' but '&#304;', which is an I with a dot.
Hsqldb converts all the words in an sql statement to
upper case. This is done according to the default locale.
So this conversion produces unrecognized characters for
hsqldb database.
The solution to this problem is, setting the default locale
as Locale.ENGLISH in hsqldb. Or using toUpperCase
(Locale.English) method, instead of toUpperCase().
I hope this problem might be resolved in the future
versions of Hsqldb.
Thanks for great work and effort...
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

402
#402 update with correlated subquery does not evaluate correctly
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
"update tableX set colY = (select colY from tableY
where tableY.colX = tableX.colX)" throws "Single value
expected" exception if tableX.colX contains NULL.
Without the null row, update works corectly.
With null row, update works on Oracle and breaks on
HSQLDB 1.7.1 and 1.7.2rc1.
Please see attached Java code, try to run it with and
without line 52:
insert into table2 (col1, col2) values (null, 5)
commented out.
Dima
dmaziuk@bmrb.wisc.edu
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

403
#403 LIKE error in RC2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Abstract:
Observe this is not a memory table bug. The same
behaviour appears for example in a cached table.
I think is a very important bug near the final 1.7.2
release.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

404
#404 LONGVARCHAR(size) no longer works
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This worked in the older version of HSQLDB, but no in
1.7.2 RC 2:
Is there a reason for that change?
this works:
Thanks,
Thomas
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

405
#405 Unwanted output to System.out
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In 1.7.2 RC2 there are sometimes messages written to
System.out. This should not be so. Imagine a console
application that needs to write to System.out...
Please disable this messages by default.
Thanks,
Thomas
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

406
#406 Data loss in TEXT tables and ClassCastException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
A problem concerning current release V1.7.1.
The problem does not appear in 1.7.2 RC 2.
But I've found nothing about it in forums (known bug)
I found loss of datas in TEXWT tables files after a
checkpoint. A ClassCastException appears after
restarting and selecting data, see the case below.
then it is impossible to shut down the database
and in properties files there is modified=yes.
And in the table_zone.dat, there is only :
1,ZONE1,HOUSE
And nothing about the 4th column !
Regards,
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

407
#407 org.hsqldb.Library.monthname bug
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

408
#408 problem with negative long values
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The attached testcase produces an error where a
newative long -9129298924866545938L is stored to a
BIGINT column and becomes 922310382 when it's
retrieved.
Sorry, but the testcase requires Hibernate 2.x in your
class path.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

409
#409 gigabyte limit
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
While inserting data into 1.7.2 rc2 - I run into batch
update
expections which I believe are caused by java.nio expanding
the data file another gigabyte when the data file is
already a
gigabyte. java.nio has a two gigabyte limit. Java version
1.4.2_03 on windows xp home addition. Thanks
-Tim
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

410
#410 Null pointer exception after crash
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello
I had a JVM crash during creation of a database. The
JVM crash was unrelated to hsql. When starting the
database again I get this exception in the prompt. I
found that the [Database].properties file was created
but empty (0 bytes). I suggest adding more error
tolerant reading of the [database].properties file.
Ps. I am using 1.7.2RC2
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

411
#411 Text tables marked as read only corrupt script. 1.7.2RC2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It appears there is a bug when creating read-only text
tables.
Steps to reproduce:
1. Create a new db
2. Connect and issue the following commands:
CREATE TEXT TABLE FOO(BAR NUMERIC NOT NULL)
SET TABLE FOO READONLY TRUE
SHUTDOWN
3. Observe in the script that the CREATE TEXT TABLE
command is duplicated.
4. Starting the DB fails with "error in script file".
Also, SHUTDOWN COMPACT fails due to the same issue.
BTW, this worked in 1.7.1. I haven't tested other 1.7.2
releases.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

412
#412 Concat behavior col1+':'+col2 error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When running
create temp table testtable (col1 char(2),col2 char(2));
insert into testtable values ('xx',null);
select col1+':'+col2 from testtable;
HSQL1.7.1 just returns null.
HSQL1.7.2RC2 sends an error
Wrong data type: For input string: "xx"
(SQL State=37000, Errorcode=-16)
took me a moment to figure out the reason.
Maybe the ideal thing would be to return
xx:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

413
#413 Updates ignore check constrain
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Check constraint added to columns work when doing
inserts but fail (are ignored) when doing updates. -
1.7.2 RC2
Example
This example runs and wrongfully inserts a erroneous value.
drop table prospects_may_04 if exists;
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

414
#414 sum(col) returns a Long not an Integer
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When a select sum(col) query is issued on a column
which is integer a long is returned.
Code can crashed when people update to 1.7.2 and use
casts.
For large sums Long is good of course, but then people
could be using BigInt datatype for such columns.
If you keep the returning of longs, you could make a big
announcement in the readme.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

415
#415 IndexOutOfBoundsException on update
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
With the current 1.7.2-rc2 release, I can reproduce the
crash described by the stack trace below. The exception
is thrown when updating a single row in a table.
The environment is Windows 2000, J2SDK1.4.2. hsqldb is
being used as the back end to an Orion application
server, and is running in server mode. The table being
updated is defined as CREATE CACHED TABLE.
If someone contacts me I can package up a database
and send you the statement to provoke it. Running the
statement from the manager utility reproduces the
problem. I'd prefer not to post the database or the
contents publicly.
Cheers!
Jon
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

416
#416 NPE on Constraint.replaceTable
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi.
In RC 3 (and previous, I would assume), it is possible to
get a NullPointerException when atering a table,
presumably when it has certain CHECK contraints. I
haven't had the time to do a thorough test of all cases,
but here is the one that caused the NPE for me:
create table parent(id int primary key, name varchar);
create table child(id int primary key, parent_id int, name
varchar, constraint fk_child_parent foreign key
(parent_id) references parent(id), constraint
check_child_name_is_not_null check(name is not null));
alter table child drop contraint fk_child_parent;
After running a short debug session, the apparent cause
surfaced in the Constraint.replaceTable method, where
the name of the main index is used without checking if
the main index exists for the contraint. For the
check_child_name_is_not_null constraint, there is no
main index, thus an NPE is thrown.
Here's a quick patch.
Fred: please verify that this is all that needs to be
done. It's been too long since I've poked about in
Constraint.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

417
#417 RC3 Select INTO using prepared statement is broken
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Was dingling about with PowerBuilder 9 and HSQLDB
today when I noticed that a "select ...into..." issued
from the SQL console failed to create any table and
instead fetched a result set.
Upon probing a bit, it turns out that PowerBuilder
prepares the statements issued on the console before
executing them....hmmm.
Anyway, as of RC3, the "INTO" part of a prepared select
does not get executed, only the select part.
After a bit of time spent today, I have a tested and
working patch for this.
It will be part of my next release into CVS, due by the
end of this weekend.
Actually, this is a cool feature, now that it is working,
since it is one of the few "prepared" DDLs that can
accept parameters.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

418
#418 CallableStatement set by name broken
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
My Fault.
Bad loop and failure to create the lookup map.
Fix will be like this:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

419
#419 HsqlDateTime.java switch to 1.2 error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HsqlDateTime.java version 1.26
some methods has direct access to a Calendar instances
P.S.
should be fixed in attached file
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

420
#420 SQLException for Empty batch
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I get a "java.sql.SQLException: Invalid argument in
JDBC call: Empty batch" when I do a jdbcStatement.executeBatch().
I think it's an error to throw an exception in this
case (the batch is empty). See also
Version "hsqldb_1_7_2_RC_3".
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

421
#421 ArrayIndexOutOfBoundsException on select union
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Problem with select statement, releases 1_7_2_RC_1,
No such problem with release 1_7_1.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

422
#422 HSQLDB 1.7.2 RC4 with JDK1.3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using JDK1.3 on Solaris, i'm unable to build the jar :
It seems that BaseHashIterator sould be
declared "public".
Thanks.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

423
#423 IndexOutOfBoundsException while loading large table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Bug detected while loading up a huge table with lots of
foreign keys, etc....
It runs nearly to completion (running for maybe 5
minutes), and then throws this. Not sure why. This is
in RC_3
I'm using SQuirreL around it, it is the standalone
version loading up from a script file, here are my
properties....
will try to reproduce on RC-4. The total size of the
data file is about 500 MB, script is around 20 MB
compressed. Hope this helps.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

424
#424 Problem with VARBINARY type
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I just noticed that there's a problem with the
VARBINARY data type using the current Release
Candidate. It drops the last character if the inserted
string is less than 10 characters.
Here's the DDL:
If you need any more information just let me know.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

425
#425 strange spaces occur with update in textfile for Table.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I update a Table (refering to a Textfile), my
textfile gets screwed up:
(the dots are spaces). I get a lot of lines filled with
spaces and it seems I have as many spaces as the last
line is long.
properties:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

426
#426 RC4 - Assert Exception in standalone mode
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi!
I'm running a JDK 1.3.1 application perfectly fine with
HSQL 1.43 and HSQLDB 1.7.1.
But with 1.7.2 RC4 i have the following exception 1 or 2
seconds after star:
Assert failed: java.lang.Exception
This only occure in standalone mode.
Server mode works fine.
This working fine if my application is single threaded.
As soon as more than 1 thread access the database,
then this exception occure...
Note: I'm using multiphtreaded prepared statements.
And non-prepared statement are synchronized within the
application...
Looks like a thread safety issue since the exception is
unpredictible and occure randomly in different parts of
the application.
Hope this helps...
Georges
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

427
#427 Text Tables FK problem
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
There is a problem with using Text tables as parent
Foreign key table. The HSQLDB throws an Integrity
constraint violation when attempting to insert into a
table that refers to a text table unless the text table is
previously "selected".
I've attached an example to demonstrate the problem.
The example basically does the following:
1) create database tables. 2 tables: one cached table
USER and one text table USER_TYPE. This is sourced by
user_types.csv. The USER table has fk constraint to
USER_TYPE table's unique index on USER_TYPE_CD.
2) attempt to insert into USER table. It fails.
3) select * from USER_TYPE
4) re-execute same insert statement: it succeeds.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

428
#428 TestCacheSize
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
TestCacheSize results for Windows 2000.
Ran 40000 and 1000000 cases in expected time.
1500000 case was aborted at 18 hours (expected time
2 - 4 hours).
Since the 1.5 million rows case took so long, I haven't
attempted the larger cases.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

429
#429 Insert-Select problem in RC4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This jdbc sql query have problem.
step A: create table
It looks like jdbc will set null value not constant value 1.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

431
#431 Strange Exception message on own library functions
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am working on a library function that can throw
IllegalArgumentExceptions.
The method itself is working and can be called from hsql
(after GRANT ALL ON CLASS "de.test.Library2" TO
PUBLIC).
However, if an IllegalArgumentException is thrown it is
preceded by
Unknown
Now I know what's going on, but users might be
confused by the false "Unknown Function"
package de.test;
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

432
#432 NullPointerException in Connection.close(), RC4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I obtained NullPointerException during connection-close
() method:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

433
#433 hsqldb 1.7.1 on Mac OS X
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We are using hsqldb 1.7.1, on Mac OS X(10.3) with JDK
1.4.1 bundled. Our program starts the hsqldb using
Server Mode, and connects to the hsqldb using jdbc.
Our open connection codes are,
We have traced into the program and found that every connection is closed.
We use "netstat" on Mac OS X and found that every
connection will create one record and it is not released.
But on WindowsXP, jdk142, although every connection
also create one record in "netstat", the above error
message will not be displayed.
We don't know if the many records in "netstat" are
correct, or if there is any bug.
We're very urgent to know the solution, and we
appreciate your answer.
Thanks.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

434
#434 Building HSQLDB with j2sdk 1.5.0-beta
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I tried to build the package with
I received following error
variable named enum is used in file Server.java but enum is
a keyword in java 1.5.0
I think it is easy to correct
Jacek
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

435
#435 1.7.2rc5 breaks SQL statements working in 1.7.1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
rc5 rejects my string query on empty strings:
AND phonew like ''
And it also complains about aggregate grouping.
(Detailed sql to follow)
I have been using hsqldb since 2000, and have not had
to modify the sql statements in my application since
2001. The application survived the upgrade to 1.6 (not
sure about the exact version number) and 1.7 without
me having to touch the sql statements.
Perhaps my sql was not compliant with ansi-92 to begin
with, and rc5 is simply pointing out my original errors. I
do not know if this is the case, or whether these are
actually bugs in rc5.
To reproduce the Error 1:
Changing the GROUP BY statement to the following fixes
the problem:
GROUP BY Tickets.custid, Customers.firstaname,
Customers.lastname
My little applications had survived two major version
upgrades without sql modifications. It would have been
nice if the same would have been true with 1.7.2. In
any case, keep up the good work! :)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

436
#436 CREATE USER & getConnection() inMemory mode
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
With In-Memory mode I try to create new user and
connect. Why it does not work?
and SQLException is catched which logs:
User not found: JUKU
When I query SYSTEM_USERS then JUKU is there!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

437
#437 getColumns behavior changed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The behavior of jdbcDataBaseMetaData.getColumns(...)
has changed in 1.7.2.
With 1.7.1 I get an empty resultset back, although I
expected to get the column list for my table.
I found a workaround is to call getColumns like this:
However, this breaks my existing portable database
code.  Shouldn't 1.7.2 support wildcard for the schema
name?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

438
#438 wrong constraint name in violation exception
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I create a table with a single column and a named unique
constraint on that column:
Occurs both in 1.7.1. and 1.7.2 RC5
I would like to retrieve the name of the violated constraint
somehow fron the SQLException thrown on the violation.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

439
#439 "400 Bad Request" when proxy server changes http headers
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am trying to connect to a hsqlldb database using the
http protocol jdbc:hsqldb:http://servername across the
internet.
My ISP has some sort of proxy server in place. The http
headers should arrive at my hsqldb server as ....
Which causes a "400 Bad Request" to be returned.
I cannot connect to my hsqldb server without going
through this "device" - www.netapp.com has details of
the device - but I suspect it's just one of many.
This occurs with RC5.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

440
#440 User defined ALIASes and built-in aliases
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello! With built-in aliases, I can construct with AND
and OR as many built-in aliases together into one CALL
statement as I can and no exception is arised. But when
I CREATE my own ALIAS, it can not be constructed with ISR
Is it bug or feature I do not know. We can use CASTing,
but still would be nice to not to do it.
Tanks in advance!
Andre
Version 1.7.2 RC5 is used.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

441
#441 Time not accepting HH:MM
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When a table has a field with type TIME, it didn't
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

442
#442 getDatabaseMajorVersion fn -> error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Not exactly a show stopper, but I thought I'd mention it gave Abstract method exceptions when I tried to use them. I'm using getDatabaseProductVersion() instead,
with no problems.
HSQLDB version 1.7.1
regards
Iain
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

443
#443 Order by bug of 1.7.1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We are using hsqldb 1.7.1, on WinXP with JDK
1.4.2. Our program starts the hsqldb using
Server Mode, and connects to the hsqldb using jdbc.
Our open connection codes are,
The table to be retrieved is,
The corresponding externalized SQL is,
At the first time, this method could retrieve all the six
records from the table PP_QUESTION_PARTS, but the
second time it is called, only four of them are retrieved,
and the following records are lost,
I'm very sure that every actual SQL is the same each
time, It's so strange all the later retrievements lost the
same 2 records.
Also if I changed the record to let PART_NUMBER in the
sequential order like follow,
Then every time the result is correct.
Is it a bug?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

444
#444 hsqldb RC2 onward fails to start over nfs with jvm1.4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
From RC2 onwards, hsqldb has an error on startup if the
database is on an nfs file system and jvm 1.4 is used.
Hsqldb starts OK using jvm 1.3, or with RC1, or if the
database file is on the local disk.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

445
#445 1.7.1 client hangs trying to connect to 1.7.2 server
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I accidentally had an older 1.7.1 hsqldb jar in the
classpath (before the 1.7.2RC5 one).
The client was just hanging for ever when trying to
connect to a 1.7.2 server.
It would be helpful if some exception was thrown like "A
hsqldb 1.7.1 client cannot connect to a hsqldb 1.7.2
server"
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

446
#446 1.7.2RC5 - CALL IDENTITY() broken?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I tried out 1.7.2RC5 today and was unable to use my
application to insert data into tables that use IDENTITY
columns.
MY application uses prepared statements of the form:
In this case there is a CUSTOMER_ID column of type
IDENTITY. My program fails on the this line after the
SQL INSERT has successfully executed:
The error message is "function not supported"
I've reverted to 1.7.1 and my program is working again.
I didn't perform any 'upgrade' on the database itself, as
far as I can tell this isn't required.
Regards
Iain
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

447
#447 CREATE VIEW and USER() function problem
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I'm currently doing an evaluation of databases to select
one for a new project. Whilst evaluating your very
impressive database I have noticed a problem.
When creating a view such as ;
When connecting using SA and performing the SELECT
statement used in the VIEW command I got the SA row
as expected. When I CREATE the VIEW and do a
SELECT * from the VIEW I get no rows at all. After
further investigration I found that the SELECT within
CREATE VIEW is run as user SYS. Which means I can't
create a VIEW based on the currently connected USER.
This causes me a problem and is different to every other
database I have seen which runs the CREATE VIEW
command within the context of the connection.
Is there a workaround to this or can it be fixed ?
Thanks in advanced for your help.
Matt Shaw
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

448
#448 National characters in text table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I tried hsqldb text table but national
Czech characters are stored as '?'.
In other tables are stored in <database>.script
as escape constant.
-- for the same non-text table
Ufak
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

449
#449 ResultSet.first() throws exception...
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
A call to ResultSet.first() throws the following exception
when the ResultSet is positioned before first row:
I can see why a call to ResultSet.first() should result in
an exception if the ResultSet is positioned AFTER the
first row. But not while it is positioned either before, or
at the first row.
This is clearly a bug, and I'll need to change quite a bit
of ResultSet.first() to ResultSet.next() to get my code
to work. Well, it's doable I admit :-) But hey, I shouldn't
have to :-/
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

450
#450 Compatibility issues from Linux to Windows?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using 1.7.1:
I have created a DB instance on Linux. I checked in to CVS (as
binary) the .data and .script files. A co-worker on Windows (using the
same commandline) gets an error (see attachment) when trying to
start the DB and then the .data file is zero'ed out. I have spent
several hours combing through the web, but couldn't find anything.
What are we doing wrong?
What's interesting is that if we delete the index creation lines from the
.script file (which produces the output seen in the attachment) the
error doesn't occur, but the .data file still gets zero'ed out.
Any help would really be appreciated!
Thanks.
Jason Rogers
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

451
#451 Setting a byte[] of zero length throws exception
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I f I use a blob datatype with zero length size of the
array an exception is thrown.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

452
#452 *.data and *.backup not deleted
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I had one cached table in my database but eventually
dropped it.
I noticed that the data and backup file aren't
deleted or at least set to zero size even after
checkpoint and proper shutdown.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

453
#453 Test file for the "NOT unknown" issue
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This file contains tests in the TestSelf format.
These tests are not complete and may be improved.
Regards
Massimo
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

454
#454 server not starting
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
i'm starting a database and i get the following output:
NIO next buffer.force()
NIO next channel.close()
NIO next file.close()
System.exit() is called next
i already restored the .data file from the .backup and
the error persists.
i'll upload the database
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

455
#455 buildJDK12.bat fails to include some files in jar
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The buildJDK12.bat script that comes with RC5 fails to
include the following directories (from /classes) in the
resulting hsqldb.jar file:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

456
#456 Nightly tarball contains files with weird names (on Windows)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
All the files in the nightly tarball available from the
homepage have ",v" appended to the end of their names. Example:
I'm using winrar 3.30 (latest version released on 2004)
on WindowsXp to uncompress the .bz2 file.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

457
#457 "create view" fails when having identical column-names
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
in statement
the statement above works with DB2 und Oracle
it works as well, if you only do the SELECT:
Workaround :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

458
#458 Trigger fire callback data is incorrect
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I've been doing some work with triggers. The BEFORE
UPDATE FOR EACH ROW and AFTER UPDATE FOR EACH
ROW don't seem to be passing the correct data.
According to the documentation Row1 and Row2 should
contain the before and after data but Row1 seems to be
the data to be updated with and Row2 is null.
I tested this in my own code and also with the
TriggerSample class and got the same result.
I need both sets of data to make a decision in the
Trigger.
If this is a bug could the other trigger types be checked.
Thanks
Matt
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

459
#459 Updated trigger data violates table constraints
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
When using Triggers I noticed something which I believe
is a bug.
If you use one of the BEFORE FOR EACH ROW triggers
and modify the data that is about to be inserted into the
database the data that is inserted isn't checked against
the table constraints i.e. NOT NULL or similiar.
I found this while trying to acheive a veto of a change
request. I basically nulled the data and expected the
database not to insert the new data or throw an
exception or something instread it quite happily inserted
a null row even though all the columns were NOT NULL.
Cheers
Matt
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

460
#460 ArrayIndexOutOfBoundsException when creating indices
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I get the following exception when running the SQL
statement:
in my HyperSonic database.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

461
#461 EBCDIC platforms not supported
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The hypersonic engine is not handling character sets
other than ascii. Running on the z/OS platform causes
invalid characters to be written to the script file. There
are places in the codebase where ascii-to-unicode, and
vise versa, conversions are being done. I believe none
of this should be necessary if using Reader/Writers. An
example would be in the java.util.Properties source,
which reads and writes to a text file and is platform
independent.
The log reading is correctly using a "BufferedReader(new
InputStreamReader(dataStreamIn))", but the same is not
true for writing - and is directly using a
FileOutputStream instead of a "new BufferedWriter( new
OutputStreamWriter(fos, "8859_1") )" as is done in the
base Java Properties class.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

462
#462 CREATE TABLE with integer DEFAULT seems to fail
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The latest CVS code produces an error for the following
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

463
#463 Default column value of 'now' no longer work in create table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In 1.7.2_RC6 the following syntax no longer works in a
create table statement:
This did work in 1.7.2_RC5. In RC6 I had to update my
sql scripts to remove the single quotes from around 'now'.
Not sure what is expected behavior, I just wanted to
report it in case using 'now' in a default column value
should work.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

464
#464 RC6: switchtojdk12.bat followed by buildjdk12.bat fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I downloaded/unzipped rc6, and immediately did a swtichtojdk12.bat (successful), followed by a
buildjdk12.bat (fails). I am running jdk1.4.2_04 on this machine to build it.
Output from buildjdk12 below:
HSQLDB build file for jdk 1.2.x and 1.3.x
*** we recommend the use of the ANT build.xml instead
of this method for all jdk's include the path to jdk1.x.x\bin in your
system path statement
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

465
#465 Database won't shutdown after creating Triggers
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
If you create triggers using statements such as ;
CREATE TRIGGER AUDIT_BEFORE_UPDATE_TEST BEFORE
UPDATE ON TEST FOR EACH ROW CALL "AuditTrigger"
and issue the SHUTDOWN command, from another
connection, with no or any of the SHUTDOWN
arguments the command hangs the Server and doesn't
shutdown.
I have tracked the problem down.
In Server.java, the code;
is looping forever as the activeCount is always equal to
number of triggers that you have created.
Unfortunately I'm not sure how to fix it.
I have removed all my code from the fire() method on
the trigger to eliminate that.
This is obviously fairly serious as it means you can't
cleanly shutdown the database if you use triggers.
Cheers
Matt
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

466
#466 java.sql.SQLException: File input/output error: saveSorted
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I use hsqldb_1_7_1. It happend only once during soak
test and i don't know why.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

467
#467 Unexpected results using LEFT JOIN
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I've tested the SQL below on several other databases
and got the 'Expected Results' (see below). HSQL
returns an unusual row containing NULL values (see
'Actual Results').
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

468
#468 SELECT NEXT VALUE FOR <seq>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
select next value for <sequnce> does not work. Return
the following error. Also there is not a NEXT_VALUE
column in the SYSTEM_SEQUENCES table for the sequence
that I created.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

469
#469 Wrong handling of non HSQL URLs
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Try the following code (alone in a main, do not try to
include the Oracle driver to the classpath):
I get an exception:
This is erroneous since the URL is not a HSQL URL
anyway (you can put anything you like in the url, such
as real urls of other database drivers, except valid
HSQL URLs of course). Other drivers (Oracle, mySql,
MSSQL) do not throw an exception in that case because
the URL is not valid for them. With all these drivers,
I get this exception:
which seems much more normal.
To my mind, this is an important issue since it can
mask other (real!) exceptions from other drivers.
Actually, my initial problem was while trying to
connect to an oracle database, I had made a mistake in
my oracle URL but got the HSQL exception instead of the
oracle one because the DriverManager tried HSQL first
(and I understand that the DriverManager cannot choose
between two drivers which raise an exception)!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

470
#470 setObject and setBoolean don't map a Boolean to a int field
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
According to JDBC (table 8.9.5 on page
setObject and setBoolean should map a java Boolean to
an int field. But HSQL's JDBC driver seems to simply
output the boolean into a literal "TRUE" or "FALSE" and
will trigger an error when trying to insert such values
into a int field.
The following test shows the error:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

471
#471 NPE in ServerConnection.signalClose()
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When multi-threaded connections, the runnerThread
variable may be null.
Perhaps can we swith the test to avoid this case like
this :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

472
#472 insert..into..select..from..group..by bug(?)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
There are no rows in tbl1, but a row with null values
gets inserted into tbl2 anyway. Not sure if it makes a
difference, but the jdbc url is "jdbc:hsqldb:."
This is with 1.7.2 rc6b. Please let me know if further
info is needed.
Thanks,
Ron
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

473
#473 7.1.1 can't create table name matching keyword :-(
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Can't use this SQL to create a new table.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

474
#474 getPrimaryKeys maybe broken
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
if I call
for this table
nothing is reported.
I do not know if this is correct or not.
Bye
Lorenzo
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

475
#475 Wrong results if combining COALESCE, CAST and BETWEEN
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
if I submit the following query as PreparedStatement it
will always return all rows of Table1. The statement
always ignores the passed java.util.Date parameters.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

476
#476 Bad tmp directory for DatabaseManager
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
On Linux when I run as a non-root account and try to
start the DatabaseManager without any options:
After some investigation it seems there is a bug in
Since I'm not root, I can't write in the root dir (/)
and it fails. Need to change everywhere getTempDir()
result is use to create a file name to add a directory
separator in the ConnectionDialogCommon class.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

477
#477 Can't compile with sun jdk 1.3.1_09 under windows 2000
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Please,
I am obtaining the log above when a try to compile
BUILD FAILED
failed; see the compiler er
ror output for details.
Total time: 14 seconds
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

478
#478 Integrity constraint violation when delete all rows
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It is not possible to delete all rows from tables where a
column references the primary id of the same table (tree
structure stored in the table). Problems occurs with the
latest release candidate: hsqldb_1_7_2_RC_6b
Example of the table:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

479
#479 INSERT trigger does not fire
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I tried to register an insert trigger with hibernate using
create trigger i_role before INSERT on role for each
row call 'InsertTrigger'
This trigger (which currently only consists of an
System.out.println() statement) does not fire. It has
the signature
synchronized public void fire(String arg0, String arg1,
Object[] arg2)
When I create an UPDATE or a DELETE trigger, the
trigger method fires. The InsertTrigger class exists
and can be found in the classpath.
Any help is appreciated!
Sincerely,
Sven Herschel
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

480
#480 IndexOutOfBoundsException after crash
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following error occured when tried to open a
database that was open when we turned off the machine
it was running on.
We are running on Linux Java Desktop and accessing the
database through a java application through the JDBC
connection in hsqldb.jar from RC6 (30 may).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

481
#481 connection in stored proc. doesn't know URL
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a stored procedure that makes use of the
automatically passed java.sql.Connection.
Trying to call getURL() on the MetaData results in a
NullPointerException.
add something like to org.hsqldb.Library
and execute call "org.hsqldb.Library.testProc"()
to reproduce the problem.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

482
#482 jdbcDatabaseMetaData getTables bug
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The problem is that when something creates a table whose name is
NOT all upper case and then uses getTables to check for it's
existence
the result will fail even if the name is correct
jdbcDatabaseMetaData needs to call toUpperCase on all table
names before adding them to the SQL.
The whole point of JDBC being that you don't have to know the
peculiarities of a given db.
An even better solution would be to support lower case db names
but I'm assuming that won't happen any time soon.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

483
#483 SELECT EXCEPT EXCEPT problem
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have query which contains two EXCEPTS, the example
looks silly but it is simplification of our real world query.
The query returns different results in hsqldb and in
sapdb and it seems that it is because of order of
evaluation.
THe idea of the last select is (123) - (12) - (23) so I
would expect empty set. Thats what I get in sapdb. In
hsql rc6c I get 2,3. Is this a problem?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

484
#484 dbvis shows ghost hsqldbs
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This is against 1.7.2Rc6c, dbvis 4.1.1 on JDK 1.4.2_03
1. Create a connection to a pre-existing embedded HSQLDB
database.
2. Look around (open some tables)
3. Disconnect from the database.
4. On file system, in location of DB, note that the DB.lck file is still
there. This should have been deleted by hsqldb.
5. Delete the database files (DB.lck, DB.log, DB.script,
DB.properties)
6. Go back to DBVIS and Connect to the database.
7. Look at the previously-viewed tables. You will see the old views
from the deleted database, rather than an empty table list.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

485
#485 bit column does not work with "in" operator
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Against hsqldb 1.7.1:
I have a column that's of type "bit":
the following SQL statement does not work:
It also does not work with a prepared statement with
setBoolean:
Thanks
Moh
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

486
#486 Error in script file line: 13 out of memory
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Strange message, on DB starting i got an exception
My db files are not to big, db.script looks good also,
what happend i don't know. The same problem after
switching to RC6d.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

487
IndexOutOfBoundsException while creating index
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a database running in my application that I use
to store about 2 million records in. My table is
created with the following command:
The table is created successfully, and is fully populated with data after some time passes. After it
is done populating, I do a commit, then want to index the table, using:
The first time I run it, it crashes with an index out of bounds exception ( below ). If I run it again, it
fails. If I index the table before adding records to
it, it works, but is slow. If I use a smaller dataset,
it succeeds.
I've looked through the issues in the bug reports, and
it looks like a similiar bug was reported back in May (
well, maybe not a bug, just a dain-bramaged user ).
I'm using the default configuration for hsqldb... no
special parameters or anything. As a lark, I broke my
database in half ( each with the same tables defined in
them ). Again, the indexing fails on the first
database, but succeeds on the second. I figured it
might be a timing issue with records not being fully
written, but even blocking the code for 10 minutes
before attempting to index doesn't seem to help.
Ideas?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

488
#488 java.sql.SQLException: File input/output error: d.backup
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi there.
I m getting File input/output error. Right now I m
having 4 files in my database i.e. d.backup, d.data,
d.properties and d.script. Last time when I started my
database it was running successfully. but when I was
shutting it down at that time, the power goes off, and it
got corrupted. Right now d.data file is of 0 KB. (Zero KB)
and d.backup if of 11 MB. 'modified=yes' in d.properties
file and 'hsqldb.cache_version=1.7.0'. To my wonder,
d.script file is also containing d.properties file's
statement at the beginning with one exception
is 'modified=no' and after that it shows some non-ASCII
characters. I think that non-ASCII character is
converted by hsql for my CREATE table statements
cause I can not see first 3 CREATE TABLE statements in
d.script and after non-ASCII characters it shows rest of
the CREATE TABLE statements.
Please help me out to recover the data from the
database as it contains almost 300000 records.
Thanx and Regards,
Heet Patel
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

489
#489 cannot open database in jar when compressed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I tried to open a database which is inside a jar file.
When the scriptformat is in regular text mode, it works
fine, but when I set the scriptformat to compressed, the
database cannot be opened.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

490
#490 setFetchSize
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello, my email is fulvio.biondi@ingeniumtech.it.
I'm waiting for registration confirmation but, in the
meantime
I wish to submit a small problem with the 1.7.2 rev.
Regards
Fulvio
The setFetchSize() method throws NO_DATA_EXCEPTION.
I believe the fix should be :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

491
#491 Sub-select in WHERE clause of DELETE broke after 1.7.2RC6b
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following statement executes just fine in 1.7.2RC6b,
but throws an exception in 1.7.2RC6d and 1.7.2 Release:
The exception I get is:
JRE version is 1.4.2_05.
I'll apologize in advance for wasting your time if this
turns out to be a "stupid user" bug.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

492
#492 column name become uppercase
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
execute the following code :
you can get column name from rsmd ,but the column
name bacome "A"(uppercase),not "a",is this a bug?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

493
#493 Unnamed constraint causes error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following queries produce a "Constrain already
exists" error in the latest CVS:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

494
#494 hsqldb.files_readonly=true broken in final release ?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
just updated from 1.7.2 RC4 to the final release
(1.7.2.2) and HSQL doesn't work as I was used to.
I have a database, just defined by the two files
test.script and test.properties.
test.properties is the default properties-file, that is
generated, in addition there's the line
hsqldb.files_readonly=true
When I now connect to the database as usual:
both files test.script and test.properties are
overwritten, the line hsqldb.file_readonly vanishes.
When I insert new data they get added to the test.script.
This is not the behaviour that appeared in RC4 !
All my junit-test don't work any more, because they
always need a prepared database, when a new connection
is established, but all changes should only be done in
memory
I checked the behaviour again with SquirrelSQL just
switching jars, not changing anything else.
In RC4 everthing worked fine, in the final Version
script and property files are changed.
Here are the files in use:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

495
#495 CURRENT_TIMESTAMP SQL function generate a NullPointerExcepti
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Version of hSQLdb is from zip file hsqldb_1_7_2_2.zip
available on sourceforge.
1)
Executing the following create table statement in SQLTool
create table b ( dd datetime default CURRENT_TIMESTAMP );
generate a
2)
generate a
and SQLTool does not respond anymore
Thanks !
david
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

496
#496 v.1.7.2.2: Error while creating more than 1 CACHED table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I try to create two CHACHED tables in your Simple
Code Example testdb, following code:
Only table "sample_table" is created in database,
sample_table2 code generates this exception.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

497
#497 small typo error in html user guide
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Chapter 2. SQL Issues->Sequences and Identity ->Sequences
The table name for the sequence is missing an s.
( You can query the SYTEM_SEQUENCES table for the next
value that will be returned from any of the defined
sequences. The SEQUENCE_NAME column contains the name
and the NEXT_VALUE column contains the next value to be
returned.
SYTEM_SEQUENCES -> SYSTEM_SEQUENCES
)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

498
#498 1.7.2.2: HSQL can't read its own script files
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
to reproduce on a fresh database:
then reconnect to your database, and you'll get the
so the leading 0 of the timestamp is missing.
You should either be more strict when saving timestamps
or not as strict when reading them.
I know that this is a quite uncommon timestamp, but I
have a database where timestamps are used as
primary-keys thus heaving no other special meaning ...
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

499
#499 INSERT INTO SessionInfo VALUES (NULL, ?); CALL IDENTITY()
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Is this a bug? I'm using 1.7.2.2 In 1.7.1 this worked.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

500
#500 Server can't start after special CREATE statement
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The bug is reported against the current HSQL-DB release
1.7.2.2.
Start a new database server i.e. by an windows batch
file containing:
The database abc is now generated.
Then connect to the database using the Database Manager
and execute the statement:
No error is reported. Then shutdown the database via
SHUTDOWN.
Now the server can't be restarted via the upper windows
batch file. The error message is identical to the upper
error message thrown during the first shutdown.
I would expect:
1. If the CREATE statement is wrong, it should not be
accepted! An error should be thrown on client side.
2. Otherwise the server MUST be able to correct an
error in the database definition itself. It MUST be
possible to start the server.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

501
#501 hsqldb.files_readonly=true dioesn't have any effect
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
According to 1.7.2 documentation,
I'm setting
in my default.properties (before starting the DB of course)
and observing that this has no effect.
After debugging this problem a little bit,
I discovered that in class
org.hsqldb.HsqlDatabaseProperties
at the biginning there in the static section there
several sets of allowed properties defined, and
files_readonly is not one of them.
So, what happenes is that inside
org.hsqldb.HsqlDatabaseProperties.filterLoadedProperties ()
this property (along with quite a few others) gets
filtered out before seeing the light of day.
I don't really care about the property. I just want to
run the database in such a way that all the changes to
the DB are not logged to .log file and are lost when
the databse is closed. Is this possible?
Thank you,
Yuri
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

502
#502 avg() returns integer value for integer columns
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Creating a table
and inserting values
avga is returned as BIGINT therefore the correct result
2.333333 is casted from double to integer giving 2.
Also the avg of integer-type columns should be returned
as float-type.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

503
#503 Concatenating strings in query
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Concatenating strings in SQL-query results in output null
IF one of the strings is null.
Ex select (STRING_DATA1 || STRING_DATA1 2) from...
if either STRING_DATA1 or STRING_DATA2 is null, the
resulting output will be null.
This problem didn't occur in 1.7.1
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

504
#504 Temp tables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If you have many connections, it is possible to create temp tables
for each session (with the same name) without any conflict.
How-ever, if you close one connection, then all of the temp tables
are dropped for all of the sessions.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

505
#505 left outer join - bug with index
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
left outer joint breaks as soon as an index is created
on a column in the where clause.
-- the query doens' return a single row (ok)
--now we create an index and run the same query again
-- this time the query erroneously returns rows !
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

506
#506 1.7.2.4 cannot read script files from 1.7.1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I create a table using:
In the script file, 1.7.1. renders this as
Shutdown sequence completed in 0 ms.
It seems that older versions generate a script file with
attributes in an order that the newer version does not
support. This makes it impossible to upgrade.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

507
#507 Error when using functions in ORDER BY
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am testing Cayenne ORM with HSQLDB. The following
query runs fine on 1.7.1/Linux/JDK 1.4.1, but fails on
1.7.2/Mac OS X/JDK 1.4.2:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

508
#508 jdk1.3+hsqldb 1.7.2.4, DatabaseManager StreamCorruptedExcept
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
we use jdk1.3.1+hsqldb1.7.2.4, if we run command :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

509
#509 problem with ResultSet.getMetaData
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When trying to call
I get a NullPointerException, when working in a Library
function (1.7.2.4).
add the following procedure to org.hsqldb.Library and
execute call "org.hsqldb.Library.test"();to reproduce.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

510
#510 Statement.setMaxRows(int) restricts inner queries
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Statement.setMaxRows(int) restricts the number of rows
returned by a select statement in inner queries like
'INSERT INTO test1 SELECT * FROM test2'.
The JDBC documentation says: "Sets the limit for the
maximum number of rows that any ResultSet object can
contain to the given number. If the limit is exceeded,
the excess rows are silently dropped." Therefore only
the queries that return a ResultSet should be affected.
My release number is 1.7.2.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

511
#511 hsqldb startup problem
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I try to startup the db in code. (In a new Thread) Most
times it starts up ok but sometimes it startsup but
doesn't listen on the port it's supposed to listen.
Below is the code i use to start the server.
Hope you can help me out.
Regards,
Wim Heijboer
wheijboer@triennium.com
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

512
#512 HAVING missing into the documentation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HAVING works fine but doesnt appear in the
documentation :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

513
#513 jdk 1.4 requirement due to java.sql.Savepoint
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Has a dependency on JDK 1.4 been introduced?
specifically, the class java.sql.Savepoint is new to JDK
1.4.
for additional details, see bullet [1] at
-markus
markuskhouri@hotmail.com
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

514
#514 1.7.2.4 LEFT OUTER JOIN joins to many rows
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
the LEFT OUTER JOIN on a column joins also rows of the
joined table if its column is null.
Example:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

515
#515 Unexpected token with "ORDER BY"
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I've got a small table with 3 columns. When I use the command
I've received the error
If I remove the ORDER BY-part, everything works fine.
Here's the create-command:
At the moment, there is only one row in this table.
Michael
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

516
#516 NullPointerException after restarting of HSQLDB
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After some restarting of my server which uses HSQLDB
internally I got an error when I try to start server
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

517
#517 Function not supported
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi.
Following code lead to SQLException :"Function is not
supported":
Btw, this code works on Hypersomic 1.7.2 alpha M release.
Test example attached.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

518
#518 connection (not really) broken - memory problems
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When trying to retrieve a rather large resultSet (about
500.000) from a 1.7.2.4 database, I get a
java.sql.SQLException:
However, the connection wasn't completely broken.
Executing another select with a small resultSet worked
just fine.
Instead the cause of the problem was insufficient
memory.
Assigning some more memory with -Xmx caused the
problem to disappear.
I did this on my SuSe Linux 9 (512MB RAM), my Windows
XP computer with (768MB) didn't need a higher memory
assignment.
It took me quite a while to figure this out.
If the memory use is justfied and not the bug itself an
outOfMemory-Error or at least
would be helpful.
If you need more details - let me know.
Thanks,
Meikel
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

519
#519 1.7.2 failed to restore DB after the application crashed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
hi all,
I have this message in my error.log.
extra:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

520
#520 fails to add foreign key on quoted column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This problem shows in 1.7.2.
A message appears saying that a token is not expected.
It seems that quoted column names are not fully
supported. I also had a problem where the column
names in the scripts written by HSQLDB did not quote
the column name even when the column name was a
reserved word or case sensitive (eg 'check').
We are creating columns to represent fields with case-
sensitive names over which we have no control, so
sticking with upper case names and avoiding key words
is awkward.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

521
#521 first values are used for all formula in select
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
all computed columns have the same value as the first one
assuming a table tbldesc:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

522
#522 Metadata.getColumns gets very slow after a while
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi all,
I discovered a bug which seems not so critical but
which is critical for me...
Try this algorithm:
for 0 < i < thousands of times
perform a getColumns()
create a table Ti
drop Ti
After a while, the getColumns() call gets slower and
slower. You have a code example joined to this bug
report. Actually, it seems that the getColumns
indirectly forces the recreation of the SYSTEM_COLUMNS
table, and that this recreation is slower and slower,
just as if it was following back all actions that have
occurred on tables since we connected.
I'd really like this bug to be corrected since the only
solution for now is to relaunch my app each time to
reset everything is HSQL... or go back to version 1.7.1
which doesn't have this bug. :-(
Thanks!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

523
#523 HAVING clause doesn't work properly with particular case
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Engine doesn't return result if HAVING clause contains
expression about aggregated value and non-aggregated
value.
this is sample SQL occurs this phoenomenon.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

524
#524 jdbcUtil.notSupported is broken
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
whenever hsqldb wants to report a "not supported" condition, it throws a prefabricated exception instance, known as "jdbcUtil.notSupported".
However, the stack trace reported by it is the stack
trace when the jdbcUtil.<clinit> executed and not the stack trace of the call that throws the exception.
Therefore, it is completely useless for locating the place of the exception.
I'd recommend you to remove jdbcUtil.notSupported and replace all occurrences of "throw jdbcUtil.notSupported" throughout the code with so that correct stack traces are being generated.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

525
#525 Error in reflection class name in 1.7.2.4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In RowInputBase.java
I found a non existing class reference:
This should be corrected to:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

526
#526 text tables not completely honoring database location
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This test creates two databases, one in /tmp named foo,
the other in /tmp/bar/ with no extra naming information.
Database files for the second test appear as
/tmp/bar/.properties, etc...
The second test should create the test.csv file as
/tmp/bar/test.csv, but it creates the table as
/tmp/test.csv. This confilcts with the schema created
in /tmp named foo, which also has a test.csv file.
Discovered in 1.7.2.7
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

527
#527 TOP does not work with UNION
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The TOP selection in a select statement does not work
when connection two selects with UNION.
I am using the latest stable version 1.7.2.7.
To reproduce the bug, use the following SQL-statements:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

528
#528 select with group-by no longer works
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a select that was worknig on 2003.12.28 RC_1/1.7.2
but when I upgraded to 1.7.2.7 it reports an error.
More detail on new version: 'Oct 2004 zip named -1.7.2.7'
aka "latest release 5 of HSQLDB 1.7.2" from readme file
The SQL:
I do not know of another way to form the sql. It looks
valid to me.
SUGGESTIONS? HELP!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

529
#529 NIO IndexOutOfBoundsException thrown from NIOScaledRAFile
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm not really sure how to reproduce it. But today we
got the following exception trying to connect to brand
new (all db files have been removed) database.
About this db, it's using 1.7.2 with cached tables.
Any ideas why would this happen?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

530
#530 can't run on jdk 1.1 NoSuchMethodError java/io/File.deleteO
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi ,
I try to run the HSQL engine Version 1_7_2_[7/8] on a
zaurus with the evm JVM (JDK 1.1).
To build the hsqldbz.jar I use ant with switchtojdk11
and jarzaurus.
If I try to run hsqldb I got this exception :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

531
#531 getTime/getDate/getTimestamp problems with Calendar
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
getTime(int, Calendar) does not work.
getDate(int, Calendar) does not work.
getTimestamp(int, Calendar) ignores Calendar all together.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

532
#532 parsing of user defined function args
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a user defined function with a String constant arg.
Example:
parsing the function args causes an IOException in
when it hits StringConverter.hexToByte(String).
If I change this to
return (String)o;
everything seems to work as expected.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

533
#533 conversion from Float to int
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The method Column.convertToInt(Object) is failing for
Objects of type Float.
The resulting problem can be experienced with this
sample code :
Results in :
java.sql.SQLException: Type Conversion not supported
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

534
#534 curdate trailing space
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
curdate() seems to be returning a string with a trailing
space.
This doesn't return any rows
select sdate , curdate() from mytable where sdate =
curdate()
Whereas this does:
select sdate , curdate() from mytable where sdate =
rtrim(curdate())
sdate was created as follows:
java.sql.Date jDate ;
The result looks okay e.g. "2004-11-30"
I'm using version 1.7.1 but I've not seen this mentioned
in the change log as a fix.
email address alan@terapin.f9.co.uk
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

535
#535 Nullable column rendered non nullable by check constraints
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
-- num_code is rendered non nullable by the check
constraint applied to table KEN
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

536
#536 java.lang.IllegalMonitorStateException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
/home/federico/Eclipse Projects/Java Corporate PostOffice
Java Corporate Post Office is starting...
I developed a class that extends TimerTask. This class
has a "begin()" method that initialize an internal
hsqldb instance. The code can be found here
http:
This code uses a server instance of hsqldb, so I have
my personal database and tables structure and data is
saved on filesystem
When starting the application, I have to create the
empty db structure, BUT, with version 1.7.3 (and even
1.7.2.9) i receive the following error:
I saw the error comes from line 1260 of Server.java that is
and, at a first glance, it is working fine
Hope this helps
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

537
#537 Error closing db.data (IOException)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I am using HSQLDB embedded within JBoss 3.2.6
(Windows 2000, JRE 1.5.0), which is, I think, 1.7.2.4.
I got a strange error while running the server:
I really do not have any idea where this error comes
from.
Cheers,
J-F
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

538
#538 Wrong database file version
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
A database created with java 1.4 can't be accessed by
java 1.5:
java.sql.SQLException: Wrong database file version
checked on HSQLDB 1.7.2 and 1.7.3
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

539
#539 java 1.5 compatibility
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
A database created with jre 1.4 can't be open with jre 1.5
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

540
#540 update test set id=1, id=2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This is currently allowed.
An error should be thrown, similar to Oracle:
create table test(id int)
update test set id=1, id=2
> duplicate column name
This is a low priority bug, I suggest I fix it myself.
Thomas
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

541
#541 Value for a Numeric column is not checked
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
when doing the following (extraction from the log
script) (HSQLDB version 1.7.2.4)
It is possible to insert a value which is greater than
the allowed ones.
Best regards,
Ocke
PS: For another description see
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

542
#542 can't login as another user during DB creation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The behaviour I will describe is new to 1.7.2.9-1.7.3 -
it never happened in 1.7.1 (I didn't check all the
versions in between).
Let's create testLogin.script with the following line:
CREATE USER TEST PASSWORD "TEST" ADMIN
Then let's try to connect to this new database with url
jdbc:hsqldb:<path to folder>/testLogin
We will get the following error message while trying to
connect as user TEST (not user SA):
User not found: TEST
but files testLogin.lck.lck and testLogin.properties
will be created.
If you try to connect as TEST second time - everything
will work !!!
After some additional testing I found that if
testLogin.properties exists - database is created and
everything is ok, otherwise - we get the error message.
Workaround: precreate properties file before making new
database.
Note: I opened the same bug in tools section by error,
you can delete the duplicate there. Sorry.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

543
#543 Uses undefined com.sun classes
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
From the hsqldb gump run on Kaffe:
It would be nice if we could build hsqldb with Kaffe,
but using unspecified Sun classes that are not part of
the Java APIs prevents that.
cheers,
dalibor topic
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

544
#544 Crash evaluating a function in a subselect
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Core function evaluation fails when the function is
used in a subselect. (S1000 NullPointerException)
Reproduced on 1.7.2 and 1.7.3 :
Smallest test case i could get :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

546
#546 SQL error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We have tried this sql statement on MYSQL and MSSQL
it is okey.But HSLQ does not execute it.
What is the problem.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

547
#547 NPE in subselect
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When we use the following query, we get a NPE when no
results can be found in the subquery:
select f1.* from foo f1 where f1.id = (select
max(f2.id) from foo f2 where 1=2)
select f1.* from foo f1 where f1.id in (select
max(f2.id) from foo f2 where 1=2)
doesn't give a NPE.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

548
#548 complex calculation cause exception error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In an attempt to calcuate standard deviation of data.
Below is the select statement.
SELECT POWER(((AVG(power_data.power_2)) -
(POWER(AVG(data.mark), 2))), 0.5) INTO subject_data
FROM data, power_data WHERE
data.subject=power_data.subject GROUP BY subject;
The tables are defined with the data below:
create table data (id INTEGER GENERATED BY DEFAULT AS
IDENTITY, surname VARCHAR, first_name VARCHAR,
entry_score FLOAT,course VARCHAR, stream VARCHAR,
year_level INT, subject INT, semester VARCHAR, mark
INT, grade VARCHAR, supplementary VARCHAR,
special_consid VARCHAR, supp_mark INT, supp_grade
VARCHAR, points FLOAT, student_id INT, year INT,
host_study_type VARCHAR, class_code VARCHAR,
fee_paying_type VARCHAR, PRIMARY KEY (id));
SELECT subject, POWER(mark,2) power_2 INTO power_data
FROM data;
This causes the following error to occur:
java.lang.IllegalArgumentException
at
sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:585)
at org.hsqldb.Function.getValue(Unknown Source)
at
org.hsqldb.Function.getAggregatedValue(Unknown Source)
at
org.hsqldb.Expression.getAggregatedValue(Unknown Source)
at org.hsqldb.Select.buildResult(Unknown Source)
at org.hsqldb.Select.getSingleResult(Unknown
Source)
at org.hsqldb.Select.getResult(Unknown Source)
at
org.hsqldb.CompiledStatementExecutor.executeSelectStatement(Unknown
Source)
at
org.hsqldb.CompiledStatementExecutor.executeImpl(Unknown
Source)
at
org.hsqldb.CompiledStatementExecutor.execute(Unknown
Source)
at
org.hsqldb.Session.sqlExecuteCompiledNoPreChecks(Unknown
Source)
at
org.hsqldb.DatabaseCommandInterpreter.executePart(Unknown
Source)
at
org.hsqldb.DatabaseCommandInterpreter.execute(Unknown
Source)
at
org.hsqldb.Session.sqlExecuteDirectNoPreChecks(Unknown
Source)
at org.hsqldb.Session.execute(Unknown Source)
at
org.hsqldb.jdbc.jdbcStatement.fetchResult(Unknown Source)
at
org.hsqldb.jdbc.jdbcStatement.executeUpdate(Unknown Source)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

549
#549 Unable to use WebRowSet.acceptChanges()
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This is not actually a hsqldb bug, but it's currently not
possible to use the new JDBC RowSet's acceptChanges
() method to update the database with data from a
RowSet.
Here's some sample code that shows the problem:
The last line results in this exception:
The real problem seems to be that sun's WebRowSetImpl
is trying to set a transaction isolation level that hsqldb
does not support. It would be nice if something could
be done to allow the method to continue on with a
fallback isolation level, so that the acceptChanges
method can be used. The way it is currently,
CachedRowSets are extremely hampered by the fact
that they cannot be used to update the hsqldb
database.
Anyway, like I said, this is not a hsqldb bug, more of a
nice to have, especially since RowSet is part of the new
JDK1.5. (but my testing was done using JDK1.4 in
conjunction with Sun's rowset.jar).
Thank you.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

550
#550 column not found in correlated IN subquery
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I try to run the attached sql script I get the
following error:
Column not found: B_ID (ANSI Code: S0022)
The same SQL runs fine on postgresql. Interestingly
enough the problem only occurs if there are rows in the
tables. When the tables are empty the query executes
without error and returns zero rows.
I have verified that the bug occurs in the 1_8_0_RC1
release candidate as well as the 1.7.3 stable version.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

552
#552 NullPointerException with recursive constraints
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
if executing
Zal
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

553
#553 Was: "Not equals (<>) excludes null", is now "Bug in LIKE"
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following query
select * from mytab where col1 <> 'xxxx'
As expected returns all rows where col1 has text not
equal to 'xxxx' but does not return rows where col1 is
null. I would expect the null valued col1 rows to be
included.
This query returns all rows correctly
select * from mytab where col1 not like 'xxxxx'
Regards
Alan
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

554
#554 SUBSTR(myfield, 0, 1) returns Java exception error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The second argument in the SUBSTR() SQL clause accepts
the offset of the string to extract, offset 1 for being
the first character. If you erroneously put 0 as the
offset, HSQL returns the cryptic error message "String
index out of range: -1". I believe this comes from a
Java exception thrown inside HSQL (it has the same text
as a common Java exception) It would be nice if the
database returned a more friendly error message.
I'm running HSQL in Server mode, and accessing it
through a JDBC interface. I'm using v1.7.3.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

555
#555 Command line parsing errors
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Running HSQL from the command line throws Java
exceptions on bad arguments:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

556
#556 OUTER JOIN WITH IS NULL
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
From Open Discussion Forum, posted by rainfun@users...
I have two table a(a1,a2),b(b1,b2).
there are three rows in table a. different is in the first row)
so ,i change the sql into 'select a.A1,a.A2,b.b1,b.b2
from a left outer join b on ((ifnull(a.a1,'')=ifnull(b.b1,''))',
the hsql say this sql is not supported.but i execute thus
sql in microsoft sql server ,the returned result is what i
want.
then , i change the sql into 'select a.A1,a.A2,b.b1,b.b2
from a left outer join b on ((a.a1=b.b1) or (a.a1 is null
and b.b1 isnull))' again,
the hsql returned result is not what i want either,the
result is but i execute thus sql in microsoft sql server ,the
returned result is also what i want .
what wrong with my sql syntax? thanks in advance.
The last query should work but HSQLDB does not
support it yet.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

557
#557 UPDATE fails in a no-primary-key table when any value is NUL
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
bug report for hsqldb SQL
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

558
#558 select * into table no longer works
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Its gives the error:
Unexpected token: INTO in statement [into] / Error
Code: -11
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

559
#559 insert into texttable (select * ...) does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
1) Create test tables
2) create a text table identical to customer (call it test)
3) set test source to "test.csv"
4) Execute insert into test (select * from customer)
Statement fails with a NullPointerException
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

560
#560 select * from text table returns no rows
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
select * from text table returns no rows
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

561
#561 SQL: problem on recursive constraints on single rows
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
tested on 1.8 RC6&7
here is the step to reproduce:
create table mytable(field1 int not null, field2 int);
alter table mytable add constraint pk primary key
(field1);
alter table mytable add constraint pk foreign key (field2)
references mytable(field1) ;
now process the following statements:
a) insert into mytable values (0,0); --> ok
b) insert into mytable values (1,1); --> ok
c) insert into mytable values (2,1); --> ok
d) delete from mytable where field1= 2; --> ok
e) delete from mytable where field1= 1; --> Integrity
constraint violation PK table: MYTABLE
I don't know if it is truly sql99 but logicaly if you can
process the statement b), you should be able to process
the statement e) ?
In most of other database it is the case.
This case is quite typical for a tree structure.
The workaround I found is to have the rows (0,0) and
update the row (1,1) to (1,0) before trying to delete it.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

562
#562 OutOfBoundException: reparsing logs after abrupt shutdown
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
this bug has been found on a 1.8RC7
on a specific case I got a OutOfBoundException during
reparsing the .log file at the restart of a server abruptly
shutdown.
It seems that it is linked to an Update statement on a
table with a foreign key or an index.
note2: If I do a checkpoint, or a clean shutdown it
works fine.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

563
#563 Please remove profanity
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In a recent source scan the following profanity was detected.
Can you please have it removed for our internla
compliance.
Package HSQLDB
Profanity to remove: shit
Files affected:
Thank you in advance.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

564
#564 Server putPropertiesFromFile wrong result
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Trying to setup a server by loading the properties from a
not existing file using the
method 'putPropertiesFromString' returns 'true'. The
Javadoc says: "Returns:
true if the indicated file was read sucessfully, else
false". So I think the returned result is wrong.
Example:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

565
#565 Error with CompiledStatementManager.freeStatement
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am running a stand-alone application with Hsqldb in
server mode. I am
using Hibernate and get the above exception when
Hibernate executes a
prepared statement. It looks as though Hsqldb is
attempting to remove the
compiled statement from the Hsqldb session but throws
the execption. If I
run this same code using file access and not server
access, I do not get the
exception.
Please let me know if I can provide any further
information.
Logging Statements
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

566
#566 create user
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Statement <create user a- password ''> is valid. you can
log on with user a- . After you shutdown hsqld and
restart you get an error
System.exit() is called next
unexpected token -
it should be both possible to create a user a- via jdbc
and to restart hsqldb with that user;
or it should be both **not** possible for having a
consistent db. The actuell behavior could cause strong
problems after restarting, think about much users that
are allowed to create new logins !
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

567
PDF version of User Guide truncates lines in"Example Blocks"
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In PDF version, on page 4; "Example 1.1" shows:
Connection c = DriverManager.getConnection
("jdbc:hsqldb:hsql://localhost/xdb",
The Web version shows complete correct statement:
Connection c = DriverManager.getConnection
("jdbc:hsqldb:hsql://localhost/xdb", "sa", "");
This problem is not limited to this one occurance.
Truncated examples occur further down page 4 and on
page 5.
This problem affects the usability of the PDF version.
The Web version of the user guide does not have this
problem.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

568
#568 create user !
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Sorry for sending again:
I test it once again:
First of all :
create user '8' password ''
via jdbc!!!
Look at the system user table: that user 8 is available
connect user '8' password '' works fine !
you run into an exception (same problem with user a- etc.)
System.exit() is called next
change manually create user 8 password"" to
create user '8' password""
in the .script file fix that problem!
Thanx!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

569
#569 Creating table with a foreign key in self throws NullPointer
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Version 1.7.3.3 has a bug that was not in 1.7.3.0.
try:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

570
#570 ORDER BY in SELECT for CREATE VIEWs
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I cant create a view using the ORDER BY clausule in
SELECT statement.
Is it a bug or a feature not implemented ?
See the example:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

572
#572 Issue with Sequence nextvalue
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Next Value in a sequence is returning
current Value + Number of Sequences in the database
If you have 3 sequences Seq1, Seq2 and Seq3 in the
databse, following query returns 3 values each
select next value for Seq1 from
SYSTEM_SEQUENCES
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

573
#573 Don't support hibernate 3rc1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Got the following exception on the server side when
execute application that uses hibernate3rc1:
Comments:
1) The same application works fine with hibernate 2.1.8
2) Don't work with hibernate3 and hsql 1.7.2, .1.7.3
and 1.8.0rc9
It seems like hsqldb (not hibernate) error because hsql
_server_ (not jdbc driver) hung.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

575
#575 LIMIT and TOP cannot be used in subqueries
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The changelog for 1.7.0 states that "SELECT LIMIT <n>
<m> ... is now treated as a complete SELECT
statement and can be used anywhere a SELECT is
used".
However LIMIT and TOP cannot be used in a subquery
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

576
#576 select ... where varcharColumn in ('abc')  case sensitive
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm using 1.7.3.3.
Columns of type VARCHAR_IGNORE case can be
selected ignoring case. This works only with
varcharColumn = 'abc'
which matches values like ABC, abc, Abc ...
But the IN clause always reflects the case of the
characters.
I expected to worl this too, like
varcharColumn in ( 'abc', 'def')
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

577
#577 zip file for RC8 and RC9 does not have bin directory
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I am trying to package a RC9 (or RC8 at least) RPM for
hsqldb in JPackage.org devel area (we have a 1.7.3.3
already in the production area) but I encountered the
following problems:
1) The zip files for RC8 and RC9 do not have the bin
directory anymore, so no 'hsqldb' command, for instance.
2) The RC9 zip file has the compiled .class files in it
(I can just ignore it but it makes the src.rpm much
larger unecessarily)
Regards,
Fernando
fnasser@redhat.com
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

578
#578 Out by one in jdbcClob.position
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Found an out-by-one bug in jdbcClob.position(Clob,long).
Have commited patch to hsqldb-dev1.
Should be:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

579
#579 Error in Order By
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Table: Test(ID:Integer, Cost:Float)
SQL: SELECT ID, ' kr' + Cost FROM Test ORDER BY
Cost;
In this scenario HSQLDB sorts the resultset by using a
String comperator instead of a float-comperator. This
error-case is also true when doing descendent ordering
and for double types. I have not tried this scenario for a
disk-based HSQLDB.
A possible work-around is to insert the Cost-field without
concating strings around, before the concated cost-field
SQL: SELECT ID, Cost, ' kr' + Cost FROM Test ORDER
BY Cost;
This select-statement yields correct results but the
extra Cost-field is not an option for my project
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

580
#580 Empty resultset with quote in string
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This SQL statement gives empty resultset:
select * from customer where last_name='O''Connor60'
I would have expected 1 record in the resultset.
The following SQL statement gives 1 record in resultset:
select * from customer where last_name like '%O''Connor60%'
Can someone explain why the first SQL statement does not
return a record ?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

581
#581 Select doesn't contain all rows in the rusult
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Select doesn't contain all rows in the rusult. It happens for
big tables only. Some rows are lost.
Example:
If A and B is big enough, the result doesn't contain some
rows.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

582
#582 getColumnDisplaySize returns huge number for a float
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
hsqldb 1.73
Here is the simplest sql:
select 1.0 from ...
then getColumnDisplaySize will return 646456995.
On the other hand,
select 1 from ...
then getColumnDisplaySize will return 11 which is
reasonable.
Binh
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

583
#583 How to start server
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hai
i need a small help how to start HSQLDB server. i
typed the following command in command prompt.I
installed my HSQLDB 7.1.3.3.zip flle in c:\HSQLDB3
folder i tryied this command in
i tryied as per your specification but it shows the
following error
exception thread "main" NoClassDefFoundError.
and what is xdb please provide some elabration.
so What is wrong please give me some suggestion.
Thank you for reading this message.
withregards,
Baburao.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

584
#584 java.sql.SQLException: socket creation error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I`m geting this error message when trying to connect to
hsqldb engine Server.I`m starting the server and
mannager normally but cant connect to the bank.
the commands I using are
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

585
#585 Error: java.sql.SQLException: Unexpected token ORDER, requir
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I got a error when use hsqldb_1_7_3_3, Error:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

586
#586 StoredProcedure - rset.getMetaData() causes Null Pointer
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Calling getMetaData() after a query using a Connection
passed into a stored procedure causes a null pointer
exception.
Stack Trace:
e-mail: marc.smith@bi-uk.com
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

587
#587 Too many open cursor
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The method SQLFile.processSQL doesn't close the
create Statement.
So when it is used many times the error
"Too many open cursor" comes from Oracle.
It's easy to fix only insert the line
statement.close();
at line 1717
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

588
#588 jdbcResultSet does not properly adjust timestamps w/calendar
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In general, the internal Hypersonic handling of
Timezones is flawed since the conversion done in
Column.java initially occurs without reference to any
TimeZone supplied on entry through PreparedStatement.
This creates date objects violating the java.util.Date
contract that the enclosed fastTime field represent
milliseconds since UTC epoch.
This problem is largely corrected by adjustments
performed in jdbcResultSet, however these cannot be
completely correct since they use the supplied time as
the basis for daylight savings adjustments. As a result
daylight savings times for retrieved dates will occur
at a time offset by the current timezone, and not for
example at the local midnight.
However, a more serious problem is that Timestamp
objects are not corrected at all by jdbcResultSet, with
the result that Timestamps will be corrupted by storage
and retrieval through Hypersonic, even if the same
Calendar object is supplied to ResultSet and
PreparedStatement.
This problem(s) are exhibited by HEAD as well as
historical versions of Hypersonic stretching back at
least 2 years.
A patch is enclosed against jdbcResultSet to illustrate
a partial solution of the more serious problem. This is
against hsql 1.7.3.3 (rev 1.6 of jdbcResultSet)
although the relevant code has not changed since then.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

589
#589 Update on TEXT TABLE do not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If a update on a Text-Table is executed only the first
characters of the old row will be replaced by linefeed.
Reason is the method remove in class
org.hsqldb.persist.TextCache. Calling of
r.getStorageSize() returns 0.
To fix the problem I r eplaced r.getStorageSize() with
r.getRealSize(rowOut).
Karl-Heinz
mailto:Karl-Heinz.Fleck@subito.de
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

590
#590 Get java.sql.SQLException: error in script file line: 8 out
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HSQLDB 1.8.0RC9:
I get the following exception when trying to connect to
the database on startup:
java.sql.SQLException: error in script file line: 8 out of
memory
The relevant line from the script is:
It appears that the script is trying to set the table index
at a point in the data file that is well beyond the end of
the data file (which is only 4,194,304 bytes long).
This probably occurred because there was a non-clean
shutdown.
Let me know if you want me to attach the database as
I'll have to get permission from my customer.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

591
#591 hsql doesn't check length of text field
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
hsql doesn't check length of text field
the text field can save text out of the length .
it has no errors. I found the problem until I move to
Oracle
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

592
#592 arithmetic operation on bind variables fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The attached code attempts to add two bind variables.
Preparing the statement fails with
The same error resulted.
This was tested on 1.8.0-rc9.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

593
#593 non-null bind variable evaluates as null
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Executing the statement "select count(*) from test
where ? is null" with 'hello' set as the bind variable
does not return 0. A test case is attached. This was
tested on 1.8.0-rc9.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

594
#594 bind variable in limit clause causes NPE in parser
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Calling prepareStatement with "select limit ? 0 id from
foo" causes an NPE in the parser which results in a
S1000 general error SQLException thrown to the client.
A test case (run on 1.8.0-rc9) is attached. The NPE
that is printed to the console is:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

595
#595 Integrity constraint violation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
simple example:
Lets say there is a row such as:
1,1,1
you can not delete this row..(btw, it is working in
PostGre)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

596
#596 Script command vs Unicode
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello!
I have HSQLDB 1.7.2 with Russian characters inside. I
wish to export DB via script command and a bit later
import it to another copy of DB.
If I script DB I'll have something like INSERT INTO TTT
VALUES('ID','RU','\u0423'). If I execute such script at
the DB, I'll get value \u0423 in the column instead of
Russian letters.
Elias Ross (genman) said that it looks like a bug and
ask me for good test/example for it (see attahced file
and
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

597
#597 getColumnDisplaySize different between hsqldb:file & server
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The result returned by getColumnDisplaySize is different
when connecting to the database as a file and when
connecting through the server. The hsqldb:file results
are correct but the hsqldb:hsql are not.
eg:
lightware@mailbox.co.za
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

598
#598 null exception in script file (Windows only)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Procedure to reproduce:
- create a test database with OpenOffice.org base
- extract its contents with unzip
- change to database/ directory
- rename "data" to "oo.data", "backup" to "oo.backup", etc,
to conform with HSQLDB naming conventions
- start the Database Manager or the SQL Tool, connecting to
a "jdbc:hsqldb:file:oo" resource
- HSQLDB fails connecting when reading first line of the
oo.script
- that line is a CREATE CACHED TABLE (...) instruction
- you get a
error in script file line: 1 null exception message
- the exception comes from DatabaseManager.newSession()
call, line 2449 of jdbcConnection.java
Workarounds:
- Removing the first line of the script removes the exception
- That happens only under Windows.
It works perfectly under Linux (!!!)
Environment:
- HSQLDB Release Candidate 10 for 1.8.0
(but same with 1.7.3)
- OpenOffice.org 2.0 beta m104
(but same with m100)
- SDK 1.4
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

599
#599 SET Password => sqlException(Unknown Source)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I wanna set Password via MD5 but this causes Exception.
Looking more detailed I think it's due to some
misshandling of the input stream. e.g.
??? why it fails in the one case, while in the other
not ???
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

600
#600 NPE if sql.compare_in_locale=true in 1.8.0 RC 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Opening a DB with sql.compare_in_locale=true cause a
NPE in
in org.hsqldb.Database.reopen() has do be placed befor
the call of
databaseProperties.load();
to fix the bug.
Mit freundlichen Gren
Karl-Heinz
mailto:Karl-Heinz.Fleck@subito.de
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

601
#601 Duplicate row after uptade in 1.8.0 RC10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After update of a text table and closing the db with
out calling SHUTDOWN COMPACT, I get duplicate row in db.
Mit freundlichen Gren
Karl-Heinz
mailto:Karl-Heinz.Fleck@subito.de
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

602
#602 General error when attempting to getConnection
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I got the following error:
Every time I run my application i get this error. The
only way i can fix it is to delete my
database.properties file which was mysteriously empty.
then it works fine (created a new database.properties
file). But then it happens again after a while. (occurs
quite rarely).
I can only think it became empty because of a strange
sequence of events in the previous execution of my
application.
Thanks,
richardk@carbontwelve.com
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

603
#603 missing ends of lines
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
PDF doc for 1.8.0 RC10
Long lines, formated using non-proportional font, are
stripped.
Screenshot from AcrobatReader 7 included.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

604
#604 Isolation Level not supported but no error or warning
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In latest 1_8_0 RC10 version Transaction Isolation Levels
are still not supported. Method setTransactionIsolation in
jdbcConnection looks like ready for this feature, but method
setIsolation (which is called by jdbcConnection) is still
empty:
public void setIsolation(int level) throws HsqlException {}
so, now I can set Isolation Level and hsql driver doesn't
throw any Exception, but when I try get current level I still
receive of course 0...
I think, that If this functionality is not implemented yet, it
shouldn't be possible to set any custom transaction isolation
levels without any errors or warnings.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

605
#605 incorrect row size in 1.8.0 RC11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello Fred,
the class TextCache generates a incorrect row size, if
the row before is empty. The size of the empty row
while be included.
Line 508 in TextCache:
int length = (int) dataFile.getFilePointer() - firstPos;
firstPos contains the start position of the empty row.
If you will use pos instead of firstPos, you get the
right size.
Mit freundlichen Gren
Karl-Heinz
mailto:Karl-Heinz.Fleck@subito.de
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

606
#606 Cannot open DB inside of jar in 1.8.0 RC10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a in-memory database which opens fine in 1.7.3.
I upgraded the DB to 1.8.0 RC10 and re-jared it. I now
get this error:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

607
#607 SELECT INTO does not use "hsqldb.default_table_type"
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Existing default in parser switch is:
default :
However, to be consistent with
other "hsqldb.default_table_type" handling, it likely
should be (roughly) the equivalent of:
perhaps getDefaultTableType can be factored out of
DatabaseCommandInterpreter and placed in Session.
I don't think Database is the best place to put this,
because we may eventually want to implement session
scope equivalents for some database properties, e.g.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

608
#608 Incorrect calculation for var_samp and stddev_samp
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
not using (n-1) divisor:
patch follows:
Index: SetFunction.java
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

609
#609 DATEDIFF function returns incorrect results
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a simple database and get incorrect results from
the following query - between 23:00 and midnight local
time:
This normally works as expected, and returns events
received within the current day, but between 23:00 and
midnight it returns no results at all. "time_received" is
defined to be a TIMESTAMP.
After midnight, the select statement works as expected
again. I am wondering if this has something to do with
day light savings processing or something?
HSQLDB version 1.7.3 running under JRE 1.4.2 and
1.5.0 on Windows XP Pro (all service applied). System
clock is running day light savings time adjusted local
time.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

610
#610 CONVERT not working
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
CONVERT doesn't seem to be working and throws
an "Unexpected token in statement" error in 1.8.0-RC11.
Examples I've tried:
I've used CAST to work around this.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

611
#611 Random ArrayIndexOutOfBoundsException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
running 1.7.3.3 built with JDK1.3 running in 1.1
Doing some load testing we have 30 clients running hsql
database, we frequently are getting an
ArrayIndexOutOfBoundsException in
HsqlByteArrayOutputStream.write() method. The client
that fails and the location of the failure varies from run to
run but we consistenly get this same exception.
This was the failure on the last test run:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

612
#612 SET PROPERTY "hsqldb.default_table_type" does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
'SET PROPERTY "hsqldb.default_table_type" cached'
throws and access denied error in 1.8.0-RC12
--
Access is denied: hsqldb.default_table_type in
statement [SET PROPERTY "hsqldb.default_table_type"] /
Error Code: -33 / State: S1000
--
1.8.0-RC12 documentation clearly states that
hsqldb.default_table_type can be set using SET PROPERTY:
-- cut --
The CREATE TABLE command results in a MEMORY table by
default. Setting the value "cached" for this property
will result in a cached table by default. The qualified
forms such as CREATE MEMORY TABLE or CREATE CACHED
TABLE are not affected at all by this property. (SET
PROPERTY)
-- cut --
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

613
#613 <column> IN ('value1',...) requires trailing spaces on value
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When doing a SELECT * FROM <table> WHERE <column> IN
('value') with sql.enforce_size=true set in the
<Database>.properties file value needs the trailing
spaces up to the field size in order to match.
For example:
Table Def:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

614
#614 Invalid username's
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi Fred,
I've just tried to create a user in RC12 with a name of
TEST-USER
It didn't like the dash (-). What are the valid characters
for a username ? What is wrong with a dash in a
username ? If this isn't a bug but a feature where is the
code that I could change to allow dash's in usernames ?
Cheers
Matt
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

615
#615 ResultSet.getObject() returns wrong value type
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
version: 1.8.0.RC12
I created a table with a column of type smallint; the
manual (section 8) says that columns of that type are
treated as short/java.lang.Short values. However, when
I call ResultSet.getObject() on that column, i got an
Integer value.
I queried the SYSTEM_COLUMNS table and found out that
integer and smallint values have the same buffer size,
so it seems that short values are handled as int
values. I think it's not the expected behavior.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

616
#616 jdbcConnection(Session) -> jdbcConnection(SessionInteface)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
in jdbcConnection
the method
public jdbcConnection(Session c)
should change interface to
public jdbcConnection(SessionInterface c)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

617
#617 ArrayIndexOutOfBoundsException from BaseHashMap
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
stacktrace from 1.7.3.3
Our application has a lot of agents which collect data
and then send
a copy of their database to a central server where the
databases are openned and their data is copied into an
Oracle database.
After openning and closing a number of Hypersonic
databases, we
start getting this exception on every database we open.
I first noticed this problem in 1.7.2 RC 6b and thought
I should upgrade to 1.7.3.3 and test it before I
reported it.
Unfortunately, I do not have a good isolated test case
to attach to this. I have only reproduced it with our
application running for an hour after I shutdown the
Oracle database; it's openning/closing new Hypersonic
databases the whole time.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

618
#618 database grows unlimitedly
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Here is bug scenario:
1) I have database with cached tables.
2) I delete some LARGE rows with data
3) These rows disappeared from table - but table size
is NOT reduced. As a result, now I have 100M database
without ANY data!
Database version 1.7.2
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

619
#619 Significant performance optimisation in SetFunction.java
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In SetFunction.java, function addDataPoint, the
function uses Math.pow to square sk - (double) (n-1) * xi.
This is vastly less efficient than simply multiplying
the number by itself. the pow function is based on logs
which is why it is so inefficient.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

620
#620 Use of variable N very inefficient in SetFunction.java
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In SetFunction.java, we have:
private long n;
However, every time we use n, we cast it to a double.
So we may as well have
private double n;
instead. Casting is inefficient and as function
addDataPoint will be called many times if the
statistical functions are used, this is a big weakness
in the algorithm.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

621
#621 SOME returns null instead of false
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In SetFunction.java, line 250, the function is meant to
return false if count==0. However, line 183 returns
NULL if count==0 and is in front of this line.
Hence the SOME operator will never return false. It
returns NULL instead.
INcidentally, the expression:
return count == 0 ? Boolean.FALSE
: Boolean.TRUE;
can be optimised to:
return(count != 0)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

622
#622 non-null doesn't work properly in TEXT tables.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using hsql 1.8.0.0, I got the following error:
error in script file line: 17 bad TEXT table source
file - line number: 1 Attempt to insert null into a
non-nullable column: column: DESCRIPTION table:
MM_OALIAS.txt looks something like this:
where DESCRIPTION is the 5th collumn.
According to the documentation ,"", should be
interprted as empty string, and ,, as NULL. The error
message makes me think that it interpreted ,"", as NULL
any way, which it cannot be.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

623
#623 Aggregate function requiring group by clause check bypassed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If you use a function such as "left" on a column in a
select statement with aggregates, the checking that all
columns not aggregated are in a "group by" is bypassed
resulting in a garbage result. Here is my example:
I have this table:
name datatype width no-nulls
Now, issuing this statement gives the correct error:
select sale_date, sum(sales_price) from salestable;
SQL Error at 'stdin' line 60:
"select sale_date, sum(sales_price) from salestable"
Not in aggregate function or group by clause:
However, if I now wrap a "left" around the sale_date column, no error. Garbage result:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

624
#624 The "DISTINCT" command is redundant in the MINUS statement
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It seems that the DISTINCT command is either not
implemented or is redundant, in the MINUS statement
(select ... MINUS select).
Not sure if this is a bug because some functionality is
missing, or if this is functionining as designed.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

625
#625 Error restoring attached log file
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Version: 7.1.3.3
Tonight I encountered an interesting problem with the
attached database. The attached database fails to load
all data from the *.log file when you start up the
database. No error is generated, the data is just missing.
I traced through with a debugger to find out what was
going on.
The problem seems to be when a delete statement
references a column named position. Looking through
the documentation I see position is a valid function
call so I suspect that is what has caused the problem.
Unfortunately the way the system handles this is not good:
1. It reports the eror using the Trace function.
Unfortunately all tracing seems to be off by default.
2. It stops trying to load any more data and truncates
the log file.
The net effect of those two things is that the data is
just gone.
To get around this I'll probably have a go at renaming
that column but in the mean time I suggest the
functionality should be correct:
1. Position should be a reserved word and you shouldn't
be able to use it in a table in the first place.
2. Always log that an error has occured.
3. Stop all processing and leave the database in a
consistent state.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

626
#626 DESC keyword is sometimes ignored
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
For some queries, if a DESC keyword is supplied it is
ignored and the result set is returned in ascending
order. This phenomenon appears to be a function of how
the order-by column is referenced in the SELECT
statement. There may be other factors involved as well,
as some attempts to design a minimal example did not
exhibit the problem. I have encountered a number of
independent instances of this problem, all of which
have involved ordering on a DATETIME column.
The attached file provides an example of the problem.
For two variants of a SELECT statement the records are
correctly returned in descending order. For two other
variants, the records are incorrectly returned sorted
into ascending order.
Note that the phenomenon occurs with queries
auto-generated by Hibernate (http://www.hibernate.org/)
when operating via its HSQLDB dialect. Therefore, it is
not feasible to simply avoid the problematic way of
referencing the order-by column (unless the Hibernate
dialect is changed).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

627
#627 Null Pointer Exception from using version 1.8.0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using HSQLDB v1.8.0 in conjuction with JPOX, I receive
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

628
#628 Error using JPOX query with HSQLDB
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Note this error does not occur with using the same
query on a different database.
Here is the error message reveived:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

629
#629 ArrayIndexOutOfBoundsException in jdbcDriver.java
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I get an ArrayIndexOutOfBoundsException in
jdbcDriver.java:
java.lang.ArrayIndexOutOfBoundsException: 4
at
org.hsqldb.jdbcDriver.getPropertyInfo(Unknown Source)
The DriverPropertyInfo array is initalized to a size of
4. The method then makes assignments to
DriverPropertyInfo at indices 1 through 5. The
DriverPropertyInfo array should be initialized to a
size of 6 to prevent this error.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

630
#630 COALESCE/NVL/IFNULL(MAX(...), ...) returns NULL
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If I create an empty table with hsqldb 1.8.0.1 by
CREATE TABLE TEST_TABLE(VALUE DECIMAL);
then
SELECT MAX(VALUE) FROM TEST_TABLE;
correctly returns one record with a NULL value.
But if I execute one of the queries
SELECT COALESCE(MAX(VALUE), 42) FROM TEST_TABLE;
SELECT NVL(MAX(VALUE), 42) FROM TEST_TABLE;
SELECT IFNULL(MAX(VALUE), 42) FROM TEST_TABLE;
then the result is the same, although 42 should be
returned instead.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

631
#631 hibernate-generated SQL runs *very* slowly
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm trying to use HSQL v1.8.0.1 with Hibernate, and
have come across a problem that appears to cause the
database server to hang and/or enter an infitite loop
(CPU usage stays 100%).
The attached file contains the database, which consists
of some tables, with some sample data.
With debugging etc, I've obtained the SQL that
Hibernate generates, that causes the problem:
This version works, returning the expected 0 rows
almost immediately.
Unfortunately, I dont have control over the SQL that
Hibernate is generating, so havent found a way to use
this (working) version of the select.
No additional messages appear on the console. Also, if
I repeat the test with an empty database (ie tables
created, but no rows inserted) then the problem does
not appear.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

632
#632 Misleading exception in trigger handling
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
I wasted a fairly long time trying to find the error in the sql
statement i used to create a trigger. The exception, that was
thrown had message "Unexpected token while parsing trigger
command". I thought something is wrong with the sql query, i
started. But after a time, i started debugging into your code, and
found out, that the error occurred, while hsqldb tried to instanciate
the trigger class, that unfortunately had no default constructor.
I would appreciate finding a hint in the documentation about that
indispensible default constructor in a trigger class. In addition, the
exception has in fact nothing to do with the problem. Could you
please fix it, too?
Best regards
Christoph
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

633
#633 Unique Constraint Violation with autoincrement on Text Table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When using generated keys on text tables, the engine
adds ALTER COLUMN RESTART WITH statements in the script
file. If, after adding new records to the table the db
shuts down incorrectly, these values may not be
updated. If the db is then restarted, future inserts
into the table cause a unique constraint violation.
If the ALTER COLUMN statements are removed, the
database oprates correctly and continues with the next
number.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

634
#634 Under 1.8.0.1 the getTables() method doesn\'t work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using the below code snippet, under 1.8.0.1 the first
set of results (below the snippet) does not return the
table requested. When this same snippet was used uner
1.7. 1 the second results are provided. This appears
to be a bug in the current implementation. My only
work around is to go back to 1.7.1.
that can't be rewound.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

635
#635 global temp tables with preserve rows not working
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If I run the below script via the HSQLDB Database
Manager the select returns no results.
If the "on commit preserve rows" is removed then the
select returns results.
drop table TEST_TABLE
create global temporary table TEST_TABLE (
id int not null
) on commit preserve rows
insert into TEST_TABLE values (1);
select * from TEST_TABLE
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

636
#636 COT implementation should call ATAN
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Most of the trig functions in Library.java call the
underlying Java code directly. But for some reason the
COT implementation doesn't call ATAN() in java.lang.Math.
I am guessing it's because the coder didn't know that
COT is actually ATAN, being 1/tan().
The problem with is being implemented as 1/tan() is
that the Java specifications are very clear that the
result must be within 1 ulp of the correctly rounded
result. Results must also be semi-monotonic.
This is guaranteed in the java.lang.Math library but is
not when you do the maths yourself as in 1/tan().
A simple test is to play around with select statements
where you select multiples of cot(+-PI()). They do not
give Infinity. Select cot(0) and it does.
Of course, this is also because of the resolution of
PI() which can only approximate it, so this test isn't
really fair but it does make the point.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

637
#637 DELETE query with JOIN under 1.8.0.1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Executing a DELETE query with a JOIN throws an
error. Trying to work around it with:
DELETE tblChequeDetail FROM tblChequeDetail
WHERE EXISTS (SELECT chequeMasterID FROM
tblChequeMaster WHERE
tblChequeDetail.chequeMasterID=tblChequeMaster.cheq
ueMasterID AND tblChequeMaster.CustomerID=1)
also throws an error:
Unexpected token TBLCHEQUEDETAIL, requires
FROM in statement [DELETE tblChequeDetail]
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

638
#638 In the MOD function, a divisor of zero aborts entire dataset
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If you use mod(number,divisor), with divisor zero, the
entire result set is aborted on the divide by zero error.
It SHOULD return NaN for that instance.
This is very much needed when MOD is used across an
indeterminate dataset (i.e. rows in a table where the
values in those rows cannot be predicted).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

639
#639 CONCAT doesn\'t handle a single null value correctly
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This is really bizarre because looking in Library.java
But it returns NULL.
Likewise CONCAT('ABC',null) doesn't return 'ABC'.
I have a table DUAL defined that allows me to select a single row (yes, I know, it's an Oracle thing, sorry!).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

640
#640 CONVERT statement syntax is wrong. Comma causes error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The CONVERT statement syntax should be:
CONVERT(term,type)
However, this produces an error in the latest version
of HSQL:
sql> select convert('12',float) from dual;
SQL Error at 'stdin' line 47:
"select convert('12',float) from dual"
Unexpected token in statement [select convert('12',]
However, this works:
sql> select convert('12' as float) from dual;
12.0
Which is the syntax for the CAST statement.
Looking in Parser.java line 2785 there is this statement:
if (isConvert) {
readThis(Expression.COMMA);
} else {
readThis(Expression.AS);
}
It seems that isConvert is not behaving correctly to
the syntax is changing to that of CAST instead.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

641
#641 type-conversion error when using COALESCE with dates
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
COALESCE does not seem to work correctly when dealing
with dates or timestamps in some situations. For
example, if I have a table "foo" with a DATE column
"foo_date", this statement causes an exception:
SELECT COALESCE(foo_date, '2005-01-01') FROM foo;
The error is:
java.sql.SQLException: Unresolved parameter type as
output of CASE when operand types are LONGVARCHAR and
DATE in statement [...
The problem goes away if I CAST the second argument to
a DATE, e.g.:
SELECT COALESCE(foo_date, CAST('2005-01-01' AS DATE))
FROM foo;
Which doesn't seem unreasonable. However, I get the
same exception when using a JDBC PreparedStatement and
with a Date bind parameter, rather than a string literal:
PreparedStatement ps = c.prepareStatement("SELECT
COALESCE(foo_date, ?) FROM foo");
ps.setDate(1, new Date(99, 11, 31));
ps.executeQuery();
Using CAST in this case will also fix the problem, but
it seems less reasonable to require it, since the value
is unambiguously a DATE.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

642
#642 SQLTool -> NullPointerException on Timestamp-columns
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
SQLTool fails if a select is done on a null timestamp column. Example:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

643
#643 SELECT statement behave different in different platform
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm using hsqlDB version 1.8.0. I've re-compiled it
for Java 1.1 for the iPAQ platform. It was running
fine, except when I test a SELECT statement, I noticed
not all the records are returned from the result. But
if I run the same statement on the Linux/Windows using
the 1.8.0 for java 1.4.2. All the records will be
returned.
Here is the select statement:
SELECT schedule_id_pk FROM schedules
WHERE from_date <= '2005-09-02'
The problem is:
Any records that has the from_date='2005-09-02' will
not be returned on the iPAQ platform (hsqldb for Java
1.1), but they are returned if I run on Linux/Windows
(hsqldb for java 1.4.2).
The database table (cut down version):
CREATE TABLE schedules (
schedule_id_pk INTEGER GENERATED BY DEFAULT AS
IDENTITY (START WITH 1) NOT NULL PRIMARY KEY
, from_date DATE
, to_date DATE
);
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

644
#644 java.sql.SQLException: User not found :SA
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have lost two databases randomly from this and the
data is lost too. It appears to occur at shutdown that
the files are not being shutodwn properly. When the
application is restarted, the files become corrupted. I
have lost both a tomcat user database and an Apache
Mail database. I have also received copies of the
corrupt database from end users who are using an
application which has HSQLDB embedded.
The only thing in the script file is:
CREATE SCHEMA PUBLIC AUTHORIZATION DBA
SET WRITE_DELAY 20
The properties file appears to be normal.
#HSQL Database Engine
#Fri Sep 02 07:32:04 EDT 2005
hsqldb.script_format=0
runtime.gc_interval=0
sql.enforce_strict_size=false
hsqldb.cache_size_scale=8
readonly=false
hsqldb.nio_data_file=true
hsqldb.cache_scale=14
version=1.8.0
hsqldb.default_table_type=memory
hsqldb.cache_file_scale=1
hsqldb.log_size=200
modified=yes
hsqldb.cache_version=1.7.0
hsqldb.original_version=1.8.0
hsqldb.compatible_version=1.8.0
The exception I see in the log files is below:
java.sql.SQLException: User not found: SA
at org.hsqldb.jdbc.Util.sqlException(Unknown
Source)
at
org.hsqldb.jdbc.jdbcConnection.<init>(Unknown Source)
at org.hsqldb.jdbcDriver.getConnection(Unknown
Source)
at org.hsqldb.jdbcDriver.connect(Unknown Source)
at
java.sql.DriverManager.getConnection(DriverManager.java:525)
at
java.sql.DriverManager.getConnection(DriverManager.java:171)
at org.hsqldb.util.RCData.getConnection(Unknown
Source)
at org.hsqldb.util.SqlTool.main(Unknown Source)
at org.hsqldb.util.SqlToolSprayer.main(Unknown
Source)
Failed instances: [mail]
The application is shutdown with this code snippet
which has worked on the all the databases from 1.7+.
Nothing has changed in the environment except now the
engine is 1.8.0.2.
StopService ()
{
ConsoleMessage "Stopping HSQLDB Database Engine"
exec "$JAVA_EXEC" -classpath "$CLASSPATH"
-Dsqltoolsprayer.propfile="$PROPERTY_FILE" \
-Dsqltoolsprayer.rcfile="$SQLTOOLRC"
org.hsqldb.util.SqlToolSprayer 'SHUTDOWN COMPACT;' >>
"$LOGFILE" 2>&1 &
}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

645
#645 java.sql.SQLException: Unexpected token: GROUP in statement
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have just upgraded to 1.8.0.2, from 1.7.2 and my code
threw this error, when I was attempting to create a
table. I have distilled it down to the attached example:
What seems to be happening is that the SQL statement
is being truncated after the word group. I have an easy
workaround by changing the name of that field. As far
as I am aware the word GROUP is valid in this context,
as it is just a field name but I am no SQL expert
Reverting back to 1.7.2 or cfhanging the name works fine
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

646
#646 Create table failes after the table rename operation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following code returns
*java.sql.SQLException: Table not found in statement
[select * from ss]* in the last line.
String driverClassName = "org.hsqldb.jdbcDriver" ;
Properties properties = new Properties () ;
properties.put ( "user" , "sa" ) ;
properties.put ( "password" , "" ) ;
String dbURL = "jdbc:hsqldb:file:/home/ali/deleteMe" ;
Class.forName ( driverClassName) ;
Connection connection = DriverManager.getConnection
(dbURL , properties ) ;
connection.createStatement().execute(
"create memory table ss(TIMED BIGINT NOT NULL IDENTITY
PRIMARY KEY)");
connection.createStatement().execute("alter table ss
rename to
ss2");
connection.createStatement().execute(
"create memory table ss(TIMED BIGINT NOT NULL IDENTITY
PRIMARY KEY)");
connection.createStatement().execute("select * from ss");
Best,
Ali Salehi
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

647
#647 Failed to upgrade database
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Data Crow uses the database in byte mode
(hsqldb.script_format=3) as it can grow quite large
(images are stored in the database, etc).
It is virtually impossible for Data Crow to pick up a
newer version (1.8) as we have to explain the user to
run a SQL statement on their (as can be read in the
documentation) old database / data crow version before
installing a newer version of Data Crow.
Is there a way to upgrade a "byte" database to 1.8
without having the user to run sql scripts ? Can we
solve this in the code ?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

648
#648 Script reading in Windows 98
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When using hsqldb 1.7.2 or 1.8.0.1 on Windows 98 with
Sun JDK v 1.4.2, whenever a script has a line break
within an SQL statement, the reading of the script
fails with the message that there is an unexpected
token ' '. That is, it is stumbling on a non-printing
character. If I remove all the line breaks from within
each statement, it runs fine, so I figure it must be
choking on either the presence or lack thereof of the
CR in the newlines on Windows.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

649
#649 1.8.0.1: NullPointerException in Index.child
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We have a NullPointerException in the HSQLDB 1.8.0.1
engine during an INSERT. The problem always produces
the same stack trace but doesn't necessarily happen on
the same data when we re-run our program (hence, we
can't provide a concrete test case). The problem only
happens on Linux (we have a Red Hat 8.0), whether we
use 1.4.2 or 1.5 (SUN VM).
Here is the stack trace:
java.lang.NullPointerException
at org.hsqldb.Index.child (Unknown Source)
at org.hsqldb.Index.balance (Unknown Source)
at org.hsqldb.Index.insert (Unknown Source)
at org.hsqldb.Table.indexRow (Unknown Source)
at org.hsqldb.Table.insertNoCheck(Unknown Source)
at org.hsqldb.Table.insertRow (Unknown Source)
at org.hsqldb.Table.insert(Unknown Source)
at
org.hsqldb.CompiledStatementExecutor.executeInsertVa
luesStatement (Unknown Source)
at org.hsqldb.CompiledStatementExecutor.executeImpl
(Unknown Source)
at org.hsqldb.CompiledStatementExecutor.execute
(Unknown Source)
at org.hsqldb.Session.sqlExecute (Unknown Source)
at org.hsqldb.Session.execute (Unknown Source)
at
org.hsqldb.jdbc.jdbcPreparedStatement.executeUpdate
(Unknown Source)
...
We have an index on two columns of that table.
We don't have the problem with 1.7.3.3.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

650
#650 Ant does not build corret hsqldbmin.jar
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
For release 1.8.0.2, if build is exectuted with "ant
hsqldbmin" for the non-server jar, the resultant jar
file does not contain any class files.
D:[...]>dir ..\lib\hsqldbmin.jar
Volume in drive D is DATA
Volume Serial Number is B4CC-9308
Directory of D:[...]\hsqldb\lib
09/21/2005 01:13 PM 467 hsqldbmin.jar
1 File(s) 467 bytes
0 Dir(s) 2,916,093,952 bytes free
D:[...]\thirdparty\hsq
>jar tvf ..\lib\hsqldbmin.jar
0 Wed Sep 21 13:13:54 MDT 2005 META-INF/
362 Wed Sep 21 13:13:52 MDT 2005 META-INF/MANIFEST.MF
The other jar targets build fine.
Thanks.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

651
#651 columnDefinition disallows NOT NULL DEFAULT value
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It appears that <columnDefinition> makes it either-or
choise, which is... unexpected and somewhat inconvenient.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

652
#652 OOM when deleting from tables with a lot of  rows
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
create cached table articles
( id integer generated by default as identity primary key
, submitted timestamp not null
, subject varchar(50)
, text varchar(1000)
);
all records have:
- subject = 'Test record NN'
- text = 'This is article NN in this batch (MM to go).'
There was 1983 "batches" 100 "articles" each, i.e.
198,300 rows in the table.
DELETE FROM articles;
java.lang.OutOfMemoryError: Java heap space
SQL Error at 'stdin' line 3:
"delete from articles"
out of memory
select count(*) from articles;
160293
DELETE FROM articles;
java.lang.OutOfMemoryError: Java heap space
SQL Error at 'stdin' line 5:
"delete from articles"
out of memory
select count(*) from articles;
119041
and so on...
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

653
#653 Cannot rollback to named savepoint
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Creating a named savepoint and attempting to rollback
to it after issuing a single SQL command fails with the
following error:
java.sql.SQLException: Savepoint not found: savepoint
Attached is a small example that demonstrates the problem.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

654
#654 Scalar functions X Distinct
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
from : dellaColetta@hotmail.com
the query :
select distinct tipojuridico, left(razaosocial, 5) from
pessoa;
raise :
ORDER BY item should be in the SELECT DISTINCT list
Error Code: -71 / State: 37000
for any scalar function used then left (ex. substr...)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

655
#655 how IFNULL work?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Not really undestand why this dont work,
for examle, in engine in memory
for execute use HSQL database manager,
Specification-Version: 1.8.0
Create Table Y(id int not null)
and statement
Insert Into y Select IFNULL(Max(ID),0) from y
give error
Attempt to insert null a non-nullable column: column:ID
table Y
how i may do that in one statement?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

656
#656 HSQLDB will not load after a restart
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am having a problem where I have to reboot my app
server anytime there is a problem or I make an app
server change. This is because if I stop and start just
the Cocoon application the HSQLDB does not start. On
initial boot of the app server everything works just fine. I
am running WebSphere 6, but I've also deployed this
cocoon application on Tomcat and experience the same
results. Things I have tried so far to resolve:
1) Upgraded to latest version of HSQLDB 1.8.0.1
2) Modified web.xml to use the ParanoidCocoonServlet
3) Changed the init-classloader parameter to true
Has anyone experienced similar problems with
HSQLDB? I've done a netstat before trying to restart
and the HSQLDB port (9002 in my case) had been
successfully released. I can get around this by
rebooting my app server every time, but it would
certainly be a lot easier to just restart the cocoon
application.
Relevant section of web.xml
<init-param>
<param-name>load-class</param-name>
<param-value>com.ibm.servlet.classloader.Handler
org.hsqldb.jdbcDriver
org.apache.cocoon.transformation.LDAPTransformer</pa
ram-value>
</init-param>
Relevant section of cocoon.xconf
<datasources>
<jdbc logger="core.datasources.dashboard"
name="dashboard">
<pool-controller max="10" min="5"/>
<dburl>jdbc:hsqldb:hsql://localhost:9002</dburl>
<user>dashboard</user>
<password>d@1abas3</password>
</jdbc>
</datasources>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

657
#657 TriggerSample doesn\'t work in Server mode
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In the 1.8.0 release, I found the TriggerSample
example which run perfectly in Mem mode.
When I change the url for
jdbc:hsqldb:hsql://localhost/, I found two issues :
1) impossible to create audit table due to the fact
that the server needs an argument for tn VARCHAR
2) once the audit table created with tn
longvarchar, the server hang during the audit phase -
impossible to insert values in audit table. If I skip
this audit phase all run OK, but it is the audit
functionality that I want to use in an OOBase document.
Best Regards
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

658
#658 timestamp format bug
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
hello,
it seems like timestamps are stored in a not
jdbc-typical way which causes the system to actually
store another value than the one provided. the problem
is the representation of the nano seconds: the
java.sql.Timestamp class likes to represent them in a
way where you can cut the ending zeros. so for example:
2005-10-13 19:31:34.046
has 46000000 nano seconds. hsql likes to cut the first
zeros and preserve the ending ones. the bad thing about
that is, that it seems to be mixed up in the code so
the result is flawed. because
2005-10-13 19:31:34.46000000
has 460000000 nanos in jdbc representation (10 times
more than it should have).
the problem occurs when the db is shut down and
restarted. after that the values are flawed. i've
attached a testcase to reproduce the problem.
i'm using hsql 1.8.0_2 and it seems to work with cached
and memory tables. actually, when using memory tables
you can see the following line in the script:
INSERT INTO TIME VALUES(1,'2005-10-13 19:31:34.46000000')
which is pretty much different to
INSERT INTO TIME VALUES(1,'2005-10-13 19:31:34.046')
as explained above
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

659
#659 build for JDK1.3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Have any one successful build project for JDK 1.3?
It keep complaint about
java.lang.NoClassDefFoundError: java/sql/Savepoint,
which I believe it is JDK 1.4 things
Thanks
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

660
#660 can not make a remote jdbc connection
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I am trying to make a remote connection to a hsqldb
server running on another windows xp desktop from my
latop, but got a java.sql.SQLException: socket creation
error:
java.sql.SQLException: socket creation error
at org.hsqldb.jdbc.Util.sqlException(Unknown
Source)
at
org.hsqldb.jdbc.jdbcConnection.<init>(Unknown Source)
at org.hsqldb.jdbcDriver.getConnection(Unknown
Source)
at org.hsqldb.jdbcDriver.connect(Unknown Source)
at
java.sql.DriverManager.getConnection(DriverManager.java:539)
at
java.sql.DriverManager.getConnection(DriverManager.java:189)
at org.hsqldb.util.RCData.getConnection(Unknown
Source)
at org.hsqldb.util.SqlTool.main(Unknown Source)
following is the section from sqltool.rc file:
urlid remote-sa
url jdbc:hsqldb:hsql://10.0.0.10:9001/xdb
username sa
password
the local connection on the desktop was ok, the
'netstat -a' shows that the hsqldb server is listening
on the default port 9001, and the firewall on the
desktop was disabled.
Not sure it is a bug (I don't beleive), but any
suggestion will be appericated.
Thanks,
Marlin
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

661
ArrayIndexOutOfBoundsException on index creating
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello, hsql team!
The next exceptions happens very often when some
indexes are created.
If I restart the hsql server, the indexes are created
without problems. But, after sql statements described
beneath, creating of indexes causes this exception:
To get the exception I repeat the next statements:
Regards,
Yarick.
P.S. my profile: Pentium4, winXP, java 1.5.0_05,
hsqldb-1.8.0.2
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

662
#662 NFS not supported anymore
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We are using HSQLDB with JBoss 3.2.7. When we use
HSQLDB version 1.8.0.2 we can't start the database
when the files are on a NFS mount. With 1.8.0.0 (default
JBoss) it worked fine.
Vincent de Weger
hsqldb@traffic-its.nl
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

663
#663 NullPointerException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am getting this null pointer in the middle of a very
large transaction ~45000 records over 121 tables. The
version is 1.8.0.2
And one of my tables becomes corrupted though I am not
sure if it is the cause or result of this exception.
While I can't send you the database due to security
issues, I can run instrumented code to test a fix. I
will also see if I can narrow it down over the weekend.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

664
#664 JOIN on columns fail if alias with same name exists
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following select statement
SELECT t1.test, t2.idtest2 as idtest from test1 t1
but still an empty result which is wrong.
Regards
Stefan
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

665
#665 Desc order by doesn\'t work with alias columns
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hsqldb 1.8.0_2 has a bug in desc order by when using
columns aliases: it doesn't order data in descending
order. I found out this bug using Hibernate 3.0.5
generated queries.
The following commands are able to reproduce the bug
condition:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

666
#666 Mac OS X service instructions incomplete
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Mac OS X does not have a useradd command; instead, one needs
to run the following to create an hsqldb group and user.
Unfortunately, the commands below do not autogenerate a uid/gid,
so one has to be selected. I chose 101.
Doing this eliminates the need to create a full-fledged System user, with it's own (fat) home directory while maintaining the security of an hsqldb user. For a home dir for hsqldb, create /Library/Hsqldb/ and store your data there. This is where OS X has other services keep their local files.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

667
#667 null value as boolean error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
create table test (state char(1));
The same problem occurs if instead of false a subselect
of the form 'id in (select id from othertable)' is used.
Release 1.8.1 and 1.8.2, not yet tested with earlier
release.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

668
#668 PreparedStatement: setInt() does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If i have created a Column with Type OTHER I cannot
save an Integer as Integer.
If I try to setInt I get an Exception which say cannot
convert from Long to ??? (since I already applied my
own workaround, don't know the EXACT ExceptionType -
but I remember clearly it has to do with conversin into
Long)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

669
#669 PreparedStatement: setObject fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have created a Column with Type OTHER and I want to
save PageFormat I get an Exception.
java.lang.ClassCastException: java.awt.print.PageFormat
at
org.hsqldb.jdbc.jdbcPreparedStatement.setParameter(Unknown
Source)
at
org.hsqldb.jdbc.jdbcPreparedStatement.setObject(Unknown
Source)
Is it now supported or not?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

670
#670 Error with the \'LIMIT\' at the and of \'SELECT\' statement
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Good day.
When i execute 'SELECT' statement without anything
between table name and 'LIMIT' at the end of statement,
for example:
SELECT * FROM customer LIMIT 5;
then error occurs with message:
'Unexpected token: 5 in statement [5] / Error Code: -11
/ State: 37000'
When i add 'ORDER BY' or 'GROUP BY' or 'WHERE' to the
statement, eg:
SELECT * FROM customer ORDER BY id LIMIT 5;
or
SELECT * FROM customer WHERE id > 5 LIMIT 5;
then it executed successfully.
Thank you for a great work (and sorry for my English).
Alexei.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

671
#671 Built-in functions NOW, CURDATE, CURTIME return null
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The built-in functions NOW, CURDATE, CURTIME return
null. See the related functions in org.hsqldb.Library.
In earlier releases (e.g. 1.7.1) the functions
returned a valid Date, Time ir Timestamp.
So for instance
CREATE MEMORY TABLE MYTABLE(ID VARCHAR NOT NULL
PRIMARY KEY,LASTMODIFIED TIMESTAMP)
INSERT INTO MYTABLE VALUES('MyID',NOW)
results in a null value i.s.o. the current timestamp
in the LASTMODIFIED column of the newly created
MYTABLE record.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

672
#672 UNION fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Release: hsqldb 1.8.0.1
OS: Windows XP
I have a table called "sta_authentication", this
table has >40000 rows.
When I tested UNION and got some strange results. Try
and execute the following query:
SELECT SUM(inner_count) FROM (
SELECT COUNT(authentication_id) AS inner_count
FROM sta_authentication
WHERE authentication_id<10000
UNION
SELECT COUNT(authentication_id) AS inner_count
FROM sta_authentication
WHERE
authentication_id>10000 AND
authentication_id<20000
)
This results in 19999 rows, it's correct.
Now change:
authentication_id>10000
To:
authentication_id>=10000
This results in 10000 rows, it's WRONG!
The correct should be 20000 rows.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

673
#673 Criteria order on date/time, setMaxResults/setFirstResult
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
With Hibernate (3.0.5) and HSQL (1.8.0.2) when I'm
trying to get the latest object with Date/Time fields,
I get unexpected results which seems to point to off by
one errors.
With 10 objects, this returns unexpectedly the next to
last:
criteria.addOrder(Order.desc("startDate"));
criteria.addOrder(Order.desc("startTime"));
criteria.setMaxResults(1);
criteria.setFirstResult(0);
While this works:
criteria.addOrder(Order.desc("startDate"));
criteria.addOrder(Order.desc("startTime"));
criteria.setMaxResults(1);
criteria.setFirstResult(1);
I'll attach my testcase to demonstrate. Note that I've
found this at work and the same code with Microsoft SQL
worked.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

674
#674 Transfer and QueryTool missing from standard jar
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The Transfer and QueryTool utilities are now missing
from the standard jar (target "hsqldb"). I see this
change was the result of revision 1.69 of build.xml,
which has as log message only "post 1_8_0 RC11",
which I cannot interpret. The documentation
continues to claim that these utlities are included,
so I don't know if this was an intentional change.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

675
#675 Can not have filed with "#" in filed name
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
create table tmp_1134119839218
(
SNAP_ID BIGINT,DBID BIGINT,INSTANCE_NUMBER
BIGINT,TEXT_SUBSET VARCHAR(31),SQL_TEXT VARCHAR
(1000),SQL_ID VARCHAR(13),SHARABLE_MEM BIGINT,
SORTS BIGINT,MODULE VARCHAR(64),LOADED_VERSIONS
BIGINT,FETCHES BIGINT,EXECUTIONS
BIGINT,END_OF_FETCH_COUNT BIGINT,LOADS BIGINT,
INVALIDATIONS BIGINT,PARSE_CALLS BIGINT,DISK_READS
BIGINT,DIRECT_WRITES BIGINT,BUFFER_GETS
BIGINT,APPLICATION_WAIT_TIME BIGINT,
CONCURRENCY_WAIT_TIME BIGINT,CLUSTER_WAIT_TIME
BIGINT,USER_IO_WAIT_TIME BIGINT,PLSQL_EXEC_TIME
BIGINT,JAVA_EXEC_TIME BIGINT,
ROWS_PROCESSED BIGINT,COMMAND_TYPE BIGINT,HASH_VALUE
BIGINT,OLD_HASH_VALUE BIGINT,VERSION_COUNT
BIGINT,CPU_TIME BIGINT,
ELAPSED_TIME BIGINT,OUTLINE_SID
BIGINT,OUTLINE_CATEGORY VARCHAR(64),CHILD_LATCH
BIGINT,SQL_PROFILE VARCHAR(64),
PROGRAM_ID BIGINT,PROGRAM_LINE#
BIGINT,PLAN_HASH_VALUE BIGINT,COST BIGINT
)
fails due to use of "PROGRAM_LINE#" field. Table is
created automatically by using metadata from other
RDBMS.
Error: Unexpected Token: # in statement [...]
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

676
#676 data not fully loading in 1.8.0 with hibernate
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We have an application that uses hibernate 2.1.8. We
are preloading data into the db with a small java app.
We have been loading this data into a postgresql
database for a while with out issue. Wanted to create
a portable version so we moved the database over to
hsql 1.8.0. The data loader would not load all data
completely and often varied in which data was being
loaded for certain tables. The majority of data was
there but a few rows were missing. Switching to 1.7.3
everything seems to work normally.
Using the hsql standalone engine.
Feel free to contact me for additional information.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

677
#677 \"Order by <column_alias>\" causes floating error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Issue:
1. hsqldb would not issue streight SQL error during
parsing
2. Error is floating - Order by works in one case and
failes in another
SQL:
select 1 DELTA_TYPE ,a.value - b.value EXEC
...
order by 1, EXEC -- > Bad order by
Error:
java.lang.ClassCastException
at org.hsqldb.Column.compare(Column.java:735)
at org.hsqldb.Result.compareRecord
(Result.java:1095)
at org.hsqldb.Result.sortResult
(Result.java:1047)
at org.hsqldb.Select.sortResult
(Select.java:702)
at org.hsqldb.Select.getResult(Select.java:494)
at
org.hsqldb.CompiledStatementExecutor.executeSelectState
ment(CompiledStatementExecutor.java:339)
at
org.hsqldb.CompiledStatementExecutor.executeImpl
(CompiledStatementExecutor.java:119)
at org.hsqldb.CompiledStatementExecutor.execute
(CompiledStatementExecutor.java:90)
at org.hsqldb.Session.sqlExecute
(Session.java:1228)
at org.hsqldb.Session.execute(Session.java:846)
at
org.hsqldb.jdbc.jdbcPreparedStatement.executeQuery
(jdbcPreparedStatement.java:377)
Code Section:
>>>>>>>>>>
case Types.NUMERIC :
case Types.DECIMAL :
i = ((BigDecimal) a).compareTo
((BigDecimal) b);
break;
>>>>>>>>>>>
Values:
Long - a=1929
Long - b=1928
Error:
((BigDecimal) a) -> Cannot cast an instance of "class
java.lang.Long (no class loader)" to an instance
of "class java.math.BigDecimal (no class loader)"<
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

678
#678 Name of constraint in case of vilolation not available
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Submitted by Michael Codini
When running the following script a primary key
violation is produced.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

679
#679 Synchronize problem with small .log
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
java.lang.NullPointerException at
org.hsqldb.persist.DataFileCache.get(Unknown Source)
The NPE occurs when the .log file reaches its limit
of 10 MB, while in the process of removing many rows
in chunks of 1000 rows for each commit.
Using .log default size, 200 MB the NPE never occurs,
my work-around for now.
I have tested both latest release and (module hsqldb-
dev) CVS-tags: hsqldb_1_8_0_3 and HEAD, same problem.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

680
#680 Wrong error message (alter table)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I had a create table intruction:
CREATE CACHED TABLE document (..., create_date timestamp)
The problem I am reporting:
After running
ALTER TABLE document ADD COLUMN create_date timestamp
I received this error message:
java.sql.SQLException: Column constraints are not
acceptable in statement [ALTER TABLE document ADD
COLUMN create_date timestamp]
although I should receive column already exists error.
Not a big thing, but I was looking for it for some time.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

681
#681 Exception caching gives incorrect stack traces
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Exceptions for features not supported are cached.
For example; in Connection.getTypeMap() rather than
creating a new exception, it returns a reference to a
static final exception (stored in Util) that is
initialised when first refereneced, hence stack trace
is frozen to be the stack trace when exception is first
created.
public synchronized Map getTypeMap()
{
throw Util.notSupported; // static final reference
}
This leads to very confusing debugging! (esp if the
execption is for a different feature than was first
called).
For example:
Connection c = DriverManager.getConnection( .. );
try
{
c.getTypeMap(); // XXX
}
catch( SQLException e )
{
; // swallow exception
}
System.out.println("Foo");
c.getTypeMap(); // YYY
The exception thrown from point YYY has the stack trace
of exception thrown from XXX! Even though 'Foo' is
printed to the console!
Fix - throw new UnsupportedException rather than using
static finals to cache.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

682
#682 ResultSetMetaData.getTableName(int col)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi
when doing a select with an alias as follows you don't get the alias but get
the table name instead
select name as n from company as c;
ResultSetMetaData.getTableName(1) returns company not c like expected.
This happens in the latest 1.8.02 version of hsldb..
Thanks!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

683
#683 Multiple Sequences Cause Multirow Return with Next Value
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I tested this under 1.7.2.2 and 1.8.0.2
Create multiple Sequences:
CREATE SEQUENCE test AS BIGINT START WITH 0;
CREATE SEQUENCE test1 AS BIGINT START WITH 0;
CREATE SEQUENCE test2 AS BIGINT START WITH 0;
Retrieve the next value for sequence test1:
SELECT NEXT VALUE FOR test1 FROM
INFORMATION_SCHEMA.SYSTEM_SEQUENCES;
This will return 3 rows instead of just one with the
values:
1
2
3
Repeating the SELECT will produce a result of:
4
5
6
If you change the SELECT to a different sequence like
test2, it will return a rowset of 3 as follows:
1
2
3
So each sequence maintains individual counts but
instead of a single row being returned, the rowset is
based on the total number of system sequences created.
This makes it difficult to maintain multiple sequences
with the expected increment.
submitted by:
Shawn Potter <spotter@invoqsystems.com>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

684
#684 Inserts if some variables not bound
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Code:
===
ps = conn.prepareStatement("INSERT INTO t (c) VALUES (?)");
// nothing here
ps.executeUpdate();
===
Executed successfully, however value for column c is not bound.
SQLException should be thrown.
Found in 1.8.0.1.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

685
#685 DISTINCT does`nt work for functions
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
DISTINCT does`nt work for functions in version 1.8.0.2
The bug is described in
http://sourceforge.net/mailarchive/message.php?
msg_id=13893701
sql> create table tbl (dc date);
sql> insert into tbl values ('2004-11-10');
1 row updated
sql> insert into tbl values ('2004-9-12');
1 row updated
sql> insert into tbl values ('2005-11-18');
1 row updated
sql> select distinct year(dc) from tbl;
SQL Error at 'stdin' line 6:
"select distinct year(dc) from tbl"
ORDER BY item should be in the SELECT DISTINCT list:
org.hsqldb.Expression@72ffb in statement [select
distinct year(dc) from tbl]
I guess it caused because the code
} else if (exprType == FUNCTION) {
function.
collectInGroupByExpressions(colExps);
} else if (exprType == CASEWHEN) {
eArg2.
collectInGroupByExpressions(colExps);
was added to the specified version.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

686
#686 Add note that text tables not supported with res proto
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Per thread:
Since text tables are not supported via the res
protocol it would be good to have a note stating such
in the documentation for the sections describing text
tables and the res protocol.
Bob
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

687
#687 Batch updates lose the original error message.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am using HSQLDB 1.8.0.2 and am noticing batch updates
don't return detailed error messages. For example, if
an underlying update fails because of a unique
constraint violation, HSQLDB only reports "failed
batch" to the client. Here is a specific example:
I'm stepping through the code in the
Session.sqlExecuteBatch(Result) method.
Line 1093 looks like this:
in = compiledStatementExecutor.execute(cs);
This results in a unique constraint violation, so the
"in" Result object represents an error. Here are some
of the fields from that object:
mode = 2 (ResultConstants.ERROR)
mainString = "Unique constraint violation: SYS_CT_206
in statement [...]"
subString = "23000"
statementID = -104 (corresponding to
Trace.VIOLATION_OF_UNIQUE_CONSTRAINT)
So at this point, the "in" Result looks correct. But
the last few lines of the Session.sqlExecuteBatch
method seem to discard the error information:
out = new Result(ResultConstants.SQLEXECUTE,
updateCounts, 0);
return out;
So the "in" object that contains the error details is
never sent to the client, making it very difficult to
determine the true source of the problem.
Instead, clients see something like
java.sql.BatchUpdateException: failed batch", with a
vendor code of 0.
Also see this forum posting:
http://sourceforge.net/forum/message.php?msg_id=3337445
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

688
#688 multi-column index not always used
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
hsqldb 1.8.0.2 does not seem to be using multi-column
indexes for multi-column queries if a single-column
index matches and was created earlier. Example:
If I drop index1, the explain plan will then show
indexMultiColumn as being used, so it is a valid index,
it's just not being chosen when it would be the most
appropriate index...
--Erich
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

689
#689 PreparedStatement run as Admin
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
All PreparedStatements are run with Admin privs. Giving
access to all tables for any user. This is a big
security problem.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

690
#690 Connection is broken: org/hsqldb/lib/ArrayCounter
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am using version 1.8.0.2
I execute this query:
select * from xcl_lists where id = 469843432;
from the HSQL Database Manager and I get the titled error.
I execute this query:
select * from xcl_lists where id = 469843431;
and the result is returned fine.
I call script <script_file>
and all records are present. The table is a memory
table containing two columns: id and ref_count. id is a
primary key.
Within my code, I'm fairly sure something happened with
the write for this record and although JDBC reported a
successful update, the next call to identity() came
back with the previous written record's id.
Please let me know if I need to be concerned about this
behaviour.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

691
#691 concat(round(xxx, 0), \' yy\') appends unwanted .0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello there, I observed the following behaviour with hsqldb distributed with OpenOffice Base 2.0 (with Button "Run SQL command directly" toggled):
select round("thickness",0) as "thickness"
from "material";
thickness
3
4
5
(thickness being a float column). This is what I expect. Now, trying to append a string to the result, I try
select concat(round("thickness",0), ' mm') as "thickness"
from "material";
which produces "3.0 mm" instead of "3 mm". Same behaviour with "||". Is this intended?
Regards, Ulf
<ulf.mehlig@gmx.net>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

692
#692 Assert failed: beginNestedTransaction
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Version 1.8.0.2 HSQL, JBoss 4.0.3
The following was thrown by JBossMQ when commiting a
transaction:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

693
#693 IndexOutOfBounds on delete table.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
am using Tomcat 5.0.28, jdk 1.5, HSQLDB 1.8.0_02 and
access HSQLDB using a datasource. When executing:
I get the exception below. I recompiles Hypersonic
with debug.option on and stepped through the code.
Here are some of my observations:
In SchemaManager
The table mentioned is existing.
This exact code runs without any problems in 1.7.2
HSQLDB version.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

694
#694 The database is already in use by another process (embedded
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I run the following simple skript per jdbc in hsqldb
1.8.0.2 embedded mode twice. (The bug does not appear in server mode).
The output of the 1st run is OK.
In the 2nd run i become an error.
If i wait 10 seconds i become in the 3rd run the OK output.
The output of the 2nd run:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

695
#695 Adding column to table before a column with an index
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am using 1.8.0.1. Consider the following SQL:
ADD COLUMN col_three INT NULL BEFORE col_four;
The addition of col_one works fine, but the addition of
col_three fails with the error "There is an index on
the column to be removed". It appears that the code is
checking erroneously for indexes on the "BEFORE" column.
I glanced at the source and there does seem to be
common code that deals with addition, modification and
removal of columns, so you can imagine the removal
checking code being confused by the "BEFORE" column.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

696
#696 HSQL WebServer
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
We are using hsql webserver to provide a java Applet
through the network.
It seems there is something wrong with request
processing since our applet cannot be downloaded to the
web browser. After a lot of debug and trace into the
processGet method of the webserver it seems that
flushing the output stream doesn't end and the stream
is closed before the applet jar is sent to the browser...
Very strange isn't it ?
To strange bug, we have found an odd (and dirty) fix:
we added a sleep(2000) between the flush and the close
of the stream. Even if we know that this fix is really
dirty, it works in all cases for us. You should
probably investigate further to find the "real" fix
because all this looks like a thread problem.
NL.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

697
#697 NegativeArraySizeException from readUTF method
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have the database file, if interested. It is quite
large.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

698
#698 English collation is not supported
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I ran the following command
SET DATABASE COLLATION 'English';
but I got an exception. All the other collation names
are working exceptin 'English'.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

699
#699 NullPointerException while reading a text table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I get a NPE when I try to set the table source (SET
TABLE test SOURCE
StackTrace
NullPointerException:
The file:
I traced it down into the code and it looks like hsqldb
doesn't count the leading quote as a quote and
therefore misses the end of the first data line (it
assumes it's still inside a quotation and therefore the
newline is part of the value. The NPE is due to the
parser reaching the end of the file and skipping the
last (empty) line. This leads to not reading a real
line of data which causes the NPE inside the
DataFileCache#get (the object got from the store is null).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

700
#700 allow double args for modulo
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Currently mod() only allows ints, so you get weird
results if you try to use it on a table with doubles.
For example:
call mod(4.4, 2)
gives you 0 when you would expect .4.
I think the mod function should be expended to allow
double arguments. This might be as simple as changing
public static int mod(int i1, int i2) {
return i1 % i2;
}
to
public static double mod(double i1, double i2) {
return i1 % i2;
}
in Library.java and updating
src/org/hsqldb/resources/org_hsqldb_Library.properties.
But if mod should only be used with integer arguments
for some other reason, then mod(4.4, 2) should cause an
error instead of converting the arguments to integers.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

701
#701 Failing NULL value checks with complex expressions
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
There are still problems with complex expressions
evaluating to NULL in hsqldb 1.8.0.2, as I've already
mentioned in
Bug 1242448 [COALESCE/NVL/IFNULL(MAX(...), ...)
returns NULL]
and
Patch 1243098 [DECODE function].
After executing
CREATE TABLE DUAL(DUMMY VARCHAR);
INSERT INTO DUAL VALUES('X');
in hsqldb 1.8.0.2 the following calls produce false
results:
SELECT CASE NULLIF(23,23) WHEN NULLIF(23,23) THEN 1
ELSE 2 END FROM DUAL; -- 2!!
SELECT CASE NULLIF(23,23) WHEN null THEN 1 ELSE 2 END
FROM DUAL; -- 2!!
SELECT CASE null WHEN null THEN 1 ELSE 2 END FROM
DUAL; -- error!!
Suggestion: The evaluation error for NULL values should
be fixed somewhere in the object comparison code as it
affects some more functions and expressions.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

702
#702 NULL Pointer Exception from DatabaseMetaData
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
A NULL pointer exception is thrown if the
jdbcDatabaseMetaData class is instantiated from a
stored procedure. This occurs if the getMetaData method
of the Connection object passed into the procedure by
the database server is used.
The problem appears to be that the connProperties
member of jdbcConnection is accessed directly from the
jdbcDatabaseMetaData constructor. The Connection that
is passed to stored procedures as an automatic first
argument has connProperties set to NULL.
I have added a check to jdbcDatabaseMetaData to check
for NULL, and setting the useSchemaDefault to true by
default. I have noticed no adverse effects; all
DatabaseMetaData functionality seems to be fine.
However, this is just a work-around.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

703
#703 Query failures/syntax errors.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following have been tried on 1.7.3.3 and 1.8.0.2
and the results were the same. 1.8.0.4 failed as we
could not get the server started.
We are looking to find versions of the queries that
will work across HSQL, Oracle and SQL Server.
Example 1
This query works as written:
Select e.UserID, e.EventTime, cast (e.EventTime as
DATE),
l.LogStart, cast(l.LogStart as DATE)
from LogEvent e left outer join LogDaily l on (
e.UserID = l.UserID)
This does not:
Select e.UserID, e.EventTime, cast (e.EventTime as
DATE),
l.LogStart, cast(l.LogStart as DATE)
from LogEvent e left outer join LogDaily l on (
cast(e.EventTime as DATE) = cast(l.LogStart as DATE)
This does
Select e.UserID, e.EventTime, e.EventDate, l.LogStart,
LogStartDate
from (Select UserID, EventTime, cast(EventTime as
DATE) as EventDate
from LogEvent) e
left outer join (Select LogStart, cast(LogStart as
DATE) as LogStartDate
from LogDaily) l
on (e.EventDate = l.LogStartDate)
Example 2
This query works as written:
Select e.UserID, e.EventTime, DAYOFYEAR(e.EventTime ),
l.LogStart, DAYOFYEAR(l.LogStart)
from LogEvent e left outer join LogDaily l on (
e.UserID = l.UserID)
This does not:
Select e.UserID, e.EventTime, DAYOFYEAR(e.EventTime ),
l.LogStart, DAYOFYEAR(l.LogStart)
from LogEvent e left outer join LogDaily l on (
DAYOFYEAR(e.EventTime) = DAYOFYEAR(l.LogStart)
A strategy similar to Example 1 works here as well.
Example 3
This does work:
Select Cast(EventTime as DATE) as EventDate from
LogEvent
This does not work:
Select Cast(EventTime as DATE) as EventDate, count(*)
from LogEvent
Group by EventDate
A strategy similar to Example 1 works.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

704
#704 IndexOutOfBoundsException, SHUTDOWN deleted .data file
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HSQLDB 1.8.0.1 ... The DB was probably nearing the 1GB
mark.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

705
#705 SCHEMA and reloading server make tables public
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
These are the steps I am performing:
- Define my schema and nest all of the tables and
indexes with that definitions. These are CACHED tables.
Run with no problem
- I insert some data into a couple of maintenance
tables that I just created.
- I can select the table data by using the schema name.
It is running as designed.
- I then take the server down cold and restart it I see
the following. The schema that I created is still
there.but the tables that I defined to that schema are
now not associated with it. The tables are now public.
I am running the current 1.8.0.4
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

706
#706 Erroneous rejection of input sql?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am using HSQLDB 1.8.0.
The following query:
Select Top 100 * From SampleStars as a Group By a.RA, a.DEC
fails with the following error:
Since RA and DEC are valid columns in the SampleStars
table, I would expect this query to work - it certainly
does on the other DBMSs that I've tried.
Is this a bug or a feature request from your point of view?
NB The query :
Select Top 100 * From SampleStars as a
is fine.
Thanks,
Kona Andrews / kea@ast.cam.ac.uk
AstroGrid project
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

707
#707 docs for SQL features of TEXT tables erroneous
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I think HSQL is a great application and I love the text
table feature, but would you please change the
documentation on Text Tables to indicate that (as fredt
stated in April 2005) "Alterations to the table
structure are not supported for TEXT tables," even in
version 1.8.0.4
As was pointed out in the thread below, the docs
erroneously indicate that full SQL is supported by text
tables. The cryptic, "This operation is not supported
in statement <alter statement>" does little to disabuse
the user of that misconception.
From: fredt <fredt@us...>
Re: Add Column-not working for text tables
2005-04-22 11:51
Alterations to the table structure are not supported
for TEXT tables.
Fred
----- Original Message -----
From: <sudhakar@da...>
To: <hsqldb-user@li...>
Sent: 22 April 2005 06:54
Subject: [Hsqldb-user] Add Column-not working for text
tables
Hi,
I tried adding a column to a TEXT table through
the HsqlDbManager, it
gave the following error-
This Operation is not supported in Statement[alter
table dual add column
test_column varchar(5)]:Error code-73
Cant we add column"s in TEXT tables as we do in normal
tables ?.As described
in the documentation "The full range of SQL queries
can be performed on
these files", we should be able to alter the text
table. Am i missing
something?
please reply,
thanking in advance,
sudhakar.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

708
#708 Possible bug causing exception in subselect
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following code throws an exception with a message
"Column not found: MY_VALUE in statement". Note the
very similar query in the comment that happens to do
the same thing, but does not throw an exception:
This is on version 1.8.0.4, binary release.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

709
#709 NullPointerException while calling Server.shutdown() method
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
i'm starting and stopping a hsqldb server (1.8.0.4)
from my application.
If i want to stop the hsqldb server i use the shutdown
() from the org.hsqldb.Server class. While executing
the shutdown() method it came to the following
NullPointerException
It seems that the "protected void shutdown(boolean
error)" method called two times. One time from me and
another time from the finally statement at the end of
the run() method in the Server class which is called
after closing the socket from the first running
shutdown() method. I think both methods runs at the
same time and without synchronize this error occurs.
So I found a solution for that problem. I
changed "protected void shutdown(boolean error)"
to "protected synchronized void shutdown(boolean
error)". This change make sure that only one shutdown
() method is executed and not two or more.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

710
#710 hoe to get next value of sequence
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
i want next value of sequence to store this as a
student ID. for example the rows of table are student
ID (integer),name (String) and Data of join(Date).in
application i called sequence.nextval to getting unique
ID for student to store student information. when i
call this statement like seq_student.nextval which is
showing syntex error.how can i solve this
problem.please give a solution.while inserting how to
get seq_student.nextval.
my query is "insert into student
values(seq_student.nextval,name,12-03-2003)";
it is showing syntex error.please tell me how to solve
this solution.
sorry for my bad english.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

711
#711 Column Alias Incompatibility
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HSQLDB supports this:
SELECT ID I FROM TEST WHERE I>0
Other databases don't support it. I think HSQLDB
shouldn't support it as well, as it leads to
incompatibilities: the same query means something else
depending on the database used. Example:
CREATE TABLE TEST(ID INT);
INSERT INTO TEST VALUES(1);
SELECT ID+1 AS ID FROM TEST WHERE ID>1;
HSQLDB returns 1 row, all other databases 0 rows. It is
a valid query for all databases, but the result is
different. This is really bad if you want to write an
application that runs on multiple databases.
I suggest the behaviour should be changed in HSQLDB so
it is compatible to all other databases.
Thomas
(P.S. this is a forum post, but as I didn't get any
feedback there I create a bug entry for it).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

712
#712 Column Alias Incompatibility
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HSQLDB supports this:
SELECT ID I FROM TEST WHERE I>0
Other databases don't support it. I think HSQLDB
shouldn't support it as well, as it leads to
incompatibilities: the same query means something else
depending on the database used. Example:
CREATE TABLE TEST(ID INT);
INSERT INTO TEST VALUES(1);
SELECT ID+1 AS ID FROM TEST WHERE ID>1;
HSQLDB returns 1 row, all other databases 0 rows. It is
a valid query for all databases, but the result is
different. This is really bad if you want to write an
application that runs on multiple databases.
I suggest the behaviour should be changed in HSQLDB so
it is compatible to all other databases.
Thomas
(P.S. this is a forum post, but as I didn't get any
feedback there I create a bug entry for it).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

713
NullPointerException in jdbcConnection.close()
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
hsqldb v1.8.0.4
When I use Eclipse to turn on a break point exception for
NullPointerException I see that the Finalizer thread is
throwing a NullPointerException in
jdbcConnection.close() if the Connection object has
already been closed.
According to the source code there can be only one culprit:
should be
Thanks for HSQLDB !
Tony.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

714
#714 Expression.toString makes debugging difficult
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
ERROR hibernate.util.JDBCExceptionReporter - Not in
aggregate function or group by clause: org.hsqld
b.Expression@182ef6b in statement [select top ?
count(distinct context_id) as cnt, org_id from (sele
ct context_id, org_id from spm_invocation where
invoke_date between ? and ? and bs_id = ? group by
org_id, context_id) order by 1 DESC]
Note the Expression.toString() does not return anything
of any use so it is impossible to know exactly which
part of this query is causing a problem. I had to plug
it into mysql to get more helpful info.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

715
#715 Delete with unknown column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Having a table 'foo' with one column 'bar', the
following query returns an update count of '0' (zero):
"delete from foo where xyz = 'bla'". I would have
expected an exception, like "column 'xyz' not found for
table 'bla'" (at least Oracle behaves like that).
Did I miss something or is it a bug?
Rob
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

716
#716 Not a condition error when alias=real name
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
1.8.0.4
using the sample database,
SELECT SUM(TOTAL) AS TOTAL
FROM INVOICE
HAVING SUM(TOTAL) > 0
leads to the error
Not a condition/Error Code: -106
After changing the alias from TOTAL to TOTAL1
(something different from the real name), the error
is evaded.
The software I'm using needs to set an alias with the
same name as the real column name. Needs to have it
fixed.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

717
#717 HSQL startup too slowly
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Now I use HSQL text DB to store data which are saved
into .CSV files. In the HSQL text DB, there are five
text tables, one of the tables is much larger than
others, called [table A].
I do a performance test for my application, made the
HSQL DB be about 500M Bytes. I think there are about 1
million records in [table A]. Then I restart the
application, it cost nearly 1 hour to startup the HSQL
text DB, too slowly. And after the HSQL text DB
startup, almost any operation could not be processed.
How to resolve the problems? thanks very much!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

718
#718 SELECT: S1000 General error java.util.NoSuchElementException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Simple SELECT statements like SELECT COUNT(*) FROM
TABLE fail:
Unable to get row count for table PUBLIC.SENDJOBS.
Using value '0': java.sql.SQLException: S1000 General
error java.util.NoSuchElementException
java.util.NoSuchElementException
at
org.hsqldb.Index$IndexRowIterator.nextUnknown Source)
at org.hsqldb.TableFilter.next(Unknown Source)
at org.hsqldb.Select.buildResult(Unknown
Source)
at org.hsqldb.Select.getSingleResult(Unknown
Source)
at org.hsqldb.Select.getResult(Unknown Source)
at
org.hsqldb.CompiledStatementExecutor.executeSelectState
ment(Unknown Source)
at
org.hsqldb.CompiledStatementExecutor.executeImpl
(Unknown Source)
at org.hsqldb.CompiledStatementExecutor.execute
(Unknown Source)
at
org.hsqldb.Session.sqlExecuteCompiledNoPreChecks
(Unknown Source)
at
org.hsqldb.DatabaseCommandInterpreter.executePart
(Unknown Source)
at
org.hsqldb.DatabaseCommandInterpreter.execute(Unknown
Source)
at
org.hsqldb.Session.sqlExecuteDirectNoPreChecks(Unknown
Source)
at org.hsqldb.Session.execute(Unknown Source)
at org.hsqldb.jdbc.jdbcStatement.fetchResult
(Unknown Source)
at org.hsqldb.jdbc.jdbcStatement.executeQuery
(Unknown Source)
at
org.hsqldb.util.DatabaseManagerSwing.getRowCounts
(Unknown Source)
Database seems corrupted. Files are available.
Are there any tools available to repair hsqldb files?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

719
#719 Refer to constants by name in Server.getState() javadoc
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Clarify javadoc of Server.getState():
from:
I'm reporting this because I had to look at the source
to find out the symbolic name of the constants.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

720
#720 Max Aggregate function does not work for me
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I have this table schema:
- When i run the following query:
I get an exception:
"Not in aggregate function or group by clause"
The same query in posgrees and MySQL works fine.
Regards, Pablo
PS: I've tried this with hsqdb v 1.8.0.5 and 1.8.0.1
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

721
#721 getColumnClassName returns "double" for function columns
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The Java API for
ResultSetMetaData.getColumnClassName(int i) says that
it should return the "fully-qualified name of the Java
class" type that will be returned by getObject for a
column.
However, when it is called for a column that is the
result of a function returning a double, the result is
the String "double". Since this is not a class name,
it cannot be used to get a Class object through
Class.forName. It also does not follow the description
quoted above.
This can be most easily demonstrated by opening a
connection to an existing database (server or local),
choosing a table from that database, and running the
query: "select pi() from TABLE_NAME", replacing the
table name. This will need to be done in code, so that
you can call getResultSetMetaData() on the ResultSet,
and then call getColumnClassName(1) on the meta data
object. It will return "double".
This does not only happen with pi, however. I first
noticed it with floor, but have tested it with acos
and abs as well.
For, example in an table called EXAMPLE with a column
a of type bigint, and data:
a = (100, 150, 200) Each item is a record.
Execute the query "SELECT floor(A/5) from EXAMPLE" and
you will get the same results from above.
One workaround is to nest the select statement inside
another statement, such as "select * from (select
floor(a/5) from example)" This fixes the problem,
causing getColumnClassName to return
"java.lang.Double," but this isn't feasible for our
application.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

722


        SourceForge.net: Log In to SourceForge.net


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
[]
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

723
#723 When adding a primary two column in one table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Error When adding a primary, two column in one table.
find HSQL1.8.0rc1 ~ HSQL1.8.0rc5
error message :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

724
#724 PK constraint picking up name of FK constraint.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Run the ant build file in the attached archive. It
creates a small db in /tmp/bugdb The script will
execute successfully. Run it again. It will remove the
contents of the existing db and recreate it - again
successfully. Run the script a third time and it breaks
with the following error:
The constraint FK_QUESTIONS_1_FROM_N_UQID was given to
a foreign key constraint and yet the script file
assigns it to a primary key constraint. This is very
wierd and very wrong.
Possibly related to 1387237 ?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

725
#725 Add OSX-friendly path to search for hsqldb.cfg
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
OS X likes to have non-system daemon configs stored in
(Yes, Hsqld is capitalized on purpose).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

726
#726 Tables/Views not found after db restart.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After running the attaced build set (similar to that
used to identify bug #1547479), I reconnect to the
database and a large number of tables and views are
missing. No trace of them exists in either the .script
or .log file and no data can be selected.
There appears to be a cut-off point after which all
table creation statements, data entries and view
creations are lost. No SQL error is reported in the
schema creation.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

727
#727 Correct HSQL version number
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
hi there,
HSQL Database Engine - 1.8.0
is the result of
logger.info(metaData.getDatabaseProductName() + " - "
+
metaData.getDatabaseProductVersion());
Yet I'm using 1.8.0 version 5.
I suppose there were fixes between 1.8.0.1 and xxx.5 ?
For correctness, I believe the version number should be
the full one.
cheers,
Geert
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

728
#728 The .script got corrupted!
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When connecting the following NPE occurs:
SQL error: S1000 General error
java.lang.NullPointerException in statement [SET TABLE
STA_NODE INDEX'32 32 2'] error-code=40, state=S1000
What has happened, why is my .script file corrupted?
Please test the attached database to reproduce the
problem.
How can I restore the database?
Best Regards,
Krister
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

729
#729 Version Number out of date
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HSQL Version Number
is out of date: 1.8.0.5 (needs to be 1.8.0.7).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

730
#730 Negative SQLException error code
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Current behavior:
Expected behavior: SQLException.getErrorCode() returns
int value which could be compared to Trace.XXXX without
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

731
#731 NullPointerException on Tomcat 5.5 at org.hsqldb.lib.HsqlTim
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We are experiencing a persistent NPE after database
shutdown with an application running in Tomcat 5.5.
Stack track from catalina.out follows:
HSQLDB version is 1.8.0.7
Tomcat version is 5.5.20.
The error does not seem to manifest on Tomcat 5.0.28.
We do an explicit SQL SHUTDOWN call from the
application when the application is shutdown.
With "hsqldb.applog=1" we see "Database closed" in
the .app.log file. A few seconds later the above
stack dump is generated in catalina.out.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

732
#732  patch 1329486 is not merged with latest release
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have just downloaded the hsqldb1_8_0_7, however I do
not see patch 1329486 "ensure HsqlTimer thread quits".
Moreover I checked the references of org.hsqldb.lib.HsqlTimer.shutDown() and it has appeared there is no call of this method within hsqldb sources.
Is that expeced?
I use hsqldb with webapp: without mentioned patch each
webapp server's shutdown results in following exception:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

733
#733 Read-Only setting is ignored
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Just to follow the discussion at the Help-Forum:
I include the Log from my application which shows, that
the update is executed without any problems. No
exceptions, nothing.
I would expect, that an SQL-Update would either result
in an Exception AND the values are not stored in the
DB. I get neither an Exception NOR a rejected update of
the DB (=values remain the same).
Either I have missed an option or ...????
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

734
#734 can't create view with aggregate extract select
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
this fails in 1.5 Java with 1.8 hsqldb
thrown:
Have managed to get this operating with the 1.4 JVM.
It's the "extract" that is not liked.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

735
#735 Order by is not working correctly?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
First create table:
Am I missing something?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

736
#736 Inner select with union returns bad results
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
bellow is a sample SQL script to demonstrate the
problem. The last select should show two rows, the row
with event_id=100 should show value=100, but the value
is empty (or null). When I change the
select id, null as value from Event_A
to
select id, 999 as value from Event_A
the value for event_id=100 is shown correctly
Tested with HSQL 1.8.0.7 (in memory).
Thanks for your support, Jan
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

737
#737 ClassCastException with union query
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
A ClassCastException is thrown by the server when
executing an union query with constants.
Here is a simple example:
An SQLException, wrapping the ClassCastException is thrown.
With a more complex query the ClassCastException is not
thrown by the JDBC driver within an SQLException but
only logged in hsql trace. After that the server
doesn't work anymore.
Regards
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

738
#738 SHUTDOWN COMPACT aborts at a TEXT table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Since my DB.data file continued to grow I discovered
"SHUTDOWN ABORT". Result (embedded DB driver):
--- snip (console output of my program) ----
SQL-DB update with command SHUTDOWN COMPACT
Couldn't shutdown DB.
The lines of the respective table in the script file
looks as follows:
Maybe HSQLDB has a problem with TEXT tables with only
one column?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

739
#739 length of binary is wrong by factor 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi!
The length function on columns of type "binary"
returns a wrong value, its two times of the value it
should be:
create table hallo (zack binary)
insert into hallo (zack) values ('0004ff')
select length(zack) from hallo
the last query returns a single row of 6, but it
should return 3.
From the docs:
Binary data starts and ends with ' (singlequote), the
format is hexadecimal. '0004ff' for example is 3
bytes, first 0, second 4 and last 255 (0xff).
Apart from the wrong length, storing and fetching the
binary data works well. So a "select zack from hallo"
returns the correct 3 bytes. The behaviour does not
change when using prepared statements. The length
function is wrong in the same way when using it in a
check constraint. Using varbinary or longvarbinary
shows the same results. Using function "octet_length"
instead of "length" returns 12 instead of 6.
The documentation specifies function "length" and
"octet_length" for strings only - nothing is said
about binary. I think, both functions should return 3
here.
Ralf.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

740
#740 buildJDK12.bat: ant jar building for Java 1.3 won't compile
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Since 1.8.0.5 - "NIOScaledRAFile.java" is missed in the
src-persist directory.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

741
#741 Bug in org.hsqldb.Servlet
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I found a minor bug but a show stopper within org.hsqldb.Servlet class.
In
You need to add forward slash before WEB-INF, otherwise if the real path will return C:\myproject\WebContent, dbStr will be C:\myproject\WebContentWEB-INF/database
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

742
#742 Wrong Information in your guide documentation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following section written like this:
Hsqldb Servlet
This uses the same protocol as the Web Server. It is used when a separate servlet engine (or application
server) such as Tomcat or Resin provides access to the database. The Servlet Mode cannot be started independently
from the servlet engine. The hsqlServlet class, in the HSQLDB jar, should be installed
on the application server to provide the connection. The database is specified using an application server
property. Refer to the source file hsqlServlet.java to see the details.
Correction:
It's not hsqlServlet.java but org/hsqldb/Servlet.java. There are no details in hsqlServlet.java because it's a dummy servlet class.
Also, could you add how to make connection to hsqldb servlet like putting the following line of code:
I have to trace & debug the source code just to figure-out the correct connection string for hsqldb servlet.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

743
#743 Not in aggregate function or group by clause
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
this statement work in every other dbms I use (PostgreSQL, Ingres, Oracle):
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

744
#744 Inner select not working
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The inner select is not working properly.
e.g. if we have a 1 column table: -
TEST (ID VARCHAR (10))
and we populate with some example data and do:
select * from test where exists (select * from test limit 0 2)
... does not work. I think its a problem with the limit 0 2 inside the inner select.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

745
#745 Locate function not working
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Locate function to search for the index for particular string in any string is not working and returning 0 in all cases.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

746
#746 Trigger Issue
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I have created a trigger on insert in the table and also define proper java file for trigger.Now after inserting a row in a table db is calling fire() method implemented in my java file. My problem is when i am trying to execute an statement in java file after trigger is fired, my database is getting hang.i can't do anything. After going through my trace i found that some problem is coming while calling DriverManager.getConnection(url,login,pw).
I am attaching the code to make it more clear.
Thanks
Aslam
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

747
#747 TEXT(cvs) tables does not work well with identity column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In any case that the db had a unsafe shutdown. or reopen a modified CSV file.
auto increment identity does not reset itself to a correct value. And duplicate key exception happen on next insert statement.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

748
#748 Not in aggregate function or group by clause
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
i've a problem with group by and having clause in inner queries, when the having clause references aliases from the outer select. This works fine in DerbyDB.
I get the following error:
SQL Error: -67, SQLState: 37000
Not in aggregate function or group by clause:
Here is the script to recreate the problem:
drop table PROCESSDETAIL if exists;
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

749
#749 forgets primary key constraint name
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In an empty database I create a new table with a named primary key constraint:
create table tab ( col integer, constraint blah primary key (col))
Then I ask for the name of this constraint:
select CONSTRAINT_NAME from INFORMATION_SCHEMA.SYSTEM_TABLE_CONSTRAINTS
With hsqldb_1_8_0_2 this correctly returns "BLAH", but with hsqldb_1_8_0_7 it returns "SYS_IDX_46".
This does not happen for check/unique/foreignkey constraints, there the name is ok in both hsqldb versions.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

750
#750 Data loss when on rapid startup -> shutdown sequence
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The attached sample application does the following:
First it registers a shutdown handler for itself. Then it calls startDatabase() to start a network-capable HSQLDB server using the Server class, and opens a JDBC connection to the server. Next it calls accessDatabase() to prepare, execute, close and null some PreparedStatements, which are:
OPTIONALLY it then calls Thread.sleep()
to avoid the bug. Currently that is commented out.
Having accessed the database, it calls System.exit(0) to trigger the shutdown thread. The shutdown thread calls stopDatabase() which closes and nulls the JDBC connection, issues Server.stop() then Server.shutdown(), and nulls the server.
Bug:
If the above process runs without calling Thread.sleep(), the database engine shuts down without data getting flushed to disk. Data loss occurs, but when Thread.sleep() is called, it doesn't and "db.log" contains the right data.
Tested:
On 1.5 and 1.6 series Sun JDK in a Gentoo Linux environment. Tested on different computers. The 1.6 JDK on one computer was installed from a Sun binary, the 1.5 JDK on the other computer from Portage.
Recreating:
Grab the attached Demo.java, copy hsqldb.jar into the same directory. Check sources and having ascertained their safety, issue:
I stumbled upon this by accident, while chasing another bug. For me it's non-critical, I can let it sleep. It however seemed like capable of biting someone, and I'm
not proficient enough with HSQLDB to fix it myself.
Best regards, lots of thanks for a neat database to play with, and good luck hunting those bugs! :)
Complication / fooException.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

751
#751 German '' character breaks "like" operations
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Found a bug in the German language / HSQLDB. HSQLDB does case insensitive "like" operations by making the comparison string upper case. Unfortunately, the upper case of "" is "SS". I was able to patch HSQLDB to resolve this issue, but I'm not comfortable that the fix is appropriate.
Here is a diff of src/org/hsqldb/Like.java that fixes the problem (and probably introduces others):
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

752
#752 Out of memory when manipulating indexes
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When a database with approximately 350 tables is manipulated by using either CREATE INDEX or the DROP INDEX statement, then it starts taking insane amounts of time to complete on tables with less than 10 records in them. This eventually runs the JVM out of memory. The JVM is already using the maximum amount of memory that will work cross platform. We are using the JDBC connection for communication.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

753
#753 hsqldbmin target doesn't produce jar
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello
Regarding build target hsqldbmin:
"ant hsqldbmin" doesn't produce the related jar file in the lib directory but produces a manifest-only jar file without classes nor resources.
I think this is due to a <include > tag present in the "hsqldbmin" target in the build.xml that override all.
Removing this "include" tag - and including all *.properties files in the build process - produce the correct and working jar file hsqldbmin.jar
bye
thanks for this great product anyway
bye
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

755
#755 A duplicate of bug 1400344 (almost)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
My query is virtually identical to bug 1400344,
except that the column is of type DateTime, and I wish to discard the Time part.
SELECT DISTINCT convert(element.deadline,Date) FROM element ORDER BY convert(element.deadline,Date) ASC
Because convert is not treated as a function it fails in the same manner a functions did.
I fixed the problem by adding CONVERT to the iscolumn
I am not convinced this is the correct fix, but is probably no worse than treating ADD etc in this manner.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

756
#756 org.hsqldb.jdbc.* can not be compiled with jdk 1.6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
As some interfaces (java.sql.*) have changed since Java 5, several files in org.hsqldb.jdbc can not be compiled any more with javac 1.6.0.
This is a stopper for using Java 6 in Openoffice. Please make hsqldb compatible with Java 6.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

757
#757 No support for reading data from older version using 1.8 jar
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If I try to read from older version of HSQLDB( in my case 1.7.2) using version 1.8 JDBC driver i get the following error:-
SQLException: java.sql.SQLException: Connection is broken: java.lang.OutOfMemoryError: Requested array size exceeds VM limit
I think newer version of JDBC driver should support reading from older database version.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

758
#758 Database URL of type res: is converted to lowercase
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
A Database URL of type res: is converted to lowercase
while parsing it (DatabaseURL.java:238). Thus it is impossible to open a database contained in a package hierarchy whose name contains uppercase characters. Java class loading is case sensitive with respect to both Class and Package names.
Package names containing uppercase characters are discouraged but not forbidden, so converting a database URL to lowercase should not take place, at least not for database type S_RES.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

759
#759 Throwing Exception: Starting Server using J9 in Windows CE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

760
#760 Exception: Starting Server in J9 under Windows CE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Throwing Exception: Starting Server using J9 in Windows CE Private: (?)
No
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

761
#761 SELECT LIST Does not allow literal 'ALL' as first column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Does not all 'ALL' (uppercase) as first column in the SELECT list.
Query works fine.
This happens only when 'ALL' is the first literal in the select field list.
Understand that ALL is a reserved keyword but it is not same as literal 'ALL' or is it?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

762
#762 select(select... union select...) broken (!!)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
on (at least) the latest hsqldb release (1.8.0.7) there's a bug in union when it's the source of a select. There's an older report of this which is filed as "feature request":
The problem is that these queries are not as uncommon as one might assume. E.g. when using hibernate for relational persistence and using union-subclassing, these queries are used often.
So i would consider this as a bug with high relevance rather than a feature request.
I've attached a test case that shows this bug.
Michael.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

763
#763 java.sql.Connection#createBlob() not implemented
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
do it ASAP, i do not want to introduce HSQLDB-specific code like "new org.hsqldb.jdbc.jdbcBlob(byte_array_var)" into my app, as it SHOULD be DB invariant.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

764
#764 STUCT typo
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In table INFORMATION_SCHEMA.SYSTEM_ALLTYPEINFO the record for the STRUCT type has "STUCT" as the value for the TYPE_NAME field.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

765
#765 Temporary Tables lose ON COMMIT PRESERVE ROWS
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If you create a temporary table with option ON COMMIT PRESERVE ROWS, then you add columns with ALTER TABLE ... ADD COLUMN... the attribute ON COMMIT PRESERVE ROWS gets lost.
Using the second form, the attribute ON COMMIT PRESERVE ROWS gets lost (as you can see in the database script file)
It seems that this problem exists with any subsequent ALTER TABLE command, not only ALTER TABLE ... ADD COLUMN...
Greetings,
--
Alex
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

766
#766 ORDER BY item should not be in the SELECT DISTINCT list
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Here is the script to create the test data:
When I now execute the following statement:
select distinct str from test order by pos;
I get following error message:
ORDER BY item should be in the SELECT DISTINCT list: org.hsqldb.Expression@90cb03 in statement [select distinct str from test order by pos] / Error Code: -71 / State: 37000
The spec of SQL does not require an ORDER BY item to be in the SELECT DISTINCT list.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

767
#767 Critical Date bug in latest version (1.8.0_7)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Suppose you insert a Date in a Date field (not a long representing an UTC for that Date) from a hsqldb table.
Then, when you read the date inserted it's not the same as the original date.
The 2 dates differ by a day or two. Even if you use TimeZone and Calendar and proper formating of the dates this still doesn't work.
This bug was not present in 1.7.3.3 version.
aureldu@gmail.com
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

768
#768 Session Class Swallowing Stacktrace
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The sqlExecuteBatch method in the Session class calls into the execute(CompiledStatement cs, Object[] paramValues) method of CompiledStateExecutor, which contains this code:
But when control returns to the Session.sqlExecuteBatch method, the result is not checked for errors.
Adding the following code to the org.hsqldb.Session class:
Results in the actual SQL exception being displayed. This is much better than what you now see, which is "Failed Batch".
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

769
#769 SQL: CALL TRUE; failes in 1.8.0.7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If you enter the statement
call true;
in the DataBaseManager connected to a HSQLDB server you will get an exception.
I stumbled on it while running the JUnit tests. Is a test case in testSQL.java, in particular testX1(). It will affect any stored procedure returning a boolean.
It does not occur when using when using the in memory engine.
Attached is a JUnit test.
The problem is in Expression.java.
A patch for 1.8.0.7 is attached.
In the repository it has already been fixed as I have seen. I filed this bug only for users of the current version. It will go away with 1.8.1.
Regards
Walter
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

770
#770 Exception using "sum" function in "case when" else clause
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
An exception is thrown for the following query:
select
case when 1<0 then 1 else sum(10) end as res from SOME_TABLE
However the following query succeeds executing:
select
case when 1<0 then sum(10) else 1 end as res from SOME_TABLE
The only difference is in the position of the aggregation function.
Both queries work on Oracle,DB2 and MSSQL.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

771
#771 RESTART WITH on text tables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Issue 1251640 needs to be reopened for 1.8.0.7.
The description was as follows:
"When using generated keys on text tables, the engine
adds ALTER COLUMN RESTART WITH statements in the script
file. If, after adding new records to the table the db
shuts down incorrectly, these values may not be
updated. If the db is then restarted, future inserts
into the table cause a unique constraint violation.
If the ALTER COLUMN statements are removed, the
database oprates correctly and continues with the next
number."
This issue was reportedly fixed in 1.8.0.2, but we are still experiencing the problem in 1.8.0.7.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

772
#772 Exception while using any of the Transfer Tool
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
There is a exception that occurs when using the Transfer,Dump,Restore Tools under the DB Manager(Below). This occurs for the version above 1.8.0.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

773
#773 identity() doesn't work with preparedStatement
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If I create a new record using a prepared statement and then, with the same connection, create a new preparedStatement and execute "CALL IDENTITY();" the ResultSet contains an Integer '0', not the ID of the IDENTITY column.
If I do the same thing, but use a standard Statement to call IDENTITY(), I get back the expected result.
I would expect to get the correct value of the new ID with either a Statement or a PreparedStatement.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

774
#774 ON UPDATE CASCADE does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using hsqldb version 1.8.0.7.
A virgin in-memory db, connecting with: jdbc:hsqldb:file:<path to folder + dbName> (nothing else touched and no configs altered).
While defining the following tables i get: error code: -11, state 37000... it says UPDATE from the fk cascade definition is an unexpected token - no matter if defined with constraint <name> or if i do it in table definition itself.
Here is a simple script that shows the problem:
great job done guys and hopefully this is easy to fix...
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

775
#775 Unsupported parenthesis on JOIN
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following query doesn't work:
It throws a SQLException with the message:
If I remove the parenthesis, then the query works perfectly.
The problem is that, even if for this query the parenthesis aren't needed, for queries with several outer and inner join you have to use parenthesis to manage the "join priority".
In other words, it isn't possibile to execute many real-world queries.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

776
#776 unsupported NUMERIC(precision,scale)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The engine does not understand the numeric type like standard SQL dictates.
Like DECIMAL NUMERIC can get a precision AND also a scale. But it looks like scales are not allowed today.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

777
#777 Incorrect right outer join result
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Right join output incorrect result, when using after inner join.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

778
#778 Constructor of org.hsqldb.test.TestBase has wrong assignment
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The power of the Eclipse syntax check reveals:
"Assignment has no effect":
Probably has to be changed to:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

779
#779 org.hsqldb.ServerConnection close()es incorrectly in run
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In org.hsqldb.ServerConnection.run(), the close() is called outside of try/catch, which sometimes leads to non-closure (in this case observed in TestSql, which throws due to ClassCastException and does not stop correctly)
The close() has to be called inside a finally, like
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

780
#780 org.hsqldb.test.TestSql.tearDown should call super.tearDown
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
org.hsqldb.test.TestSql.tearDown() should call super.tearDown():
Instead of:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

781
#781 org.hsqldb.test.TestSql.testX1() fails with ClassCastExcep..
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
See Bug #1737389, which is the same with another title, this is just a duplicate in case someone looks for "ClassCastException".
Problems occurs in the code:
This will cause the server to end
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

782
#782 setNull(..) in preparedStatement behaves strange
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I use the follwing code:
the code does not cause any errors or SQLExceptions, but the result set is empty after the call, though it shouldn't be.
If I replace the statement with
everything works fine.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

784
#784 ArrayIndexOutOfBoundsException on condition of type char
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm using the latest code from svn (https://hsqldb.svn.sourceforge.net/svnroot/hsqldb/base/trunk), but when I use a condition on a character column, and include a subquery in the from clause, I receive the error:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

785
#785 NPE in Database.connect
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am getting this wierd error and not able to figure this out. Also, If I try to perform the action in debug mode, I do not get this error.
Help?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

786
#786 Error casewhen in condition
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Error occurs when in condition use casewhen for example:
thanks
Danilo
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

787
#787 umlaut not working with text tables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After inserting a "" into a text table via jdbc, the text file contains corrupt characters. Reading the file via jdbc returns "?".
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

788
#788 Empty rows in text tables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Deleted rows in a text table are not reused.
Insert a row into a text table. Delete the row from the text table. Insert the row again.
The result is a file like this:
Do not know if this is documented somewhere but for sure not the behaviour expected.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

789
#789 Using database as file, no exception for unwritable file.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am attempting to validate that the choise the user does, when selecting the database output file, is valid.
So for
However no RuntimeException is thrown, and I seem not to be able to detect this condition and warn the user.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

790
#790 Dates are written wrong with hsqldb-server
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I found a bug in the Server code or the JDBC Driver Client code.
If I write a Date via a PreparedStatement into a database via network (with a "jdbc:hsqldb:hsql://..."-URL) the date in the database is one day before the right date.
I am using hsqldb.jar 1.8.0-9 (the newest one) with Sun Java6 on the client and the Debian Etch package with Sun Java6 on the Server (Version 1.8.0.7-1).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

792
TEXT tables with DESC attribute are not re-read on select
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello, hsql team!
I tried the "re-reading" TEXT tables functionality via runManagerSwing.bat:
From the documentation I have understood, that when option "DESC" is specified in 'SET TABLE my_text_table SOURCE "my_text_table_file" DESC', then the source file must be reread on every 'select'.
(Doc: "This feature provides functionality similar to the Unix tail command, by re-reading the file each time a select is executed").
Regards,
Yarick.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

793
#793 javadoc on jdbcDataSource is unhelpful
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The class level javadoc on jdbcDataSource is just copied from javax.sql.DataSource. This is highly unuseful as it doesn't say anything about the implementation such as if this datasource supports pooling or not. If you're just going to copy the documentation, then you should just put an @see reference.D
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

794
#794 Infinite loop
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello.
I recently downloaded 1.8.0.9 and ran our JUnit test suite for our app. on it. A simple query produces an infinite loop in the Select.buildResult(Session, in) method.
In the while loop on line 873:
the loop terminates if nontempty is false or level is less than 0. notempty is set on 869 and, in my case, remains true. level, on the other hand, never gets a value less than 4. It alternates between 4 and 5 infinitely.
One iteration decrements level to 4, but then the next iteration increments it to 5 and back again. It goes on forever.
Here is the query that is being executed. Table names have been changed to protect the innocent:
All of the id fields are indexed, but someColumn is not. Yes, it is a six way join but there are only a hundred records in each table. The query runs fine on MySQL, PostgreSQL, Oracle, and SQL server.
The funny thing is, almost an identical query executes in our test suite prior to this one without any problems. The only difference with the previous query is that it participates in a UNION ALL with another SELECT statement and criteria is specified on a different column.
I dug a little deeper and noticed TableFilter.findFirst(session) method on line 525:
Since the tables involved in my query only have a hundred rows, I put a break point on this line and looked at the value of "value". Turns out the same result set from the query is being iterated over again and again.
Let me know if you need anymore contextual information.
I would like to use HsqlDB for unit testing, as it is very fast.
Thanks!
-Nathan
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

796
#796 getMetaData().getPrimaryKeys is not working in v1.8.0.9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Here is what my SQL used to create the table:
When running the following code it does not return any rows.
I am running the latest version of hsqldb v1.8.0.9
Here my java version running on Windows 2000:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

797
#797 Servlet NullPointerException guaranteed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

798
#798 Rollback is not performed, leaving an inconsistent state
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Problem: When executeUpdate throws an exception, a rollback should be performed on the database to prevent the inconsistent state.
Solution: The below code snippet in the function should be modified:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

799
#799 Allocated resources are not released properly in ZaurusEdito
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Problem:
The function getPrimaryKeys of DatabaseMetaData can throw exception and a close() function on its return should be invoked to close the corresponding resource.
Solution:
The below code snippet in the function getAllTables
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

800
#800 Prepared Statement is not closed in ZaurusTableForm.java
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Problem: PreparedStatement.close() should be executed when an exception occurs while executing Connection.prepareStatement()
Solution:
The code snippets such as the one shown below in those functions should be modified to close the corresponding resource. For example:
The same case applies to other methods saveNewRow, saveChanges, deleteRow
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

801
#801 Prepared Statement is not closed in ZaurusTableForm.java
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Problem: PreparedStatement.close() should be executed when an exception occurs while executing Connection.prepareStatement()
Solution:
The code snippets such as the one shown below in those functions should be modified to close the corresponding resource. For example:
The same case applies to other methods saveNewRow, saveChanges, deleteRow
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

802
#802 SQL CHECK constraints with CASE statement fail
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Like the post at suggests, HSQL CHECK constraints can not be used together with CASE WHEN statements.
If you try to insert/update, you will receive a non-informative "S1000 General error java.lang.ClassCastException", followed by the whole SQL statement. (here I have HSQL 1.8.0.9).
If you have something like:
I'd liked to add that information to the post, however I don't know how to access it.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

803
#803 Multiple servlets does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If you deploy two org.hsqldb.Servlet in the same JVM they will not work.
Servlet.doPost() has a line
But the dbId is always zero so it can't find the right database, hence can't find the right session hence throws a NullPointerException when the session is attempted to be used
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

804
HsqlTimer.TaskRunner thread must nullify contextClassLoader
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Upon shutdown, the HsqlTimer.TaskRunner needs to nullify the thread "taskRunnerThread" contextClassLoader property via a setContextClassloader(null) on shutdown()
By not doing this, if hsql is used in a hot-deployed environment, such as in Tomcat. When the context is destroyed and if shutdown() is called on the database, this thread retains a reference to the WebappClassloader, which will never be cleaned up out of the heap because a JVM thread group retains a reference to this thread. Over time this will lead to a permgen out of memory exception
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

805
#805 Literal null in subquery overrides subsequent values
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Given this temp table:
However, the last query returns null as the value for both rows.
This causes grief when using Hibernate with a table-per-subclass strategy.
This is in HSQLDB 1.8.0.9.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

806
#806 TIMESATMP in UTC and CEST/CET change (Daylightsavingtime)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When adding values at(28.10.2007) 2:30 CEST and 2:30 CET both will yield after hard storing/retrieving into db as 2:30 CET.
this is due because the given calendar is not hounored properly.
when changing this to store proper UTC values you have to look for the opposite change (at 25.03.2007 CET-> CEST) because time jumps 1 hour forward....
This problem exists in other jdbc drivers (jaybird-jdbc from firebird) also. So I don't know if this is a bug in hsqldb or in jdbc...
see attached junit-test.
Thanks in advance
Arne Plse
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

807
#807 Simple inserts / updates take randomly more than a minute
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We use the hsqldb in a process oriented system where process and store records in the database within a few milliseconds.
The hsqldb performs great most of the time, but randomly after inserting data @ 3000 records per minute, the database randomly takes a very long time to execute even a simple update query on a table less than 25 records.
I am currently on a trial and error mode to create and try to fix the problem.
How the current implementation is.
1. HSQLDB version 1.7.2 I it also happens in the current release), running in Server mode.
2. Tables definitions are Cached.
3. updating through Java prepared statements (JDK 1.5).
4. Rate of data input 3000 records per minute.
output from my logs
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

808
#808 OutOfMemory if telnet to port and enter two times enter key
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I played around with HSQLDB which comes with Spring Framework 2.5.3, the MANIFEST says it is version 1.8.0.
When I start the server and telnet to the port it started on and press the ENTER key twice I get an OutOfMemoryError. It is reproducible. The stacktrace is:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

809
#809 DatabaseMetaData.getTypeInfo.caseSensitive flag is incorrect
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The JDBC DatabaseMetaData method getTypeInfo() describe the database types. There is a boolean column
"CASE_SENSITIVE".
With HSQLDB 1.8.0.1 it returns "false" (case-insensitive)
for VARCHAR and "true" (case-sensitive) for VARCHAR_IGNORECASE.
It should be the other way around.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

810
#810 more than one tab in CREATE VIEW STMT leads to a failure
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have formated my code with more than one tab. The statement looked like:
create view x as select a.* from a union all select b.* from b
This is only a simple example.
I have formatted the code like the following:
This doesnt work. HSQLDB allows only one [tab].
More tabs are generally used within "more complex" querries.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

811
#811 Union Query returns different results with/without params
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following SQL statement (with parameters values inlined), returns the correct (expected) number of records:
select count(*) from location where id in ((select location_id from stop where job_id = 0) union (select 9550 from job))
If, however, I run the query as a PreparedStatement with parameters, it returns only the results from the first part of the UNION (ignoring the row from the secondary table-- job):
The first query returns 136 records (the expected number); the second returns only 135. If the problem is with the query itself, I would expect a SQLException trying to create the PreparedStatement (which is the behavior I get when running the same query using H2). Instead, the query seems to work (i.e, no exceptions are raised), but returns the wrong number of records.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

812
#812 Problem for operations with ifnull HSQL1.8.0.9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I think of having to find a problem in HSQL 1.8.0.9
Here the query
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

813
#813 NPE on  jdbcPreparedStatement.executeQuer(String)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi we're getting the following exception:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

814
#814 Insert trigger not working Version 1.8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
Can someone test to see if they are getting the same result. I wrote a trigger and the insert action is not populating the old and new objects.
As a test I ran the TriggerSample.java supplied with the project, and for the insert statements I get:
These is the output following the test inserts into the trig_test table as outlined in the bottom of the TriggerSample.java example file.
I tried version 1.8.0.9 and 1.8.0.10 with the same result.
I am curretly running Java 1.6.0_03.
Thanks,
Scott
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

815
#815 Missing StringComparator.java from (1.8.0.10) release
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The StringComparator.java is missing from (1.8.0.10) release.
It should be in the org.hsqldb.lib package.
It's referenced in the TestSelf.java test.
Looking in the source repository, I can see that org.hsqldb.lib.Sort has the StringComparator as an inner class - the org.hsqldb.lib.Sort in the source distribution does not.
I've tried to build using the source repository version for Sort, but it fails (which in hindsight is no surprise). The test builds and works if I extract the StringComparator from Sort and put it in its own file.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

816
#816 DATE in a HAVING clause gives wrong datatype error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Seems that the text to date conversion in SQL statements is not handled when in a HAVING clause.
See the following example:
Seems specific to DATES. The same test with INTEGER for example works fine.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

817
#817 Index conflicts for multiple schemas
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have attached an SQL scripts that create two schemas: BOOKS and SONGS. Each schema has a table named PUBLISHER. Each table creates an index and constraint with name PK_PUBLISHER. The DDL succeeds but the SYSTEM_INDEXINFO becomes corrupted. This can detected by using the \di command in the SqlTool browser. Dropping the second occurance fixes the corruption. The bug is that two indexes of the same name but in separate schemas should be allowed. Another example occurs for the UK01_PUBLISHER example. (FYI, UK is my abbreviation for User Key).
A second bug is schemas cannot be created using a database user as the owner / grantee.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

818
#818 casting bigint to numeric or decimal type does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
I have a table with bigint datatype columns and I need to perform some division operations between them, and for that I apply a cast operation to decimal (or numeric).
To my surprise, that cast seems to have zero or fuzzy effect, because the casted values still behave like integers (division gives no decimals). This should be something related to the bigint datatype itself, if I cast to double for example or if the column is just int then I get the expected result (worth to mention that in my samples, the bigint values were not even very big really).
I picked up decimal/numeric cause they offer exact precision, but with this problem the workaround for me will be using double cast, which is just approximation but looks more stable.
I just read the documentation and it tells that decimal divisions will have the larger scale of the operands ... in this case, both have no decimal part ... mmm, could that be the cause? Anyway, the result seems counterintuitive for me ... I would expect full precision in division as well (although maybe, that is not implemented yet).
Regards.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

819
#819 Testdb throws NullPointerException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
A trivial problem.
member variable conn is not initialized.
My email is bench_wang@hotmail.com
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

820
#820 Bug in org.hsqldb.persist.ScaledRAFi
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In my opinion there's a bug in org.hsqldb.persist.ScaledRAFile in all write methods.
Instdead of:
In void writeInt(int i) it should be:
In void writeLong(long i) it should be:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

821
#821 Database shutdown closes System.out (log problem)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Whenever a process database is shut down, the System.out is being closed by SimpleLog.close(). No other output (e.g. further logging in the calling application) can then be sent to the console.
Call Stack:
Thx for fixing this....
Markus
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

822
#822 1.9.0 - 'Numeric' boolean/bit columns
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
According to the HSQLDB documentation on bit and boolean types:
'This type of column can also be initialised using values of any numeric type. In this case 0 is translated to false and any other value such as 1 is translated to true.'
On attempting to use numeric values during an INSERT, this was stopped by method convertToType in the BooleanType.class (otherType.Type was Numeric, which resulted in the 'default' case being invoked, and a consequent cryptic message 'Value too long').
Some background: I am planning on migrating to 1.9.0 from MySQL community edition, which uses 1 and 0 representing true and false respectively in its boolean columns. This is for the SourceForge project Kangas Sound Editor (http://sourceforge.net/projects/kangasound/).
In anticipation of your help, thank you!
Paul
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

823
#823 1.9.0: RETURN_GENERATED_KEYS
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
In 1.9.0, the behaviour to return generated keys from method calls such as executeUpdate(String sql,int[] columnIndexes) followed by getGeneratedKeys() doesn't appear to work - is it implemented yet? A quick look at the source code (in Session.java) indicates that this might be implemented for prepared statements (there is a call to cs.setGeneratedColumnInfo() which looks relevant), but not yet for direct statements.
In anticipation of your help, thank you!
Paul
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

824
#824 Text tables with FK to memory tables produce an FK violation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a database that uses a text table that has a foreign key to a memory table. I insert data into both tables and shut down the applicaiton.
The data for the text table is persisted correctly in the .csv file, and the data for the schema and the memory tables is persisted in the .script file.
However, when the application starts again a FK violation is thrown.
I suspect that this happens because the engine initializes the text table first, and then add the constraint without first populating the memory table (the INSERT statements are below the ALTER TABLE statements in the .script file).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

825
#825 documentation not uptodate and missleading
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi there,
when trying to configure my server using the server.properties, I noticed, that the documentation of the possible properties is not consistent. Unfortunately I read the section shortly after table 4.1 in "Advanced Topics"(URL: http://hsqldb.org/doc/guide/ch04.html\), which still shows the old property names like "database.i" and dbname.i" instead of "server.database.i" and server.dbname.I". Although the correct property names are noted somewhere else in the same webpage, it is still confusing and personally took me a long time to sort it out.
The server itself also doesn't help, since it does not show an error or hint when using wrong or old property names. It just says "Properties read from..." even, if the server.properties file doesn't exist. It would save newbies a lot of time, if the docs would be updated and the server would tell the user about wrong configurations.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

826
#826 precision on DOUBLE unsupported, contradicting docs
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
According to:
"In table definition statements, HSQLDB accepts size, precision and scale qualifiers only for certain types: CHAR(s), VARCHAR(s), DOUBLE(p), NUMERIC(p), DECIMAL(p,s) and TIMESTAMP(p)."
It seems that DOUBLE should support precision (although the precision is ignored by default). However, as of hsqldb 1.8.0.10 (also affecting earlier version), a table creation of the form:
create table foo (bar DOUBLE(x));
(where x is some number for precision) results in:
SQL Error at 'stdin' line 2:
"create table foo(bar DOUBLE(22))"
Unexpected token in statement [create table foo(bar DOUBLE(22]
Either the engine needs to be fixed to reflect the documentation, or the documentation needs to be fixed to remove DOUBLE as supporting precision.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

827
#827 "checkpoint compact" causes server.shutdown to block
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In version 1.8.0.10 having a database connection issue the command "checkpoint compact" causes the shutdown method in the server (when run from within an existing process, ie. using server.start()) to block. When calling server.stop() and checking for the server's state, the state never changes to shutdown.
It worked fine in 1.8.0.9.
Also, in the documentation, the getState() return values are referred to as in ServerProperties where they are in ServerConstants.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

828
#828 (1.8.0.5) S1000 General error java.lang.NullPointerException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I am using HSQL v1.8.0.5 and I found a bug in statements execution resulting in the following error:
I understand that this bug was fixed in HSQL 1.8.0.7 but I didn't find the root cause or scenario causing this problem.
This information is important to me for the following reasons:
Any help would be appreciated...
Thanks!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

829
CALL IDENTITY()
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The example use of CALL IDENTITY() in Chapter 9. SQL Syntax doesn't work as a prepared statement.
This implies that the correct way is to use multiple statements in a single statement which is not allowed in prepared statements.
Clearer guidance on the use of CALL IDENTITY() especially in prepared statements is needed.
It is not clear what happens if multiple threads are adding records. If CALL IDENTITY() is used in a separate statement what guarantees are there that the correct value is returned to each thread?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

830
#830 bug in BitType.convertToDefaultType()
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The code involved is from the svn trunk (I wanted to test HSQLDB with connection.prepareStatement(String sql, int autoGeneratedKeys)).
I think the BitType.convertToDefaultType() has a bug. When I try to convert a Boolean to sybase BIT, it throw an error:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

831
#831 Lost data in temp table from union select.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have three tables (think table per subclass from hibernate) they all have two of the same columns, two tables have two additional fields each. I'm going to union select the three tables.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

832
#832 1.9: Delete trigger does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Delete SQL triggers doesn't seem to work in 1.9 (svn 22. Sep 2008).
Here's a script to test (deletion should trigger an entry in log table):
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

833
#833 SELECT DISTINCT (exp) ORDER BY 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If a non-trivial expression is used in an ORDER BY clause in a SELECT DISTINCT, a variety of inappropriate error messages get triggered.
For example: SELECT DISTINCT (base.Ref IS NULL) FROM Base base ORDER BY 1 gives "ORDER BY item should be in the SELECT DISTINCT list"; obviously, item #1 from the SELECT DISTINCT list is in that list.
This error can be avoided by putting the column used in the SELECT DISTINCT list, but I've been getting problems with a "Wrong Data Type" exception in some circumstances (which I haven't been able to characterize) even then.
I can provide schemas and detailed examples, but it seems to happen with all of the commands of this form that I've tried.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

834
#834 SQLC has been renamed to jIncarnate
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
On your web site (http://www.hsqldb.org/web/hsqlUsing.html) you reference SQLC tool. I'd like to inform you that the name of the tool was changed to jIncarnate.
On a similar note, you may want to add Hammurapi code review tool (http://www.hammurapi.biz/hammurapi-biz/ef/xmenu/hammurapi-group/products/hammurapi/index.html) to the list of software which uses HSQLDB. Hammurapi stores code review information into HSQLDB.
Best regards, Pavel.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

835
#835 Incorrect Wrong Date Type from "is null" as column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If you have a schema like:
create table tasks(done timestamp)
and you've got a row with a non-null done,
select (done is null) from tasks;
will give a Wrong Date Type error when it tries (done).getValue(session, Types.BOOLEAN) on line 3339 of Expression.java. It looks to me like that section should only run for an exprType of ADD, SUBTRACT, etc. (the non-default branches of the case statement following).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

836
#836 Foreign keys between schemas don't work.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
You can't make a foreign key between tables in different schemas.
These few lines of sql set it up:
> create schema schema1 authorization dba;
> create table schema1.table1 ( table1_id integer primary key);
> create schema schema2 authorization dba;
All three of the following options incorrectly produce errors:
1)
> create table schema2.table2 (table2_id integer primary key, table1_id integer, foreign key (table1_id) references schema1.table1);
produces
"invalid schema name: SCHEMA1 in statement [create table schema2.table2 (table2_id integer primary key, table1_id integer, foreign key (table1_id) references schema1.table1]"
2)
> create table schema2.table2 (table2_id integer primary key, table1_id integer);
> alter table schema2.table2 add constraint fktbl1 foreign key (table1_id) references schema1.table1;
produces
"invalid schema name: SCHEMA1 in statement [alter table schema2.table2 add constraint fktbl1 foreign key (table1_id) references schema1.table1]"
3)
> create table schema2.table2 (table2_id integer primary key, table1_id integer);
> alter table schema2.table2 add constraint fktbl1 foreign key (table1_id) references schema1.table1 (table1_id);
produces
"invalid schema name: SCHEMA1 in statement [alter table schema2.table2 add constraint fktbl1 foreign key (table1_id) references schema1.table1]"
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

837
#837 Typo on home page
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
There is a typo at http://hsqldb.org/
"In it's current version" should be "In its current version"
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

838
#838 Incorrect result with statement.setMaxRows(1)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
please have a look at this code:
The select statement always returns just one row.
But when I define statement.setMaxRows(1) I always get as result "1" (for other sql statements as well) instead of the correct result.
Ciao
Holger
hst1@online.de
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

839
#839 Function call syntax
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
jdbcConnection requires using exactly "?= call" for function call.
I'd prefer less strong check, because simply using "? = call" causes much time loss to find out why it doesn't work.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

840
#840 cancel trigger action
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
i tried to throw an RuntimeException from a triggers fire method expecting the corresponding data not to be written to the database. instead it was.
could/should be a way to cancel a triggers action.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

842
#842 trigger acitions executed after commit!
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
the acions of a before-update-trigger are executed after commit! in the example i provided you read different values in two consecutive selects after a commit!
the example prodces:
this shouldn't happen, especially not after a commit!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

843
#843 HSQL jdbcConnection has no socket timeout?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If something bad happens (like a rogue firewall rule that drops packets) to a jdbcConnection to an HSQLDB after it's already been opened, operations on the connection will hang for a long time. There's appears to be no socket.setSoTimeout() call in the connection setup to allow this to be controlled.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

844
#844 General SQL Error/NullPointerException in cascading deletes
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a schema with two tables METRIC and SLICE. SLICE references METRIC with ON DELETE CASCADE foreign key. METRIC has hierarchical structure, i.e. it has PARENT column.
When I issue delete in METRIC I get General SQL Exception / NullPointerException. Once it happens other connections start getting NullPointerException for no reason. The database server has to be restarted to fix this.
I had this error in some other case with three tables connected with FK with ON DELETE CASCADE.
I'm not privy to HSQLDB internals, my speculation is that it happens when some particular record has two cascading deletion paths, when the first path is executed it sets something to null. When the second path is executed, it dereference that something without checking for null and it results in NullPointerException.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

845
#845 Types.getTypeString returns NULL for BIT
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
According to the JavaDoc Types.getTypeString should return the SQL type string usable in HSQLDB for a given java.sql.Types int. Since BIT is one of the supported types in HSQLDB this method should return either "BIT" or "BOOLEAN" for it.
I suppose a simple "typeNames.put(Types.BIT, "BOOLEAN");" in the static initialization should be enough.
Affects: HSQLDB 1.8.0.10
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

846
#846 DatabaseMetaData getColumns doesn't include IS_AUTOINCREMENT
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
According to the Java API for DatabaseMetaData the ResultSet retrieved by getColumns(...) should include a column which states whether the table column is autoincrement. HSQLDBs method does not return this information.
(Quote JavaAPI:
getColumns
Retrieves a description of table columns available in the specified catalog.
...
Each column description has the following columns:
...
PS: Is there any other way to get my hands on that piece of information right now?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

847
#847 Documentation: SET LOGSIZE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In the documentation on the web site http Chapter 4, Table 4.8 it says
you have to issue "SET LOG_SIZE ..." to set the log file max size.
It should be "SET LOGSIZE" instead.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

849
#849 "ALTER USER SET PASSWORD" fails unconditionally
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I was exploring your svn head yesterday and I noticed that ALTER USER SET PASSWORD="..." fails unconditionally. I'd see things like:
Internal error
Naturally I looked at StatementCommand.java and it looks like a really simple fix. ParserDDL.compileAlterUser is compiling a list of Expressions instead of a list of Objects. (Why that distinction exists I'm not sure.) So StatementCommand overloaded for Expressions fails unconditionally when receiving a SET_PASSWORD type, since there's just no code to handle it. StatementCommand overloaded for Objects on the other hand already has code to handle receiving a SET_PASSWORD type. Just switching Object in place of Expression in compileAlterUser seems to fix the problem, though I'm no veteran of this project. I attached a patch against the SVN HEAD though, so do take a look and please apply if I haven't horribly misunderstood the situation.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

850
#850 get_column_name defaults false in 1.8.0.10?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The documentation at httpstates that get_column_name defaults to TRUE (so that getColumnName and getColumnLabel will return the values I would expect them to)
However after a quick test, it would seem that this is not the case, and it is actually defaulting to FALSE :-/
Not sure if this is an issue with the documentation, or the engine itself, my guess would be the latter...
Here's the code to demonstrate the problem:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

851
#851 primary key on varchar does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
with the example below the HSQLDB says there is a unique constraint violdation in the column FIRSTFIELD
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

852
#852 prepared Statement with subselect doesn't work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I tested it with folloing statement:
PreparedStatement stmt =
I did a quickfix in the constructor of CompiledStatement line 312 (version 1.8.0.9)
This solved the problem for me.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

853
#853 NullPointerException in HSQL engine
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
NullPointerException in HSQL engine
While running HSQLDB 1.8.0.10 in-process during a day with multiple insertions (more than 20 000 in a day), some insertions are rejected with a NullPointerException.
The VM Xmx parameter is set to 2048m.
I have modified the Result(Throwable t, String statement) constructor in order to trace the Full Stack of the exception. Here are the logs:
Thanks
Romain
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

854
#854 Union in subselect yields incorrect results
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following scenario:
Running just the inner query does generate the expected result.
This also appears to be the cause of the issue 2102123.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

855
#855 NullPointerException on shutdown
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I start up my HSQL Database and want to connect to it and shutdown properly. I wrote a simple class to make the connection and issue the shutdown statement. The code looks like this:
System.out.println("The database was shutdown successfully.");
When I call this code I see an exception on the server console (I built with Javac debug so the stack trace has useful line numbers):
The exception doesn't cause any trouble. The ServerConnection is trying to close a socket and the variable was never initialized.
It would be nice to have this fixed. I have tried this on Windows XP and OpenSUSE 11 with the same results.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

856
#856 patch for avoiding recursive call on ConnectionPool.setDatab
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Current setDatabase(String) in org/hsqldb/jdbc/pool/JDBCConnectionPoolDataSource.java is erroneously a recursive function. This patch fixes the problem.
The patch has been checked against 1.9.0-alpha1
Bye,
Giuseppe
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

857
#857 Hard to find how to set server IP address
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It would be helpful if you mentioned the "server.address" configuration option in the "Server and Web Server Properties" section of the "Advanced Topics" section of the documentation (http://hsqldb.org/doc/guide/ch04.html#N10AC1).
The existence of this option is implied in the java docs for org.hsqldb.Server (http://hsqldb.org/doc/src/org/hsqldb/Server.html), but it's not obvious from the "guide" (http://hsqldb.org/doc/guide): The "Server Modes" section of chapter 1 (http://hsqldb.org/doc/guide/ch01.html#N1013D) says "Server modes can use preset properties or command line arguments as detailed in the Advanced Topics chapter", but the relevant section of the "Advanced Topics" chapter (http://hsqldb.org/doc/guide/ch04.html#N10AC1) doesn't include the "server.address" option. Chapter 1 does mention that org.hsqldb.Server takes '-?' to give a usage summary (why not '-h' or '--help' or '-help' ?), which does mention the '-address' arg from which the 'server.address' config can be inferred, but it would be better if it was mentioned explicitly in chapter 4.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

858
#858 JDBC driver does not respect nested Properties objects
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Creation of a connection using DriverManager.getConnection(url, props) does not read the properties correctly if the Properties object passed in was created using the Properties(Properties defaults) constructor. It treats values in the nested Properties object as though they do not exist. I'm sure there is a simpler demonstration, but below is code approximating what is used. The shutdown property is not respected. When the line "props = new Properties(props);" is removed, the shutdown property is respected as it should be. With the code as is, the result is 42 printed twice. The expected result (and what is seen if the Properties object is not nested) is a single print out of 42, and then a SQL exception caused by "org.hsqldb.HsqlException: user lacks privilege or object not found: FOO". This is expected because the database should have been shutdown and cleared between connections. This was tested in versions 1.8.0.7 and 1.9.0-alpha2
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

859
#859 create table fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using alpha2.
BTW. alpha2 distribution goes with only one hsqldb.jar compiled with java 1.6, so in order to use it for older javas recompilation is needed.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

860
#860 timestampdiff does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Trying use timestampdiff.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

862
#862 missing changelog of 1.8.10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
There is only a changelog up to 1.8.0 but 1.8.1-1.8.10 are not documented.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

863
#863 HSQLDB doesn't complain about, but others do...
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
select
The problem: the field "F_id" can be found in both tables. HSQLDB doesn't complain about this, but other DB do.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

864
#864 Clob:  SetClobParamter : Invalidad argument
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have added support for HSQLDB inSun's OpenESB (BPEL-SE). It seem to be working fine except Clob data.
JDBCPrepareStatement.java: setCharacterStream() : When java.io.Reader is type of CharacterArray, it goes into else condition of (reader instanceof java.io.StringReader) where it's calling setParamter(paramterIndex, sb.toString()). Here sb is StringBuffer.
setPatameter() calles setClobParamter() which throws invalid argument exception because sb (StringBuffer) is not instance of Clob.
I have tried following things:
1. I tried converting sb (StringBuffer) to JDBCClob in setCharacterStream() but in RowOutputBase.java:writeData(): case Types.SQL_CLOB:
writeClob tries to convert into ClobData and throws exception.
2. I also tried to create ClobDataID object in setcharacterStram() function and modified setClobParamter() function to accept type:ClobData .
I this case, in JDBCPrepareStatement.java:performPreExecute() function, it throws exception when it type class to Clob.
I would really appreciate if you can give some hint.
NOTE: I have fixed one bug in TIMESTAMPDIFF function which I will submit a patch.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

865
transaction rollback: serialization failure" error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Fred,
Today when I started the loadtest, I am get transaction rollback: serialization failure" error so couldn't continue load test. If I run single test, It doesn't happen but if I run same test in loop for let's say 10 times, I was able to reproduce it every time and out of 10, it could happen 2-3 times.
Please find attached compressed log file. See the ThreadID=41.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

866
#866 Randomly "Table not found: SYSTEM_TABLES"
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
occures randomly in our junit load tests using multiple threads simultanously.
each thread opens his own connection to his own database file via "jdbc:hsqldb:file:/home/foo/testdb_<some_guid>;shutdown=true"
if this error occures once, all following attempts to read the system table (of newly created databases) dying with the same error. the jvm has to be terminated in order to successfully read table names again.
used version: HSQLDB 1.8.0.10
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

867
#867 Alter table resets identity
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I work with hsqldb 1.8.0.8.
Altering a table with an identity results in a reset of the identity.
Example:
The object inserted after the "alter table" statement is created with ID 1, altough objects with ID 1 and 2 have already existed.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

868
#868 Regression from 1.8 upon COALESCE and COUNT with GROUP BY
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Versions 1.8 and 1.9 alpha 2 (latest build as of 2009-05-22) behave differently
upon doing the following experiment. I would appreciate if you could tell
if this is by design, a bug which can be fixed, or something I should work around.
You can recreate my results like this:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

870
#870 Duplicate constraint name generated
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When using liquibase to generate the schema for a particular db, the generated .script file for a file-based database contains duplicate constraint names.
The db properties being use are:
After executing the above change sets, the generated .script file contains an entry for the AUTHORITIES table that looks like this:
Note the duplicate name CONSTRAINT FK_AUTHORITIES_USER_ACCOUNT where the primary key constraint should be AUTHORITIES_PKEY. Also note that in the liquibase change sets, the primary key constraint is added, then removed, then added again (with an additional column name added).
This error is observed in versions 1.8.0.9 and 1.8.0.10 but not in 1.8.0.7
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

871
#871 Regression: poor execution plans for SELECT with LEFT JOIN..
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This bug causes HSQLDB 1.9 build 2999 to form very poor execution plans for SELECT queries featuring LEFT JOIN and WHERE statements.
You can recreate it this way:
1) Create a test database with the following tables both under 1.8 and 1.9
Also, OpenOffice puts automatically added WHERE statements
at the end of SQL queries, and isn't smart enough to check for JOIN statements
and automatically build subqueries. I've already observed this causing problems
with OO 3.1 when I attempted to use an external HSQLDB 1.9 database.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

874
#874 1.9 Beta 3 Bugs
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
While using v 1.9 beta3 . . . noted a few items for your consideration.
1, I too, had problems with Shutdown Compact problem.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

875
#875 java.sql.SQLException: not allowed in OUTER JOIN condition
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Creating a view I get an unexpected java.sql.SQLException:
No problem with this statement (well, the equivalent statement as of the different DDL syntax and data types) in other data bases (Oracle, Ingres). Complete data model of the underlying tables see below:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

876
#876 build.xml don't allow calling from other ant task
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a "parent" build.xml that build a few other project, including hsqldb.
The structure is:
Starting from 1.9.0beta, this ant command no longer works:
(this work in 1.8.0.10)
I have created a patch, please check the attachment.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

877
#877 SELECT causes NullPointerException at ExpressionLogical.java
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When running this query with HSQL 1.9 b3, the following NullPointerException occurs. This is working with HSQL 1.8
Thanks
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

878
#878 incompatible data type in conversion when binding a boolean
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When binding a boolean object to a numeric parameter in a PreparedStatement with the stm.setObject command, it fails with the following exception :
This is a regression, works perfectly with HSQL 1.8
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

879
#879 INSERT not working correctly with DECIMAL columns
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Consider the following sql tool session:
The result should be 3.3 not 3
When retrieving the value from within a Java program using getDouble() as printl() also prints 3.0
The above script was run against a newly created empty 1.9b3 database.
The generated .log file of the database contains the following content:
As you can see no fractions in the INSERT statements.
Running a SELECT against the table displays the following:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

880
#880 SELECT .. INTO foo does not work in trunk - r3012
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
SELECT * INTO newtab FROM oldtab
gives ErrorCode.X_42581 on INTO token.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

881
#881 1.9beta3 identity generator
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I tried to access a 1.8 database with the new 1.9 beta3.
I got the following exception. The bean that is mapped with Hibernate
uses "native" as id generator.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

882
#882 Must use fully qualified names to use PUBLIC tables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It seems that tables that are created in the PUBLIC schema are not accessible unless you fully qualify the name of the table. In previous versions, tables in the PUBLIC schema did not need to be fully qualified to access them. It seems that any object that is in the PUBLIC schema should be accessible if it is not in the current schema.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

883
#883 JDBC Connections opened with the :res: option fail to open
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Changing a jdbc url from jdbc:hsqldb:file:<path to db> to jdbc:hsqldb:res:<db on classpath> the following exception is always thrown (with 1.9.0-beta-3):
This makes it impossible to package a database in a jar (which is very useful for unit testing).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

884
#884 sqltool cannot SELECT varbinary column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
incompatible data type in conversion: from SQL type VARBINARY to java.lang.String, value: instance of org.hsqldb.types.BinaryData
It should be possible to select rows which contain VARBINARY columns. Handling of String conversion from BinaryData seems to be broken in sqltool.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

885
#885 readonly fails in 1.9b3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm creating a simple file: database. Creating and using it works fine both in 1.8 and 1.9, but in 1.9 I can't use it when it is readonly. Setting readonly to true in either the properties file or the jdbc url fails and gives me this error message (from SqlTool):
Failed to get a connection to 'jdbc:hsqldb:file:Database/foo;readonly=true' as user "SA".
Cause: invalid schema name: SYSTEM_LOBS
This seems to work fine in 1.8. My database has no CLOB or BLOB columns, in case it is relevant. Shutting down with 'shutdown compact' does not help.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

886
#886 setting UNIQUE constraint on a column fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I tried to create a table with a column having UNIQUE constraint. This works in 1.8.0, but fails with 1.9.0 beta3.
Here is the SQL statement:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

887
#887 DROP TABLE IF EXISTS does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I tried to execute a DROP TABLE sql statement but it fails. Said error at EXISTS keyword.
It works in 1.8.0 but fails in 1.9.0 beta3.
My SQL:
DROP TABLE IF EXISTS table_name;
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

888
#888 Regression: SET LOGSIZE (and other settings) refused
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Threre is an error in trunk revision r3023, r3006 works well.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

889
#889 SET SESSION CHARACTERISTICS R/W does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I migrated from version 8 to beta 3 version 9 recently and it does not permits table modifications in local transactions.
The tables are cached tables. When I made the first tests and tryed to modify any table I got the error
SQL state [25006]; error code [-3706]; invalid transaction state: read-only SQL-transaction;
Then I tried to execute at the begining SET SESSION CHARACTERISTICS AS READ WRITE and it worked only for
the very first transaction. Then I tried excecuting this statement at the beginning of every transaction
and it seemed to work until the same error came back again. I gave up and decided it might be a bug.
Sincerly, gerardo.leon@cablevision.net.mx
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

890
#890 sql state for  foreign key violation is incorrest
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
On foreign key according to SQL-92 standard have to be 23503. The 1.9beta3 returns 23502 which is an insert or update a value to null , but the column cannot contain null values. The patch which fixes it is attached.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

891
#891 Timezone and Timezones off by a factor of 1000
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In DateTimeType, the following code adjusts the date by one day, not one hour. This is because session.getZoneSeconds is subtracted *before* the multiplicaton by 1000, not after.
I believe the below code fixes this:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

892
#892 Issues with DATEDIFF in 1.9.0-beta3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am having a problem with the DATEDIFF function in 1.9.0-beta3. Below are reproduction steps. These worked in 1.8.0.10 (although the DATEDIFF function has changed it's parameter order)
gives error - user lacks privilege or object not found: D1 / Error Code: -5501 / State: 42501
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

893
#893 java.lang.NullPointerException in ExpressionColumn.getValue
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am upgrading our unit tests from using 1.8.0.10 to 1.9.0-beta3. I have quite a complex piece of SQL which is now causing a java.lang.NullPointerException in the ExpressionColumn.getValue method. I have also tried with the latest SVN code (revision 3031) and get the same error. I have managed to extract the part of the SQL which is causing the issue and put the statements required to reproduce below along with the stack trace.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

894
user lacks privilege & create alias error  in 1.9.0 beta 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Code works well in 1,8.x release, but two errors I got
user lacks privilege
see attachment.
Really bad feeling for the updates
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

897
JDBC Stored Procedure calls don't work with null parameters
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If I use a Spring org.springframework.jdbc.object.StoredProcedure to call a stored procedure that has at least one java.sql.Date parameter, and pass a null value for that parameter (via StoredProcedure's execute(Map) method) then a NullPointerException is thrown by org.hsqldb.types.DateTimeType line 806 (of 1.9.0rc4).
Note that org.hsqldb.FunctionSQLInvoked's this.routine.parameterNullable[i] field (line 113) correctly detects this parameter as being nullable.
Possible fix: org.hsqldb.FunctionSQLInvoked line 118 "if (this.routine.isPSM())" should be "if (this.routine.isPSM() || value == null)" - this would prevent a conversion to a java type even being attempted if the value was null.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

898
re-sourcing a text table doesn't work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When using SET TABLE .... SOURCE a second time on a text table, the new contents are not immediately reflected.
I continue to see the old, original data when executing selects until I restart my application. Closing and re-opening the connection does not help. Somewhere in memory, independent of the connection, it appears that my data is being cached.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

900
#900 Meta data COLUMN_SIZE returns 0 in 1.9.0-rc4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
While testing the new 1.9.0-rc4 release, I ran into a situation where the JDBC meta data doesn't return the correct COLUMN_SIZE for string types. The attached code, has the following output for 1.9.0-rc4:
Column TESTTABLE.SOMESTRING VARCHAR(0)
In contrast, 1.8.0.10 returned the following:
Column TESTTABLE.SOMESTRING VARCHAR(64).
This seems like a regression.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

901
#901 UPDATE statement fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
For version: HSQL Database Engine, version: 1.8.0
I am using the follwing native SQL query from withign hibernate:
This fails on error:
It seems like HSQL is choking on the ENGINE_TOKEN.ID part of "WHERE tmap.TOKEN_ID = ENGINE_TOKEN.ID". As HSQL doesn't support UPDATE ... FROM ..., there is no other way to transfer the ID to the inner statement.
The very same SQL query works fine on MySQL 5.1, and Microsoft SQL Server 2005, and is only a problem on HSQL.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

902
#902 fail restart server after shutdown
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
2) open DB Manager
execute in the menu <Options>/Insert Test Data
execute the command "SHUTDOWN"
3) 1) start the server
Regards
Adolf Rieger
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

903
#903 1.9rc4: Index NPE, locks
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
apologies, enountered an issue when re-running the tests using the trunk/svn (rev 3114).
During delete, this exception was generated...
thanks!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

904
#904 bitand function not same as Oracle
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
A table with a bitmap column like :
I need to count the rows for each bit of the bitmap like :
What do you think ?
Thanks,
Steve
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

905
#905 MERGE ... WHEN MATCHED clause column reference
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The MERGE ... WHEN MATCHED THEN UPDATE clause does not recognize new cursor rows during update operations. Instead, the entire target table of the merge is updated with uniform values across all rows.
This behavior appears to be constrained only to this portion of the clause. The attached script shows that the WHEN NOT MATCHED THEN clause behaves as expected.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

906
#906 Cannot add a foreign key with existing data
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If a table has a text primary key then a foreign key can be created to that table fine.
However if the table already has existing data it will fail. This works fine if the key is of type integer.
Here's a simple example which shows the problem:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

907
#907 1.9rc4  Interval Bug?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Why is it that when executing the below SQL using JdbcTemplate.queryForInt(..) Oracle returns 14421 but HsqlDB returns 1245974400? Clearly, Oracle is returning the days between the two dates and hsqldb is returning the number of seconds...who is right? (note: for the above to work on hsqldb you need to create a table called 'dual')
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

908
#908 Some comments NOT ignored by some JDK versions...
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
While building revision 3158 locally, I found a quite odd problem
which I tracked down to comment syntax in "JDBCConnection.java".
If I check out an unmodified copy of revision 3158 from SVN,
and try building it on my Debian / OpenJDK / Ant based system,
it fails to compile with the following error message:
The source of the problem appears to be in the comment syntax:
For some reason, my JDK apparently refuses to treat this as a comment,
and only compiles the code if I alter the commented-out "ifdef" directives
by inserting a space between the comment tag // and the "ifdef" directive
I realize that strictly speaking, this is not an HSQLDB bug,
but since it might confuse others too, I thought I'd mention it.
used to comment out
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

909
#909 incorrect empty results from select on indexed cached table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
i have a table built from various bits of Java code which seems to have got itself into an inconsistent state.
This is using the patched 1.9 build from 27th August on the support page.
Running a select statement matching the PK does not return any rows but doing certain other queries does return a row which the original query should have found.
So for example:
select * from si_sd_data_attribute where attr_id = 'log-record.is-cookie-new'
returns 0 rows but there is a row with that value in attr_id (the PK)
select * from si_sd_data_attribute where attr_id like 'log-r%'
Again this returns 0 rows
select * from si_sd_data_attribute where attr_id like 'log%new'
This however returns 1 row with the expected attr_id value of 'log-record.is-cookie-new'
I have attached a copy of the db chopped down to the the smallest I can get it to be.
It only has the problematic table remaining.
The table is defintely in an inconsistent state.
Running deletes against it will apparently delete rows but then after exiting and restarting the JVM the rows are no longer all deleted but the number of rows is not necessarily the same as before the delete either.
Certain deletes will cause the missing row to reappear.
Dropping columns also seems to make it reappear.
I'm guessing it rebuilds the table somehow and that's what "fixes" things.
I can probably provide the inserts that got the table into that state if that's helpful but will take a bit of work.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

910
#910 1.9.0-rc4: PreparedStatement failed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
SQL-PreparedStatement of type "INSERT INTO t1 SELECT ..., ? FROM t2" fails with 1.9.0-rc4. It works perfectly with 1.8.0-10.
Same problem I have with "INSERT INTO t1 SELECT ..., CONVERT( ?, CHAR ) FROM t2".
I use Sun's JRE v1.6.0_16.
Attached file show a simple Java program for testing.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

912
#912 1.9.0-rc4 HEAD: Problem with foreign key constraint + (a,b)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have been trying out hsqldb-1.9.0-rc4. I've found a problem with both the released jar and the latest trunk for 1.9.0.
Consider the following sequence of SQL statements:
I expect only one result to be returned in the select statement, but I'm getting two. Now, if I rerun the same code but remove the foreign key constraint, I get the expected result. I'm not sure if this is a bug in my understanding of SQL, but this code executes correctly on MySQL 5.0, so I think the problem is with HSQLDB and the interaction between foreign key constraints and multi-column IN statements.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

913
#913 count(distinct *) always returns 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
See the following example.
Run with latest 1.9 snapshot using a file based database.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

914
#914 1.9: Different date/datetime after migration from 1.8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Problem: Different date/datetime in cached tables after migration from 1.8.0.7 to RC
I migrated a HSQLDb database from 1.8.0.7 to 1.9 (RC from 10. Sept 2009) by shuting down the database under 1.8 and starting it up on 1.9. I did nothing special. On the first look everything went well - but when I did take a closer look on tables with date and datetime columns, I have recognized that the date content was wrong (e.g. 2009-02-01 is after the migration 41056-03-31).
Further it seems to be that this is only a migration problem for cached tables. With memory tables this problem does not occur (may be due to the fact that the data is "stored" as script).
In the following I provide you with some steps to reproduce the problem:
a.) Create table in HSQLDb Versioan 1.8.0.7
create cached table testupgrade (datefield date, datetimefield datetime, varcharfield varchar(10));
b.) Insert data into table
insert into testupgrade (datefield,datetimefield,varcharfield) values ('2009-02-01', now(),'2009-03-01');
c.) Retrieve data by using following statement: "select * from testupgrade":
DATEFIELD DATETIMEFIELD VARCHARFIELD
2009-02-01 2009-09-12 16:08:50.439000000 2009-03-01
(Note: Output is done via resultset function getString())
d.) Migrate: Shutdown database with HSQLDB 1.8x and start it with HSQLDB 1.9x
e.) retrieve data as in step c.) - I get the following result:
DATEFIELD DATETIMEFIELD VARCHARFIELD
41056-03-31 41668-07-11 11:13:20.439000 2009-03-01
As you can see the dates where changed/shifted.
f.) Check if this is a migration problem:
Now I did the creation of the table in the insert of the table compoletly with version 1.9x
create cached table testupgrade_new (datefield date, datetimefield datetime, varcharfield varchar(10));
insert into testupgrade_new (datefield,datetimefield,varcharfield) values ('2009-02-01', now(),'2009-03-01');
The output is as follows (select * from testupgrade_new):
DATEFIELD DATETIMEFIELD VARCHARFIELD
2009-02-01 2009-09-12 16:28:36.636000 2009-03-01
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

915
#915 res: databases with text tables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This item includes a jar containing a small database. This is a demonstration of packaging a database with text tables into a jar. Note the SET TABLE SOURCE commands and how the text files are referenced there.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

916
#916 Sequence not incrementing
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
There seems to be a problem getting the next sequence value when multiple "call next" statements are executed in the same HSQLDB session. Verified using Hibernate 3.3.2.GA and SQuirreL SQL Client.
It seems as if the problem occurs only in the same HSQLDB session (found when debugging the problem in context of Hibernate 3.3.2.GA). When a new HSQLDB session is used then a new sequence value is returned, but succeeding calls to "call next" will then return the then-new value again without incrementation (until a new HSQLDB session is used).
Steps to reproduce (using in-memory HSQLDB from SQuirreL SQL Client):
create sequence myseq as bigint
call next value for myseq
-- Returns 0
call next value for myseq
-- Returns 0 (should be a value greater than 0?)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

917
#917 Scale not respected for numeric (BigDecimal)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It seems as if the scale of numerics are not respected when inserting and retrieving BigDecimal objects using the "snaphot jar of HSQLDB 1.9 Release Candidate compiled on or after 16 Sept 2009 ".
ResultSet.getObject(String) returns the right value, but both ResultSet.getBigDecimal(String) and ResultSet.getBigDecimal(String, int) rounds the value prior to returning it. I have little experience with the inner workings of HSQLDB, but it may originate from JDBCResultSet.getColumnInType(int, Type) as it seems as if the source type lookup in "resultMetaData.columnTypes" is offset by one.
I have attached a small java program that creates a table with a "numeric(32,16)" column, inserts a BigDecimal and retrieves it. Retrieved values are output to the console. When I run it with the "snaphot jar of HSQLDB 1.9 Release Candidate compiled on or after 16 Sept 2009 " I get the following output:
Connecting
Creating table: create table testtable ( testcolumn numeric(32,16) )
Inserting: insert into testtable (testcolumn) values (?)
Binding: 200.88888
Selecting: select * from testtable
Got object: 200.8888800000000000
Got bigDecimal: 201
Got bigDecimalScaled: 201.0000000000000000
Got double: 200.88888
Got float: 200.88889
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

918
#918 1.9: Different date after migration from 1.8 (reopen)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
1.9: Different date/datetime after migration from 1.8 (refer to ID: 2857537)
The problem described in bug ID 2857537 is still occurring:
When I migrate from a database on version 1.8.0.7 to 1.9 the date/datetime values are changed.
I took the jar from the support page as requested by you in your last
comment of the bug 2857537 (source:
http://hsqldb.org/support/hsqldb.jar\)
Unfortunately I got the same results as explained in in the last bug request
(means the date/datetime values are changed).
I get it with the JAR file from the 10th September as well as with the actual jar file (I took them both from the support page!!)
Then I tried the jar from the RC Package:
http://sourceforge.net/projects/hsqldb/files/alpha_beta/hsqldb_1_9_0_rc4/hsqldb-1.9.0-rc4.zip/download
Using this package everything works as expected - the dates are migrated
properly from 1.8x to 1.9x RC.
For me it seems to be that there is a regression which was already in the JAR file of 10th september.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

919
#919 1.9: Different date after migration from 1.8 (reopen)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
1.9: Different date/datetime after migration from 1.8 (refer to ID: 2857537)
The problem described in bug ID 2857537 is still occurring:
When I migrate from a database on version 1.8.0.7 to 1.9 the date/datetime values are changed.
I took the jar from the support page as requested by you in your last
comment of the bug 2857537 (source:
http://hsqldb.org/support/hsqldb.jar\)
Unfortunately I got the same results as explained in in the last bug request
(means the date/datetime values are changed).
I get it with the JAR file from the 10th September as well as with the actual jar file (I took them both from the support page!!)
Then I tried the jar from the RC Package:
Using this package everything works as expected - the dates are migrated
properly from 1.8x to 1.9x RC.
For me it seems to be that there is a regression which was already in the JAR file of 10th september.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

920
#920 Ordering a "SELECT DISTINCT" by a "CASE WHEN"
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HSQLDB 1.8.1.1 fails to execute a "SELECT DISTINCT" if it is ordered by a "CASE WHEN" even though the "CASE WHEN" is also part of the SELECT list.
Example reproduced using SQuirreL SQL Client:
-- Create a test table
create table testtable (id varchar(32), myvalue varchar(32));
-- Works
select distinct id, case when not myvalue is null then myvalue else id end
from testtable;
-- Fails with "Error: ORDER BY item should be in the SELECT DISTINCT list"
select distinct id, case when not myvalue is null then myvalue else id end
from testtable
order by case when not myvalue is null then myvalue else id end;
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

921
#921 Hard-to-pinpoint regression: unsupported internal operation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
While making preparations to transfer a database which is in production use
under HSQLDB 1.8 to HSQLDB 1.9, I encountered several queries
which 1.8 executes successfully, but which cause 1.9 to freeze the thread
servicing the JDBC connection, so that from a client's perspective,
the query seems to take forever, while the server reports:
However, when I tried generating a minimal data set to attach to this report,
I encountered difficulties. No data set of a convenient number of records,
no matter how deviously I crafted it, was able to cause this crash.
However, real-world data from a customer's database, most probably error-free
but containing 240'000 articles and nearly 100'000 warehouse saldos,
do realiably cause this crash, and I'm still working to pinpoint how.
Thus, despite my current inability to accompany my bug report with any solution
beyond a crude work-around, I decided to submit it anyway, in hope
that someone might figure it out.
The structure of the tables involved in my scenario is like this:
-- Table for product articles, most fields are likely not involved
-- in the crash.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

922
#922 Defrag failure
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hit this problem during a long-running load test (multiple inserts/deletes over a long time).
After the error the app seemed to have recovered, however. Haven't been able to reproduce this yet (I've been running the , but I thought it may be useful.
Here're the settings for the db (from the script):
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

923
#923 select count returns wrong number of rows
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Found this bug in the latest version, but also appears to be in 1.9
The below code should be self-explanatory, but in a nutshell, it doesn't appear to deal with the same column appearing twice in the predicate list for a select count
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

924
#924 invalid ORDER BY expression error when sorting on aggregate
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When sorting on an aggregate the following exception is thrown:
Rob
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

925
#925 SQLException when using upper(?) in query
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When we use the upper function on query parameters, prepareStatement fails with this exception:
This works fine in Hsql 1.8.
See attached sample program.
Rob
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

926
#926 limit with offset works different when selecting PK or not
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have 2 similar tables, when selecting with limit and offset parameters, the results are different if the table has a PK or not.
When the table has a PK the behaviour is not compatible hsql 1.8.
In HSQL 1.8 both queries return 2 rows.
Rob
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

927
#927 invalid HAVING expression error when using query parameters
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When using query parameters in the having clause the following error is thrown:
See attached sample program.
Rob
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

928
#928 Suggestion: add more classes to the "hsqljdbc" build target?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
My use case: a HSQL JDBC client is installed on a Windows CE handheld computer, and it connects to a HSQL server on a Debian Linux box. The handheld computer has rather limited RAM and Flash memory, so minimizing the disk footprint of .jar files is important.
My concern: the build target "hsqldbmain" generates .jar files of about 1.5 MB, which also contain server classes. However, when I tried using "hsqljdbc" instead (it produced packages of about 380 KB), they didn't contain enough classes for JDBC client functionality to actually work - right as I tried loading the driver, I got a crash since the VM couldn't load necessary classes.
It thus appears that the "hsqljdbc" build target is too compact to support a functioning JDBC client. Perhaps it's compete enough when considered from some other aspect, but I found that I needed to add about 20 classes to it, increasing the .jar size to slightly above 500 KB.
I'm attaching a diff of "build.xml" to this message.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

929
#929 validateSpecifiedUserAndPassword has to be improved
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
tried JDBCConnectionPo#olDataSourcevalidateSpecifiedUserAndPassword with 1.9 version. Implementation could be completely fine if it need to compare two string objects, but since it should compare username and password, implementation needs to be improved.
From pure technical point of view null reference to string is not equal to zero length string, but from semantic point of view its one and the same thing. So if for example user input argument is null reference and configuredUser local variable is zero lenght string, function should not throw exception.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

930
#930 SYSTEM_COLUMNS difference betw. 1.8 / 1.9: bug or by design?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm currently trying to discover why OpenOffice form filters break
when run against HSQL 1.9.
Among the possible reasons for breakage are different results which 1.9 gives
when OpenOffice queries "INFORMATION_SCHEMA.SYSTEM_COLUMNS"
to determine table structure.
I have the following table and index:
Please don't take my report too seriously, I'm not sure yet whether this difference
is actually involved in breakage. I am currently testing this behaviour
with the 1.9 RC6 .jar file obtained from "http".
If testing with an SVN build would likely help, I can try that too - I picked the binary
for the convenience of OO folks, so they could easier obtain the same conditions
which I tested in.
If you wish to discover more about the context of my situation,
the bug report I've filed about form filter breakage in OpenOffice
can be found at:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

931
#931 wrong package name in documentation and runServer.bat
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HyperSQL v. 1.9.0-rc6
Documenation says to start the server use: java -cp ../lib/hsqldb.jar org.hsqldb.Server -database.0 file:mydb -dbname.0 xdb
The package name is wrong. It must be: org.hsqldb.server.Server (read this exactly!)
The same bug is in the start script runServer.bat.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

933
#933 UPDATE statement with IN criterion recently broken
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
I suspect that some fairly recent change to the codebase has broken
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

934
#934 Documentation update needed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The current documentation reflects old paths to HSQLDB server mode startup. the startup command should be:
java -classpath hsqldb.jar org.hsqldb.server.Server instead of just org.hsqldb.Server ... should be easy to modify and update.
Regards
Vyas, Anirudh
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

935
#935 PreparedStatement did not return generated keys
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I tried to retrieve an auto-generated key from an INSERT statement, but the generated keys result set is empty. See my test case below. The assert statement at the end fails. I am using 1.9.0-rc6. The code works fine with MySQL, PostgreSQL and Oracle.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

936
#936 Suspecting a memory leak (OOM after day's work)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
While stress-testing an installation of HSQLDB 1.9 in real use
with about 30 database clients of various sorts performing warehouse transactions,
I obtained an OOM by the evening, despite having allowed 1.2 GB of Java heap space.
Maximum log size is about 200 MB, and 1.8 never required nearly as much,
yet 1.9 seems to quite reliably exhaust any amount of memory given to it.
Before the crash, about 3.79 million statements were processed.
I unfortunately only obtained a thread dump, not knowing how to obtain more.
I had internal event monitoring running on level 2, but its log shows nothing of interest.
The thread dump however indicates a massive amount of objects in the "Old" generation.
I suspect this is a result of their handles remaining in memory, and garbage collection
being unable to collect them... that is assuming that garbage collection *tries*
to do that, which I sort of assume it *must* try without any extra encouragement.
I have not, however, tried enabling explicit GC calls.
Any ideas about what might be happening are very welcome.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

938
#938 "Integrity constraint violation" after prolonged work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
While attempting to track the reasons of great memory use (which I mentioned in another report),
with revision 3282, I also observed an effect which I believe might be separate from it,
but might be worth reporting.
It occurs after an HSQL server has operated and serviced clients for a notable time,
often as long as 6 hours. Out of the multitude of clients this server services,
it *seems* to be occurring with one particular client -- a client which frequently
disconnects from the server and reconnects to it, since its network link
is unstable and it's been designed this way to gracefully resume operation.
I have never seen it with the same database and nearly the same client
previously on HSQL 1.8.
The error message the server issues, implies that a problem with indexes
was encountered while inserting a value into an index, but surprisingly
the call stack suggests that the call actually originated from a rollback operation.
I am unsure what to think about it:
When this happens, it does seem to effectively guarantee that a SHUTDOWN COMPACT
statement won't execute correctly, while an onling backup with the BLOCKING option,
followed by a SHUTDOWN IMMEDIATELY (my usual way of recovering
from this situation) generally does execute correctly.
I have no reason to suspct that at the time of this error ocurring,
an actual integrity constraint violation would occur, since the data this program processes
is repetitive in nature, and then I'd be getting these errors multiple times per day.
What is meanwhile ocurring on the client (in case it provides any insight into the problem),
is typically a saldo import operation, whereby a lot (usually about 40'000) warehouse saldos
are read from an XML file, which has been supplied by a separate program.
In the below excerpt from my logs, the client has successfully imported 19'000 saldos,
then suddenly seems to pause for several minutes, reports the error, and proceeds to skip the rest,
since its ability to find article records from the article table has apparently been impaired
by something:
I can additionally certify that no network connectivity problems could have been involved,
since the HSQL server and client were operating on the same host.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

939
#939 "unique constraint or index violation" inadequate message
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a table with two foreign key constraints and two unique constraints with specific different names. I am using Spring Framework's JDBC tools to add data to the table. If I add a row to the table in violation of one of the foreign keys, HSQL throws an error like this:
org.hsqldb.HsqlException: integrity constraint violation: foreign key no parent; CODE_MAP_ENTRIES_FKCODE table: LOOKUP_CODES
Spring then wraps this with a DataIntegrityViolationException from which I can extract the name of the foreign key constraint to determine what kind of error to throw of my own.
However, if I insert a row in violation of one of the unique keys, HSQL throws an error like this:
org.hsqldb.HsqlException: integrity constraint violation: unique constraint or index violation
Spring appropriately wraps this in a DuplicateKeyException, however, since HSQL provides no constraint names in this error like it does with foreign key errors, I have no way of telling which unique constraint was violated. To make troubleshooting easier, the error should be changed to be:
org.hsqldb.HsqlException: integrity constraint violation: unique constraint or index violation; CODE_MAP_ENTRIES_TABLEOLDCODE table: LOOKUP_CODES
Or something similar.
I've tracked down at least where a change needs to be made to org/hsqldb/index/IndexAVL.java line 541:
However, I am simply not familiar enough with HSQLDB source code to be comfortable contributing a change like this to it because the table name and/or violated constraint name are not readily available within the body of that method from what I can tell. The table name isn't totally necessary, but the constraint name is definitely necessary. Every other major SQL vendor supplies the violated constraint name in the JDBC exception.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

940
#940 1.9 rc 6: database.data file has no up-to-date last-modify
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a database with a cached table in it.
For reasons of doing a catalogue file backup, I looked to the timestamps of the various catalogue files.
Thereby observing that the last modify date of the .data file was several weeks in the past although updates to the table
had taken place more recently.
Is this a feature or a bug?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

941
#941 poor query performance 1.9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have used v 1.8 for the last year and performance was great. Thought I'd try out 1.9 so I downloaded it last week and converted my database. The following query runs in less than one sec in 1.8 but over 15 seconds in 1.9.
I can provide 27MB zip of data on request. Tried to attach but got failure message.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

942
#942 1.8 Script stops working in 1.9 rc
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have no problem to execute the SQL statement above in one JDBC statement or on the SWING Manager from HSQLDB. Now it stops working on 1.9.0-rc6. This is error message I got - "user lacks privilege or object not found: T1 / Error Code: -5501 / State: 42501"
Does anyone know what have changed in 1.9.0-XX which made it not working?
Thanks
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

943
#943 set ignorecase true doesn't work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In 1.9.0.RC6, set ignorecase true doesn't work. I looked at the code and I see that Database.setIgnoreCase() is called when processing the command, but I couldn't find anywhere that Database.getIgnoreCase() was used when setting the column type. Perhaps that should be passed to the constructor of ColumnSchema so the data type can be changed to VARCHAR_IGNORECASE?
Here is some test sql.
The data_type in the last query shows name as 'CHARACTER VARYING' not VARCHAR_IGNORECASE as one would expect. Defining the column type explicitly as VARCHAR_IGNORECASE works.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

945
#945 .Net Provider Connection Times out
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In System.Data.HSQL.Client assembly, file HSqlConnection.Api.cs, in the following method, the milliseconds is converted from seconds by doing a binary "and" with 1000. Shouldnt that be multiplying by 1000 ? As a result no connection is getting opened because of the low timeout value
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

946
#946 Database corrupted
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Embledded mode, cache table, 1.9 b6. After restart software -
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

947
#947 Procedures - declarations for more than one variable
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
i tried to implement a procedure which inserts data and also uses the internal keys:
the application throws no error and hangs himself.
i'm using the current RC6 from HSQLDB 1.9.0 and jdk-1.6.0_14
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

948
#948 New release should be compiled for Java5, not Java6 only
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The RC6 is compiled for Java6 only (i get an java.lang.UnsupportedClassVersionError: Bad version number in .class file loading the JDBC driver)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

949
#949 LIKE behavior 1.9.0_RC7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
LIKE statements aren"t working as expected.
I seem to have the following behavior.
In a LIKE statment the '%' matches on a single character, so the '%' has the same result as an '_'.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

950
#950 Script file error in RC7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After moving from RC6 to RC7, application runs only once, on second run I got exception "java.sql.SQLException: error in script file line: 8 unexpected end of statement". Statement on line 8 is really incomplete: "SET DATABASE DEFAULT ISOLATION LEVEL ".
Here is the first lines of script file:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

951
#951 Hibernate and use of getColumnName() for column mapping
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have migrated our Hibernate 3.2 application to HSQLDB 2.0 RC7 and certain code started to fail.
Investigation showed that queries where we are inner joining to the same table caused this issue (since column names were duplicated)
The following QUERY
from A
would fail if it was through hibernate, since getColumnName for Ba.B_VALUE and Bb.B_VALUE causes hibernate looks up column names for each column. getColumnName(1) -> "B_CD" and getColumnName(3) -> "B_CD". Then when hibernate runs the query, it looks up by "B_CD" and gets the first one both times.
I have been looking at ways to get the JDBC Metadata getColumnName() to return the aliases, and have explored "get_column_name=false" parameter, but this has not worked.
I think this bug issued for mySql actually summarizes the issue better than I can: http://bugs.mysql.com/bug.php?id=35150
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

952
#952 user-defined nested function call fails RC7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
setup:
The error message I get back is "user lacks privilege or object not found".
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

953
#953 UnsupportedEncodingException for existing encoding
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In SqlFile class in setEncoding method is this:
It raise exception on existing encoding.
Related to 2.0.0 RC8 version
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

954
#954 NPE when passing null to a date argument of a Java SP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If you create a stored procedure in Java with a java.sql.Date as the type of one of the parameters then when you pass a null to that parameter HSQLDB throws a NPE. This is in 2.0rc8.
The FunctionSQLInvoked class has the following starting at line 111:
This will ensure all parameter values given as null will not be converted - they will be passed to the SP as null.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

955
#955 hsqldb_2_0_0_rc8: syntax error saving view to .script
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have created the following view
When I shut down hsqldb the view is stored in the .script file, however with a syntax error. Instead of '... OOMEP1WS_TEAMMEMBERASSIGNMENT A ON A."P$$PARENT" ...' it is written as '... OOMEP1WS_TEAMMEMBERASSIGNMENT AON A."P$$PARENT" ...'. There is a missing blank between 'A' and 'ON'. As a result the db can not be started anymore.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

957
#957 Execute default & unrevokable
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It appears (in 2.0.0 RC 8) that accounts have execute access to procedures by default, and it cannot be revoked.
Accounts should only be able to call procedures that they have been granted access to.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

958
#958 Parameter before "IN (SELECT..)" throws NPE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Version: 2.0.0 RC8
When executing a SELECT containing a parameter before a subselect an NPE is thrown.
Workaround: Don't use a parameter at this point.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

959
#959 Wrong select result in hsqldb 1.8.1.1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi all,
i created the following table:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

960
#960 build.xml target bug
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In order to build hsqldb and sqltool jars with jdk15, it's necessary to edit the target store in the following way. Instead of:
the depends list must be changed to: depends="s"
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

961
#961 sqltool.jar has dependency on hsqldb.jar
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
attempts to use sqltool fail in 2.00 rc8 because org.hsqldb.lib.RefCapableRBInterface is not present in sqltool.jar. Works if put hsqldb.jar on the classpath
but this is not consistent with the documentation
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

962
#962 Error upon indexing a DateTime field
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Testing with a fairly recent (but not assuredly latest) SVN version, I noticed the following:
1) I have the following table:
3) The following error occurs:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

963
#963 view does not work dep. where it is defined
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have an application which creates several tables and views and then populates the database with some data.
I observed that one view did not work after the database had been shutdown and restarted again.
I was able to track the situation down to the following observation:
if the respective view is defined after the create table statements (as is done in the script file), it does not work afterwards.
If it is defined after the tables have been populated, it works.
I think, it should work independently of where it is defined, hence this incident.
I add the sql file with which the problem is reproducable (see the comments in it).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

964
#964 2.0.0-rc8 - Merge does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am using the version 2.0.0-rc8 to test the merge insert update but it seems that there is some error being thrown from hsqldb.
I have Oracle Merge in my application code and I wanted to use HSQLDB to be be able to do some good standalone testing.
If this issue has been fixed already for some reason, could you let me know which version i can use?
If it will be fixed, then could you also let know when it will be available for release?
The following is the error:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

965
#965 tree style data in single table,delete all failure
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Test code :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

966
#966 hsqldb does not work on java <6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Does HSQL really require Java 6, or will it work on Java 5? I suspect it would. In that case can it please be compiled using target Java version 5. Otherwise we can't use it in websphere 6.1.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

967
#967 HsqlException: incompatible data type in operation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Error in 2.0.0-rc8 with Hibernate, works fine in 1.8.0.9 and Hibernate.
The SQL statement below fails to compile with the following Exception :
The error is caused by the ':newTargetCodeValue IS NOT NULL' part of the CASE WHEN statement. It doesn't matter what value I pass for :newTargetCodeValue, any value or null causes the error.
If I change the SQL Statement to the following (by removing the parameter from the CASE WHEN, it works fine (except the behaviour is now different obviously :)).
So looks like parameters within a 'CASE WHEN' bulk update statement are not working.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

968
#968 create schema v2.0-rc8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I try to use my hsql script which works fine in v1.8 with the current 2.0-rc8 release
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

969
#969 NullPointerException getting column for sub-query
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HSQLDB Version : Latest develpment build dates 19th Feb. 2010.
The SQL statement below (generated by Hibernate) results in a NullPointerException:
and lower(rollup1_.C_ACC_CODE) like ?)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

970
#970 HsqlException : incompatible data types in combination
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Found in latest development build of 2.0.0-rc8 (dated Feb. 19th).
Getting an "HsqlException : incompatible data types in combination" when attempting to run a bulk INSERT statement. The statement runs without issue on version 1.8.0.9 of HSQLDB and also other RDBMSes.
Script for table creation and failing INSERT statement are attached. The scripts are SQL Server scripts as I do not have HSQLDB equivalents (Hibernate creates the tables automatically when using HSQLDB).
Stack Trace :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

971
#971 Incorrect calc when using an Alias in a group by expression
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When using a column alias which matches the name of the original column in a GROUP BY clause. The GROUP BY clause appears to pass the value calculated in the SELECT statement back into the function in the GROUP BY clause which can result in duplicate rows for example:
This is the expected (correct) result!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

972
#972 DatabaseManager.removeDatabase must be synchronized
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
DatabaseManager.removeDatabase method changes fields like memDatabaseMap and databaseIDMap w/o proper synchronization. Other methods that change the fields (e.g. addDatabaseObject) are synchronized.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

973
#973 getUpdateCount should return -1 on second call
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In RC 8, the following will cause an infinite loop:
ps.getUpdateCount() should be returning -1, but is always returning 0. This appears to happen for INSERTs as well
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

974
#974 outer join result null or 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
on making an outer join to a table with "not null" columns you get in the query result a REALLY ZERO in case of an integer instead of NULL
results in case of all columns in the table t2 definition are not null allowed
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

975
#975 Differences in date after conversion from 1.8 to 2.0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I tested OOo with hsqldb 2.0 latest (and earlier) snapshot version 2010.02.21, and found a bug with it, I tried to test it with standalone hsqldb server.
I'm not java developer only tested with scripts.
I used test database for conversion testing and found, that not all date, time and timstamp values converted vell, in date I get minus 1 day, in time minus one hour, in timestamp, somtimes both changed sometimes only date, another times correct conversions.
I used for testing standalone hsqldb servers, too. I created a table with test data in 1.8.0.10, stopped server. copied all files (test.script etc,) from 1.8 data folder to 2.0 data folder. Sstarted 2.0 server, the conversion went through without error message.
I saved scripts to file system, I attach both version scripts, 1.8 with original data, and 2.0 with erroneous one.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

976
#976 \'getBinaryStream\' returns null over BLOB type
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Executing \'ResultSet.getBinaryStream()\' over a BLOB field returns null, even if the variable has been stored correctly.
The code works fine for BINARY fields.
Sample code attached.
The code creates a new table (either BLOB or BINARY type), stores a file (RampartSample01), and tries to read it from the DB and store the content to a new file.
Using BLOB throws NullPointerException.
Using BINARY works properly.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

977
#977 More complex SQL scripts stopped working in with 2.0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After migrating from 1.8 to 2.0rc8 my sql scripts stopped working. Apparently with 2.0 its no longer possible to create an object and refer to the new object in a later statement within the same script.
Please see attached unit test for details.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

978
#978 Row not found when it does exists.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have many cases where existing rows are not found by SELECT statements when they do match.
I have narrowed down the problem and have created a simple example that creates 2 tables T1 and T2. T1 has a foreign key to T2. There is one row in each table.
Executing a select from T1 joined to T2 does not return the existing row using the WHERE clause defined in the example.
In the example, I am using a parameterised query to obtain the data. 2 parameters are on T1 (alias 'a'), and a third parameter on T2 (alias 'b'). The query in the example fails. However, if I remove *either* of the WHERE clause clauses for T1, the query works! It's only when both clauses from T1 are used that the query fails. The query also works if you remove the clause for T2 but include both clauses for T1.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

979
#979 Row not found when it does exists.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have many cases where existing rows are not found by SELECT statements when they do match.
I have narrowed down the problem and have created a simple example that creates 2 tables T1 and T2. T1 has a foreign key to T2. There is one row in each table.
Executing a select from T1 joined to T2 does not return the existing row using the WHERE clause defined in the example.
In the example, I am using a parameterised query to obtain the data. 2 parameters are on T1 (alias 'a'), and a third parameter on T2 (alias 'b'). The query in the example fails. However, if I remove *either* of the WHERE clause clauses for T1, the query works! It's only when both clauses from T1 are used that the query fails. The query also works if you remove the clause for T2 but include both clauses for T1.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

980
#980 NullPointerException in BitType.compare().
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

981
#981 PreparedStatement TYPE_SCROLL_INSENSITIVE is broken
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
TYPE_SCROLL_INSENSITIVE queries are not supported for PreparedStatements in 2.0rc8
It looks like the Result.rsProperties isn't getting propagated correctly somewhere along the way.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

982
#982 NPE on RC8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I get this while trying to compile my schema using sqltool:
It is caused by the following view definition in the attached schema:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

983
#983 OOM Truncating a cached table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I created one simple cached table:
and filled if with 5 million rows.
When i preform a DELETE FROM TABLE1 a java.lang.OutOfMemoryError: Java heap space occurs. I'm expecting this as a rollback segment must be used. But the sames occurs when trying to truncate: TRUNCATE TABLE TABLE1.
This I wasn't expecting as no rollback segment is used. Is this a bug, or am I assuming something wrong?
A work around is to drop-create the table. Is this advisable?
Thanks in advance.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

984
#984 NullPointerException at updateRow()
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm calling updateRow() on a "SELECT * from singleTable" ResultSet. The singleTable has a private key two other tables depend on (FOREIGN KEY constraints with ON DELETE CASCADE).
When I call updateRow() (after updating some insignificant fields, not the private key), I get the following stack trace:
after removing the FOREIGN KEY constraints - all works fine.
I'm using 2.0.0 rc8, jdk1.6.0_11.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

985
#985 Incorrect query result
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I wish I could find a more concise way of reproducing this bug but here goes:
Please run the attached schema.
Then please run the attached test data script.
The following query incorrectly returns one row:
test all
To make it even more clear that this result is incorrect, we can change the query to the following: here, the 'clause1' and 'clause2' columns are exactly the same as the two parts of the 'where' clause which are ANDed together: test all FALSE TRUE
So effectively FALSE and TRUE is evaluating to TRUE...
I've been staring at this all afternoon to see if there's something I'm doing wrong, but I can't find anything.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

986
#986 Exception in RC9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When testing RC9, a query which worked in RC8 is giving a runtime exception in RC9
Please see my schema attached
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

987
#987 Table alias doesn't work is ResultSet.getString()
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I have the following query:
When trying to access the column value with table alias I get Column not found SQL exception.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

988
#988 Memory issues with HSQL DB
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We are using HSQL db as an in memory engine for our data processing. We do fire large set of sql insert and select statements during data processing. Once the processing is complete, we are deleting the rows inserted initially. These operations are done using JDBC connection. Once the deletion is completed, we see that the inserted rows are actually deleted from database successfully. But the additional memory taken by HSQL db at the time of insertion of rows is found not to be released even after deletion. This is creating problem for us as the used memory of the product keeps on increasing over a period of continuous usage. We need to know if there is some way to address this issue or if this is a bug with HSQL db, we would like to get a solution to this.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

989
#989 Testdb example not working in RC 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The testdb sample does not work any longer - throws "resource not found" exception in class Thread
(when run in debugger, tested in Eclipse and IntelliJ)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

990
#990 data exception: string data, right truncation in select stat
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
works perfectly with version 1.8, but gives "java.sql.SQLException: data exception: string data, right truncation" at the first "rs.next()" on the resultset with version 2.0 RC9. The db is too big to upload
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

991
#991 WRITE DELAY needs underscore
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
My env. : "JDK 1.6 , windows, hibernate 3.5.0 CR2, Netbeans latest version, HSQLDB 2.0 RC9"
I lost many hours testing and looking the source code of HSQLDB and finally found the bug.
In the documentation "guide.pdf", table 12.3, the property hsqldb.write_delay informs that we have to change/set the property "WRITE DELAY", but it needs an underscore to work correctly, i.e. "WRITE_DELAY".
After that there is a mention about "SET FILES WRITE DELAY". I think it is also wrong . It should be "SET WRITE_DELAY".
Furthermore other properties had worked as part of my URL connection, but "hsqldb.write_delay" NOT. I think it is a bug related to the Engine 2.0. Part of my URL is
But the database.script shows always the default value "SET FILES WRITE DELAY 500 MILLIS" instead of "SET FILES WRITE DELAY 2" when I create the db in the first time.
It works only when I explicitly execute the statement below.
check_params does not seem to work also. I changed "hsqldb.write_delay" in the previous URL to "hsqldb.write_delaz" but no error is returned. In the documentation informs :
"If the property is true, every database property that is specified on the URL or in connection properties is checked
and if it is not used correctly, an error is returned"
Well, thanks to hear me!
Best regards,
Jocafi
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

992
#992 getColumnClassName returns "Integer" for bigint field
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I was using apache common beanutils RowSetDynaClass to process the result set. It throws an exception about not being able to convert Long to Integer.
BeanUtils creates properties based on the types returned from ResultSetMetaData.getColumnClassName(). It returns "java.lang.Integer" for bigint fields. Later when BeanUtils tries to populate the property value, it calls ResultSet.getObject(columnName) which returns an object of java.lang.Long. Therefore, BeanUtils throws out the abovementioned exception.
From the documentation, my understanding is bigint is mapped as java.lang.Long in HSSQLDB. Isn't the returned column data's type be the same as what is returned from the ResultSetMetaData.getColumnClassName()?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

993
#993 order by on aggregate no longer works with latest snapshot
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I downloaded the latest snapshot (April 9, java 1.5 version), in this snapshot the order by on count-aggregate throws Error: invalid ORDER BY expression
Sample sql:
This worked with hsql1.8 and with earlier snapshots of hsql 2.0
Rob
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

994
#994 name of the constraint is different between CREATE and ALTER
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We use Apache DDLutils to manage HSQL database and we have a problem with the name of the constraints.
The result of name of the constraint is different between CREATE UNIQUE INDEX CONSTRAINT_NAME ON TABLE_NAME (COLUMN_NAME ASC) and ALTER TABLE TABLE_NAME ADD CONSTRAINT CONSTRAINT_NAME UNIQUE (COLUMN_NAME). In the first case, the constraint name is CONSTRAINT_NAME and in the second case the constraint name is SYS_IDX_CONSTRAINT_NAME_989.
thanks for your response and sorry for my vocabulary
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

995
#995 ArrayIndexOutOfBoundsException: 2 2 while creating function
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm trying to create function:
language java parameter style java
I have 3 overloaded methods in StoredProcedures class:
Functions with varchar parameters works well. The one with int parameter does not. I'm getting java.lang.ArrayIndexOutOfBoundsException: 2 2.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

996
#996 Dropped function is still dependent object.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In RC9, When I drop a function F that reads from a view V, and then drop V, it tells me that it cannot drop V because of dependent object F, even though F has already been dropped.
This gives me...
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

997
#997 NPE on RC9 and latest snapshot
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Please run my schema below.
Then enter the following test data:
Then do
But when it is part of a view, it doesn't.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

998
#998 Query returns inconsistent results.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I adapted my schema to work around issue 2987677 that I raised a couple of days ago.
Now I get another issue (which could be a manifestation of the same issue)...
Run my schema on the latest snapshot and insert the following test data:
Then try the following query:
This returns 0 rows, which is incorrect.
Then run:
Then try that first query again...
So the same query has returned different results even though the intermediate query does not update any data.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

999
#999 order by null desc
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Try:
which is correct.
But
which is not correct. It should return as it did with 1.8.0.8.
BTW, the new behaviour would be ok, if DatabaseMetaData.nullsAreSortedAtStart would return true,
but nullsAreSortedLow returns true, bout in 1.8.0.8 and 2.0.0 rc9
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1000
#1000 2.0.0-rc9 MERGE bug
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
SEVERE Rolling back SQL transaction.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1001
#1001 ata exception: string data, right truncation (26.April snap)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This all happens with rc9 as well as with the snapshot I downloaded yesterday...
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1002
#1002 Unique constraint messes up select
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am trying to upgrade from 1.8.0.7 to 2.0.0-rc9. I have a problem with a unique constraint with multiple columns. The simplest test case that I can manage is this:
The select does return no rows, although of course the second row should match. This used to work in 1.8.0.7. I reproduce this using the HSQL Database Manager.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1003
#1003 servlet mode not in the ROOT
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I had tried to use hsqldb in servlet mode:
I have this in my web.xml
The servlet is running.
The database is also running.
There is nothing in the tomcat access log, so it looks like it had not reached the servlet at all.
I was desperately trying to debug the org.hsql.jdbcDriver source but my Eclipse cound not build it with Java 6 nor Java 1.4 nor Java 1.1 - main problems about missing some methods implementation in teh jdbc driver classes - so I give up, but it looks like the answer lies somewhere in the DatabaseURL.parseURL.
zdenek
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1004
#1004 CASE WHEN space-pads results
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have this kind of CASE WHEN expression (this is a Hibernate subclass
discriminator):
This has so far worked as expected in 1.8.0.7, and it works without
padding in Postgres and Oracle.
I find that I can add TRIM to fix the code for now, but I wonder if
this is intentional or a bug? It seems to me that the branches of the
CASE WHEN should be independent of each other and so there is no good
reason to pad the result.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1005
#1005 data truncation error in WHERE clause
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using a too long string in a WHERE clause I get a data truncation error, when I use a PreparedStatement and a placeholder. Rejecting too long data in INSERT/UPDATE is really o.k., but in my understanding of SQL using a too long string value as parameter in a WHERE clause I should get an empty result set (or so, depending of what was selected, of cause), and not a SQLException.
When I use a "Statement" instead of a "PreparedStatement" the behavior of HSQLDB is o.k.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1006
#1006 MINUS operator doesn't work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I performed the following queries:
It seems that MINUS always returns the second argument.
I can provide my schema if you can't reproduce it.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1007
#1007 Foreign Key problem
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have two tables like so:
It seems to be because this order of attributes doesn't match the order in the primary key clause for the first table ie
PRIMARY KEY(TEMPLATE_GROUP_ID,COMBO_ID,LANGUAGE_ID,RANK),
When I changed the foreign key clause to the configuration shown at the top, with the attributes in the same order as the primary key, then it worked.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1008
#1008 Docs: read_only > readonly property
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Please change the docs for chapter 12 for property 'read_only' to 'readonly'. The underscore version won't work for 2.0 (and I think 1.8 neither). Thanks!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1009
#1009 Missing artifactId in pom for release 2.0.0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When using maven, builds fail with this error :
This artifact might be in your project's POM, or it might have been included transitively during the resolution process. Here is the information we do have for this artifact:
It seems that artifactId is missing from the 2.0.0 release.
It can be fixed by adding this to the pom :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1010
#1010 NullPointerException when using remote_open=true
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Test case:
Actual result:
Database manager reports a NullPointerException.
My analysis:
The exception originates at the server. It happens in situations when the server loads without any databases, such as when running with remote_open=true and without additional database.x/dbname.x parameters.
The reason is circularity in static initialization of org.hsqldb.Collation. During its static initialization, it performs: defaultCollation = new Collation(). This, in turn, causes the constructor of CharacterType to call Collation.getDefaultInstance(). Since this happens during the construction of the default instance, the getDefaultInstance() method returns null.
This doesn't happen when the server loads with a database, since the classes are loaded in a different order.
To reproduce, put breakpoints at Collation.java:156 (final static Collation defaultCollation = new Collation();) and Collation.java:177 (getDefaultInstance()) and compare running the server with remote_open=true vs. false.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1011
#1011 UNION CORRESPONDING causes npe
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I tried changing a view from union to union corresponding and I get a NPE.
Please see the attached schema
If you run the scripts domains.sql then templates.sql that should be enough.
The view is all_node_format in templates.sql
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1012
#1012 TABLE T with view causes NPE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The expression "TABLE T" causes an NPE if T is a view.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1014
#1014 SqlFile pass error - create function
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Execute a SQL file with SqlFile.execute() that contains create function failed. Reported error is as following:
This happend when migrating unit test based on HSQLDB from 1.8.0 to 2.0.0 to replace the deprecated CREATE ALIAS ... FOR HsqlDB SQL.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1015
#1015 When execute query : incompatible data types in combination
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
if the 'outdate' filed is Integer type, I use "select * from user where outdate=0 " execute query , that is fine. but when use "select * from user where outdate='0' " will throw exception...
see:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1016
#1016 INTERVAL Constant '0:0:0' Not Support
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Interval constant '0:0:0' (HOUR TO SECOND) not supported. Only '00:00:00' can be recognized. It is too restrict and it is not easy for the calling application to format the interval string.
Oracle supports '0:0:0' interval constant. HsqlDB should support it also.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1017
#1017 When execute query : incompatible data types in combination
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
if the 'outdate' filed is Integer type, I use "select * from user where outdate=0 " execute query , that is fine. but when use "select * from user where outdate='0' " will throw exception...
int the HSQLDB 1.8 and mysql , it's fine.
Developers is not easy to judge the outdate data type
see:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1018
#1018 select null throws exception
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
select null reports following error:
data type cast needed for parameter or null literal
for example,
select null from dual;
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1019
#1019 hsqldb2.0 has a bug, about fetchsize
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Use hsqldb2.0 database, when the implementation of the query, PreparedStatement set fetchSize value, query the value of the number of records than fetchsize, hsqldb there will be a mistake. Here is test code:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1020
#1020 Uncommitted records in COUNT aggregate
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
Not sure if this is fixed on trunk.
Uncommitted records get included in following statement,
-thanks
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1021
#1021 Committed data lost if no SHUTDOWN issued
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Committed data for CACHED tables lost if no SHUTDOWN issued.
Steps to reproduce:
1. Start swing gui with 'java -jar hsqldb.jar'
2. Open standalone database with CACHED tables
3. Update some data and COMMIT
4. Just close GUI
Result - commited changes was lost.
Expected result - commited data never lost, even if no SHUTDOWN command issued.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1022
#1022 Transaction broken in 2.0.1?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I downloaded the latest version from June 26 (this is 2.0.1, I think), the java 1.5 version.
Following sql gives me unexpected results:
create table test(x int)
start transaction
insert into test (x) values (1)
rollback
select * from test
The insert was not rolled back.
Rob
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1023
#1023 left outer join on temporary table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
One of our tests with the latest 2.0.1 version fails, (June 26 version, java 1.5).
Data setup:
create table a (x varchar(1))
insert into a (x) values ('A')
create table b(x varchar(1))
declare local temporary table MODULE.HT_B (x varchar(1))
A left outer join on b works fine, 1 row is returned as expected:
select a.* from a left outer join b on upper(a.x) = upper(b.x) where b.x is null
However, the same on the temporary table does not return the single row:
select a.* from a left outer join MODULE.HT_B b on upper(a.x) = upper(b.x) where b.x is null
Rob
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1024
#1024 COUNT gives wrong results when using aliased table names
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This is a regression. The following code performs correctly on 1.8.0.7, but fails on 2.0.0.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1025
#1025 ch09.html bad compression
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Navigating to http://www.hsqldb.org/doc/guide/ch09.html in Firefox 3.6.6 on Windows XP SP3 yields an error:
Content Encoding Error
The page you are trying to view cannot be shown because it uses an invalid or unsupported form of compression.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1026
#1026 ArrayIndexOutOfBoundsException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In the Class of org.hsqldb.util.ZaurusTableForm, Version 1.8.1.3.
In the function of "private void fetchColumns() ":
If exception raised in "tempType.addElement(new Short(colList.getShort("DATA_TYPE")));"(because cloList.getShort("DATA_TYPE")may raise an SQLException), when the program reaches "columnTypes[i] = ((Short) tempType.elementAt(i)).shortValue();" in the loop, a new exception ArrayIndexOutOfBoundsException will be thrown and the program will exit, becasue the array columnTypes has one more element than the array tempType.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1027
#1027 IDENTITY increase even shouldn't
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I noticed that the Identity id will be increased even if insert statements fail because of primary key violation.
Now each time the Identity id will be increase:
But just the first statement inserted a value to the database!
Testet with V1.8.0 and 1.8.1.3
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1028
#1028 Table meta-data for tables created in quotes is wrong case
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following class demonstrates the problem. When the table user is created using quotes then the meta-data for the table name is "user" when the table is created without the name in quotes the meta-data for the table name is "USER". Liquibase automatically quotes the table when it is called user.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1029
#1029 wrong query result wth Multi-column (A,B) IN (,,)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The version 2.0 provides new expression types as "Multi-column (A,B,C) IN ((,,), (,,), ) both with literals and queries" but returns unfortunately wrong results. I've tried the following request with Hibernate (which is working with MySQL) and get 10 rows back instead of 2 rows as it should. In fact it seems that the constraint "IN" isn't taking into account.
The query
returns 10 rows where the sub-query
returns only 2 rows.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1030
#1030 INSERT INTO texttable (SELECT * ...) does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Only reproduced with 2.0.0 instead of 1.8.0.
I didn't find any way to reopen that, so copying:
1) Create test tables
2) create a text table identical to customer (call it test)
3) set test source to "test.csv"
4) Execute insert into test (select * from customer)
I get this ex:
java.sql.SQLFeatureNotSupportedException: feature not supported
Thanks, Ondra
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1031
#1031 JDBCResultSetMetaData.getScale() doesn't check column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
JDBCResultSetMetaData.getScale() does not check to make sure that the column that is being accessed actually exists as the other get*() methods in this class. Instead it throws an ArrayIndexOutOfBoundsException when accessing data for a column off the end of the type array. I expect to get a SQLException like the other methods.
This behavior exists in the current 2.0.0 GA as well as the current development tree. It is reproducible by examining the code for JDBCResultSetMetaData.getScale() or by calling the method with an out-of-bounds column number (eg. -1 or Integer.MAX_VALUE).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1032
#1032 JDBCArray.getResultSet(JI) doesn't report all data
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When calling the getResult() methods of JDBCArray with the position and count parameters not all the data is retrieved. For example, if the retrieved array has 3 values and getResult(2, 2) is invoked then only one value (at position 2) is returned instead of the expected values at 2 and 3. See the attached Java program that reproduces the problem.
The probable cause is JDBCArray.newColumnResult() at around line 545 in the current work branch. At that line the for loop iterates from 'position' to 'count' instead of 'position' to 'count + position'.
This problem is observable in both 2.0.0 and the current work branch.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1033
#1033 java.lang.ArrayIndexOutOfBoundsException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello, I use hsqbdb 1.8 and I have no problem. I downloaded the 2.0 distribution and found that the following query fails in 2.0 (was working in 1.8) :
-- sample query :
unfortunately, as the query is generated, i cannot change it...
(tip : when changing "o.id as p1" to "o.someref as p1", the query runs again with no error, but that is not what i need...)
Any idea ?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1034
#1034 NPE when referencing a table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
See the attached Java program which will throw a NPE.
If you remove the "REFERENCES file(id)" on line 19, the NPE goes away.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1035
#1035 NPE when trying to drop a column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This bug looks similar to the one I reported yesterday (thanks for the quick turn-around btw!!).
See the attached program.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1036
#1036 Wrong union select result
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When doing select with union subselect like following:
on any table (contract is irrelevant) values of test field are all null but should not. For first select in union should be null but for second should be 1.
Without sorrounding select like this:
query result is proper.
The problem is only with null values, if I change column definiotion from null test to 0 test result is right.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1037
#1037 HSQLDB 2.0 and Eclipselink errors
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I just updated from 1.8.0.10 to 2.0 and suddenly my tests stopped working. I'm using this config:
and I'm getting these errors (none of which appeared before):
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1039
#1039 Duplicate table aliases in SELECT do not cause an error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I would expect a query such as the following to generate a parse error along the lines of "table alias 't' used twice":
This is not the case in HSQL 2.0 - instead, the alias appears to be bound to both table names, which is unexpected and presumably not the correct behaviour according to the SQL standard.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1040
#1040 to_date() function missing
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I try and call the to_date() function I get: user lacks privilege or object not found: TO_DATE
to_char() works fine. Other functions also don't exist, e.g. to_timestamp.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1041
#1041 Java function syntax doesn't work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The syntax for creating a Java function given in the documentation doesn't work:
It complains that the JAVA keyword should not be present.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1042
#1042 array comparison doesn't work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
i wanted to compare arrays in tables, but the result is always true. example:
which can't be. all arrays are different, even the number of elements doesn't match. my understanding of the documentation is that comparison of whole arrays is possible.
i'm using a current stable version 2.0.0
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1043
#1043 NPE in LobManager
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
presumably some entry in the data array is null and this wasn't anticipated, or the count is incorrect. Unfortunately this error seems to be intermittent and happens deep inside quite a complicated application, so I can't give precise reproduction steps.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1044
#1044 SET FILES SCALE not migrated from 1.8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When upgrading a 1.8 HSQL database the SET FILES SCALE setting isn't getting migrated properly. It appears that its by design (from what I can tell) that this can only be set on a fresh database with no data. In my testing my 1.8 database has a scale of 8, after upgrade it is changed to 1; which limits my upgraded db to 2GB (if its being honored, since it's not listed as a valid setting). If it's not being honored, the setting should be set to 8 rather than 1 (in the .script file).
I'm using 2.0.0.3754
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1045
#1045 Joins with parenthesis
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I create the following tables:
The I get this error message
I tried making it work using aliases for both tables and the joined table, but nothing did work. The same code works for MySQL, Oracle and PostgreSQL.
This bug is related to #1763757 , which applied to 1.8.x.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1046
#1046 ISO Year Format Incorrect Implementation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Version of HSQLDB 2.0
to_char function using IYYY returns incorrect year.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1047
#1047 it is failed that connect to DB
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
it is failed that connect to DB when exclamation mark in the full path of hsqldb.jar .
if running the following source, error occurred .
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1048
#1048 "in" clause in case or if statement not working
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
group by product;
As you can see from the results of the select, the "in" clauses in the "casewhen"s never match, but equals does. This is a regression from v1.8, where "in" worked in a "casewhen".
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1049
#1049 Java driver does not handle fractions for TIME data type
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The TIME data type discards fraction of second when storing to database as well as when getting from database. Note that in JAVA VM, all temporal data is managed in millis since epoch. Thus when TIME(6) is used, it must be possible to retain the millis and round-trip to database must retain Date that has same millis since epoch. This is not the case in current implementation - HSQL 2.0.
E.g. update MyTable set timeCol=TIME'10:20:30.450' where mypk=25;
This stores 10:20:30 in database and 450 millis are lost.
I do have following recommendation to fix the code:
The culprits are following conversion methods in org.hsqldb.types.DateTimeType class:
In convertJavaToSQL for TIME data type switch the code must be as follows:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1050
#1050 Nullpointer in hsqldb-2.0.0 TransactionManager
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am getting a nullpointer when banging the same tables with multiple queries and inserts from different threads. The exception seems to be happend in hsqldb's internal TransactionManager.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1052
#1052 RAWTOHEX NullPointerException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I do the following:
I use one of the latest jars from the support page. It declared itself as "HSQL Database Engine 2.0.1". I used in-memory database: jdbc:hsqldb:mem:test
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1053
#1053 Low performance in CREATE TABLE statements
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After having upgraded from HSQLDB v1.8.x to HSQLDB v.2.0.0 the Performance of "CREATE TABLE" statement decreased remarkably (I feel it take ten times longer, maybe measured exactly could show a five times longer durance.)
We use an in-memory-instance, very small tables with check-, foreign key- and primary key constraints, and every table is created with default, what should mean "in memory".
Has there been added any new feature in v2.0.0, which is responsible for this low performance, and how should I deal with it?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1054
#1054 is null stop working in HSQLDB 2.0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following sql works fine in HSQLDB 1.8, but the query return nothing after the 2.0 upgrade.
If I change the query to display "is not null", it returns correct number of rows.
Thanks
Jacklty
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1055
#1055 Power() doesnt work for negative arguments
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Power(2,-1) turns out with "invalid argument for power function". This is not normal behaviour in standard SQL. Negative number is a legal argument to Power,
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1056
#1056 ad-hoc resultsets not supported
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
select 1 as n
gives
unexpected end of statement
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1057
#1057 One prepareStatement method does not support generated keys
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When using version 2.0, I found that calling this method:
ps = connection.prepareStatement(query, new String[]{autoIncrementColumnName});
results in a statement that when executed with executeUpdate, does not return any auto generated keys. However by simply replacing that line with this one:
ps = connection.prepareStatement(query, Statement.RETURN_GENERATED_KEYS );
then the query starts to return the generated keys. This seems like a bug to me. The javadoc (http://hsqldb.org/doc/2.0/apidocs/org/hsqldb/jdbc/JDBCConnection.html) indicates that the first method *should* return a statement capable of returning auto generated keys:
prepareStatement(String sql, String[] columnNames)
Creates a default PreparedStatement object capable of returning the auto-generated keys designated by the given array.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1058
#1058 duplicate column name in derived table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I receive an error if I mix * with derived column.
If I list out all individual columns, it runs fine.
select A, B, C, A+B as D
from TEST
Thanks
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1059
#1059 database engine fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am using HSQL database to replace Oracle in a web application.
My connection is currently stored as an application attribute in Tomcat.
I am not using a connection pool, but just this one connection.
To test the web application I am using JMeter to hit multiple pages with 8 users.
The web application works fine with Oracle, but when I use HSQL I get the following error:
Then the server dies.
And I don't get any errors in Tomcat.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1060
#1060 Problem setting transaction level
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I tested latest snapshot jar (2010/10/04) from http.
Setting transaction level to Connection.TRANSACTION_READ_UNCOMMITTED sets connection into read-only-transaction mode. This behavior is different to 2.0.0 GA.
I can't beleave that this is defined in SQL standard.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1061
#1061 Cannot create table containing column "filter"
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I get a SQL exception when I try to create a table containing column "filter":
Escaping this column name does not help.
This is new with latest snapshot jar (2010/10/04), but was not in 2.0.0 GA.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1062
#1062 substring omits last character
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After preparation:
I used latest bugfix jar (revision 3808) for java 1.5. This problem was not present in 1.8.0.8
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1063
#1063 rawtohex and empty blob
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After preparation
I used latest bugfix jar (revision 3808) for java 1.5 and in-memory database. This problem was not present in 1.8.0.8.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1064
#1064 Round() - Inconsistent behavior
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The build-in function - round() doesn't give consistent result.
Can you please take a look?
Thank you so much
from test
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1065
#1065 Memory Leak when using Java Language Routines
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If I use any JRT functions in sql query, somehow the statement object creates an extra connection.
Even though I close my own connection with "Shutdowm" then JDBC close(), I could not get rid of both connections.
JProfiler shows that both connections are holding the entire database in memory which causing a leak.
The version I used is 2.0.1 RC1, and 2.0.0 doesn't have this problem.
Can you take a look? thanks
Here is my JRT example
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1066
#1066 Don't work Native L&F on start in DatabaseManagerSwing
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When start DatabaseManagerSwing, native L&F don't work (checkbox enabled), when choose Native L&F directly, then work.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1067
#1067 Error message duplicate
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
and testdb not exists, catching exception java.sql.SQLException: java.lang.RuntimeException: database alias does not exist database alias does not exist.
So, don't work "... a new, empty database is created if no database exists at the given path".
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1068
#1068 NullPointerException in Swing JTree
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Stopping
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1069
#1069 Command line wrong input not checked
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1070
#1070 redundant parenthesis in check constraints
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I create a check constraint like this:
then the information schema (INFORMATION_SCHEMA.CHECK_CONSTRAINTS) reports a check clause of
The parenthesis around PUBLIC.PRIMES.PRIM and the numbers are redundant, and should be removed.
I do use one of the latest releases fromm the support page for jdk1.5.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1071
#1071 ArrayIndexOutOfBoundsException With Array Out Parameters
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We're using HSQLDB to do the unit testing for MyBatis (formerly iBATIS). We found an issue with stored procedures output parameters that are arrays. Calling CallableStatement.getArray() returns ArrayIndexOutOfBoundsException. Here's a test case. I'm using version 2.0.1 rc2.
Thanks!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1072
#1072 Loading Binary File Hangs
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Java 1.6
HSQLDB 2.0.1-rc2
The table is properly created, the row is properly added, the binary data is properly load. When I attempt to update the table I end up in a recursive loop here.
Not sure if this comes into play as I am just getting familiar with the HSQLDB code base but the txManager is TransactionManager2PL. Please let me know if this is an error or bug or a PEBKAC issue.
Sincerely,
Tim
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1073
#1073 PSM (PL/SQL) routines do not see variables in MERGE statemen
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I try to use variables as arguments in MERGE statement, I get 'user lacks' error.
There is a script in attachement that creates database schema.
And here is a procedure that can not be compiled but probably is correct ('lack' on 'vstrTrunc' and 'vnID'):
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1074
#1074 Perfectly invalid date/time truncation in TRUNC() function
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Simple test case:
So I get :
Also, my time zone is GMT+3
And function TRUNC() does not know anything about 'SS' specificator.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1075
#1075 Invalid sequence number generation on UPDATE in MERGE stmt
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Script to create test schema is in attachement.
When there is no data in TM_VOICE_MENU_STAT table, values for nID column are generated sequentially and correctly.
So this MERGE statement works:
But when I execute it again, it fails on 'UNIQUE constraint or index violation'.
That can only be when value for update ST.nID is generated once.
Tested on revision 3889 from subversion repository.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1076
#1076 In JTree of DatabaseManagerSwing view and data is the same.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When choose View->Show row counts, And right-click on table (for example, "AUTHORIZATIONS ( 5 )"), and choose SELECT * from AUT..., then created wrong query SELECT * FROM INFORMATION_SCHEMA."AUTHORIZATIONS ( 5 )"
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1077
#1077 'set database sql syntax ORA' statement
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In documentation:
Use <set database sql syntax ORA statement> to enable support for DUAL, ROWNUM, CURRVAL, NEXTVAL and non-standard data types.
But in fact 'set database sql syntax ORA;' does not work: 'unexpected token ;'
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1078
#1078 convert for double returns exponent 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I do the following:
the this statement
select x, convert(x,varchar(40)) from tab
returns the following result:
I think, that all the trailing "E0" should be ommitted. As in MySQL, Oracle, PostgreSQL and hsqldb 1.8x. I use one of the latest releases from the support page.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1079
#1079 Invalid database restoration using log file.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
TEST CASE 1:
1. Create database using hsqldb.sql file.
2. Execute test from Tm_HSQLDBTransactionTest.java.
Test case does simple: it inserts data into memory table (TM_MENU_ITEM_IMP), then executes procedure (PR_UPD_MENU_ITEM_STAT). This procedure:
- moves data from memory table (TM_MENU_ITEM_IMP) to cached table (TM_MENU_ITEM_STAT);
- removes data from memory table (TM_MENU_ITEM_IMP).
After procedure call it commits a transaction.
3. When test finishes normally, cached data is written to binary data file, but database log looks like:
So there is:
- no procedure call, no PSQ statements;
- no deletions from memory table;
- no sequence alteration;
4. Connect to database with DatabaseManagerSwing and perform 'SELECT * TM_MENU_ITEM_IMP': There are records in memory table!
TEST CASE 2:
Do similar but terminate test case abnormally (by killing process or java machine) when it reaches instruction 'System.out.println("Not terminated abnormally");'
Now you can see that there are records in memory table and no committed data in a cached table!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1081
#1081 SQLException with NullPointerException in cause
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Database creation scipt (sql), empty database snapshot (tar.gz) will provide.
Tested under jdk 1.5.0.12. For tests used latest snasphot from Subversion.
Also, test case is in attachement, too.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1083
#1083 HSQL can't read it's own timestamps
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If I want to transfer some data from one database to the next I would select the columns from one database connection and then insert into the other database connection. This works fine for strings and numbers, but when it comes to times and timestamps, the output format of the select is incompatible with the input format of the insert. It would be really nice if HSQL could reads it's own output. The error is: Wrong data type: java.lang.IllegalArgumentException: Timestamp format must be yyyy-mm-dd hh:mm:ss[.fffffffff]
I've seen this in both 1.8.x and 2.0.1-rc2
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1084
#1084 NullPointerException in changeToDiskTable (session==null)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I got a NullPointerException on an Select () on a View which has more than 10000 items and following prev. statement set:
I think the problem is in the call to <tt>changeToDiskTable</tt> here (<tt>RowStoreAVLHybrid.getNewCachedObject(Session, Object) line: 235</tt>):
I use the the newest hsqldb-2.0 version.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1085
#1085 OutOfMemory
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After updating to hsqldb (02 December 2010. HyperSQL 2.0.1 RC3 ) my previous problem is fixed, thanks.
But now, my whole TestSuite doesn't work in the in memory-variant. The server-standalone is still okay, in the last stable version the test was okay with heapspace 512m, now with a lot more heapspace (1568m) I get a OutOfMemory at all.
The attached stacktrace only tell us the part where the memory is called, but not where the other memory is hold.
If you tell me what you need, I will give your extra information.
Greetings
Christian
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1086
#1086 USING keyword not accepted
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We normally run our application with a Oracle 10g R2 database.
For unit testing reasons we'd like to use HSQLDB (we downloaded hsqldb-2.0.0).
We setup a JUnit test environment were we read in our DB scripts to create the DB.
We adapted current scripts by converting VARCHAR2 --> VARCHAR, NUMBER ---> INTEGER or NUMERIC or similar.
But we thought, the USING keyword is allowed since it is on the feature list.
But we got following error
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1087
#1087 Query with result errors when mixing subquery and group by
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
to view the error, try:
prepare data with this querry:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1088
SYSDATE not accepted
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
At http://www.hsqldb.org/doc/guide/ch09.html we can read that SYSDATE can be used:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1089
#1089 datediff 'ms' returns nanoseconds
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It seems that in hsqldb-2.0.0, the following returns units in nanoseconds.
select datediff('ms', CURRENT_TIMESTAMP, dateadd('ss', 1, CURRENT_TIMESTAMP)) from dummy
According to the documentation, it should return 1000 (milliseconds). But it returns 1000000000 (nanoseconds).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1090
wrong result set for query
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When i run the following query on the attached database (2.0.1-rc3 version) i get 2 results instead of one.
You can see that the join does not work correctly:
Checked the same on 2.0.0 version and it was fine
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1092
#1092 Case/When unsupported internal operation: ExpressionOp
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I had a query that used to work with V1.8.1 but now fails in V2.0.0 with ArrayIndexOutOfBounds exception and now in V2.0.1RC3 with 'unsupported internal operation: ExpressionOp'.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1093
#1093 Uncaught Exception
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have downloaded the latest driver for 1.6, today, and the problem still exists:
We have a DB-connection monitoring, which catch Exceptions due to connection-lost. This works quite fine. Since we have implemented a generic UncaughtExceptionHandler for all running Threads in the app the Exceptions from HSQLDB are also caught:
java.lang.RuntimeException
at org.hsqldb.jdbc.JDBCConnection$1.run(JDBCConnection.java:2829)
Little bit annoying is, that this exception cannot be handled in our app. Anyway, is there a chance that this exception is caught within HSQLDB-driver?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1094
#1094 INTERVAL Constant '0:0:0' Not Support (oringal Id  3019349)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have report the same bug (Id 3019349 ) before but it is closed as invalid. But when I check ANSI SQL-92 standard, your comment is incorrect. '0:0:0' is a valid hour to send interval according to ANSI SQL 92 standard:
Values in interval fields other than SECOND are integers. SECOND,
however, can be defined to have an <interval fractional seconds
precision> that indicates the number of decimal digits maintained
following the decimal point in the seconds value.
Leveling Rules
Because I can't reopen the original bug, I create it again.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1095
NPE in RowStoreAVLDisk
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am using hsqldb 2.0.1-rc3 compiled for JDK 1.5 with hsqldb.default_table_type=cached
The problem occurs when 2 threads are both deleting rows from the same table.
I get this exception :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1096
#1096 OutOfMemoryError when using cached tables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am using hsqldb 2.0.1-rc3 compiled for JDK 1.5 with hsqldb.default_table_type=cached
I added 50000 items in my table while using cached table (As expected I get an OutOfMemoryError when I use memory tables)
When I try to delete these items , I get an OutOfMemoryError :

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1098
#1098 incompatible data type in conversion with java.lang.Boolean
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When adding a java.lang.Boolean as an Argument of a PreparedStatement via
java.sql.PreparedStatement.setObject(int, Object)
im getting a Stacktrace like this:
The Query looks like this:
I tried 2.x rc2 and rc3 and it seemed to be no problem in 1.8.x
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1099
#1099 'incompatible data types in combination' with timestamps
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm getting an SQLException, running an arithmetic operation on a Timestamp.
My java code looks like:
This query happened to be no problem in hsql1.8.x. I already tried hsql2 rc3
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1100
#1100 'LIMIT 0' broken
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
'select * from anytable limit 0' is a valid query (I agree, doesn't make a lot of sense) and HSQLDB 1.8 correctly returned zero rows. Release 2.0 (including 2.0.1 RC3) throws an error: 'Data exception: invalid row count in fetch first clause, Sql state: 2201W, Error Code: -3452'.
The bogus part is in org.hsqldb.QuerySpecification, lines 1178-1187:
Line 1178 checks for <= 0. Line 1182 allows 0 as a valid value, but asigns 'no limit'(!) which again would be wrong, if line 1178 were fixed.
In my opinion, 'LIMIT 0' should, as in the earlier release, return a result with zero rows. All other rdbms behave like this.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1101
#1101 invalid results for queries on indexed columns
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Those 2 examples demonstrate strange behaviour relating to queries on indexed columns.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1102
#1102 Discrepancy between getObject(int) and getArray(int)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have found a discrepancy between the two PreparedStatement methods getObject(int) and getArray(int) for DATE ARRAY columns.
It can be reproduced as follows:
The getArray(1).getArray() returns an array containing java.sql.Date classes, which is the expected mapping. This also happens when selecting getObject(2). But getObject(1) returns an array holding a HSQLDB-internal class. I guess the automatic mapping was forgotten for datetime types? It seems to work for numeric or string types...
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1103
#1103 NullPointerException in insert into INTERVAL Type
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have found a NullPointerException on insert into INTERVAL Type.
It can be reproduced as follows:
SQL code
Usage on INTERVAL Type is correct?
I use by default as VARCHAR.
I guess the problem is in the call to IntervalType.convertToDefaultType method here (IntervalType.convertToType(SessionInterface session, Object a, Type otherType) line: 620):
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1104
#1104 hsqldb.init & SERVER_ADDL_CLASSPATH
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Starting from line 400 variables are being exported because the jvm is started in a new shell with eval exec.
But variable SERVER_ADDL_CLASSPATH has been forgotten.
And thus user defined classpath is not given to jvm.
So line 401 should read: export SERVER_ADDL_CLASSPATH
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1105
#1105 Parameterization of days number causes NullPointerException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Note: The query "select current_date - 20 day from document" works fine.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1107
#1107 FrameworkLogger resets application log configuration
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
it seems very odd to me that an embedded database would feel free to reset the logging configuration of the entire application. as it is, our application does programmatic configuration of the java.util.logging framework upon startup. however, when we attempt to open an hsqldb database, the FrameworkLogger class resets the jdk LogManager to its original configuration.
i'm surprised that this is not the default behavior, but at the very least there should be a configuration setting for telling hsqldb to leave the logmanager alone.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1108
#1108 hsqldbmin build target does not seem to work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
i built the 2.1.0-rc4 release using the ant targets "switchtojdk16 hsqldbmin". however, the resulting jar file would not work in my test code (using a simple, single threaded, embedded db).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1109
#1109 jdk Logger leak in FrameworkLogger
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
when using the jdk logging system, each new database adds a logger to loggerInstances, but these loggers are never cleaned out. they should be removed when a database is "shutdown". (in our application, databases tend to be transient, and we may open many different dbs while the app is running).
using hsqldb version 2.1.0-rc4.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1110
#1110 Query with DISTINCT erroneously returns no results
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
As you can see when running the attached SQL, some queries using DISTINCT return no results,
but correctly return repeated results when DISTINCT is omitted.
This bug is in all of the 2.1.0/2.0.1 release candidates, but not 2.0.0GA, nor 1.8.1.3.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1112
#1112 wrong property name in docs for max memory rows
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
the database property for the max number of rows in memory is "result_max_memory_rows", but the (2.0) documentation refers to the property as "max_result_memory_rows".
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1113
#1113 Can't delete db files after shutdown with NIO
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If HSQLDB uses NIO to access data files, I can't delete the database data files after the database is shutdown. This looks like a symptom of long standing JDK Bug 4724038 (http). When calling FileChannel.map(), the returned MappedByteBuffer will hold a handle to the underlying file until it is garbage collected. Windows can't delete files with open handles.
I've attached a test case that demonstrates this problem and a patch to ScaledRAFileNIO that implements the workaround described in JDK Bug 4724038. I believe this workaround should be safe for HSQLDB since the buffer is unmapped immediately before removing all references to it. The patch has only been tested with Sun JDK 1.6.23 on Windows x64.
It looks like the folks at H2 hit a similar issue: http
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1114
#1114 Throwing Exception: Running an HTTP server to use batch file
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I couldn't have run an HTTP server by ClassNotFoundException when starting to use runWebServer.bat.
The package name mistakes?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1115
#1115 Exception while upgrading to version 2.1 from working 1.8.7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I did upgrade from version 1.8.7 to the latest 2.1.0 and I am getting following exception for a code which was working before. I am trying to insert
a PDF binary file into a table like this
following is the java code which was working on 1.8.7 and is failing on 2.1.0
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1116
#1116 "database does not exist" opening jdbc:hsqldb:res: catalog
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In revision 4124, org.hsqldb.lib.FileUtil.FileAccessRes.openInputStreamElement(...) was updated to use the thread context classloader if the current classloader was unable to load the requested resource.
However, the isStreamElement(...) method - which is used to verify that the resource exists before calling openInputStreamElement(...) - uses only the current classloader.
This prevents a jdbc:hsqldb:res: catalog from being opened in situations where the use of the thread context classloader is called for, and results in an exception like the following:
Method org.hsqldb.lib.FileUtil.FileAccessRes.isStreamElement(...) should be updated to use the thread context classloader in the same manner as openInputStreamElement(...); a proposed SVN patch for base/trunk is attached.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1117
#1117 databasemetadata not showing 2.1 info
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Database product version still at: 2.0.0
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1118
#1118 index and ordering by multiple columns
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a database with about 500k user PROFILES. There is an index on LAST_LOGIN column. Because there are sometimes multiple rows for same values of LAST_LOGIN for queries I use ordering by LAST_LOGIN and ID. This is very useful when using queries for paginating results. For example when I have last previously received LAST_LOGIN and ID, I create query like " select * from PROFILES where LAST_LOGIN > ? or (LAST_LOGIN = ? and ID > ?) order by LAST_LOGIN asc, ID asc". For reversed direction query looks like " select * from PROFILES where LAST_LOGIN < ? or (LAST_LOGIN = ? and ID < ?) order by LAST_LOGIN desc, ID desc". The same query ordered by single column takes 15 ms, ordered by both columns takes 15 seconds, so I suspect index is ignored when ordered by additional column despite the fact second column in order by clause have small impact of returned results.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1119
#1119 NullPointerException using MINUS with large tables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When executing a minus statement such as "SELECT item_id FROM TABLE1 MINUS SELECT item_id FROM TABLE2" with tables with many rows I get a null pointer exception in RowSetNavigatorDataTable (see below). Attached is a test to reproduce this. It happens when I populate the tables with at least 100000 rows but not when less than 10000 rows. I'm seeing this in hsqldb 2.1.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1120
#1120 Util.sqlException for Result throws base SQLException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
org.hsqldb.jdbc.Util.sqlException(Result r) does NOT delegate to the sqlException function that throws the correct SQLException depending on the sqlState.
This means you can't catch any of the SQLException hierarchy from the client code.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1121
#1121 sql.enforce_size property does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Whilst upgrading from 1.8 to 2.1 ran into the problem of the change in VARCHAR length behavior that is well described in the documentation. However, one can't work around this with properties after all.
Setting the sql.enforce_size property to FALSE (before creating tables) does NOT prevent exceptions from exceeding specified VARCHAR lengths in version 2.1.
Checking the .script file shows that the statement "SET DATABASE SQL SIZE TRUE" is always present, regardless of whether the property is set to false or not.
Also, using the "older" property described in the documentation, "sql.enforce_strict_size" triggers an invalid property exception.
David
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1122
#1122 Adding support of org.hsqldb.types.Types.DATE as a Timestamp
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We are using HSQL as an Oracle DB emulation and needed this fix to support Dates as timetamps.
The attached patch fix this issue from 2.1RC3.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1123
#1123 Adding support of Inserting boolean in number(1)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We are using HSQL as an Oracle DB emulation. Oracle supports inserting a boolean in a number(1) column. The attached patch adds this functionality.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1124
#1124 Adding support of NVARCHAR2 & dotted selects
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We are using HSQL as an Oracle DB emulation. The attached patches adds support of NVARCHAR2 and dotted queries like this :
select table.col1 from table
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1126
#1126 Using ARRAY in with recursive
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If you create the following table 'RECUR' and execute the provided 'with recursive' statement the error
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1128
#1128 Upload 2.1.0 jar to maven repository
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Please upload the "Latest stable version" (i.e. 2.1.0) release artifact(s) to the Maven central repository.
I have some unit test scripts that use nvarchar, which work with 2.1.0 but give an "unknown data type" with the latest version currently available there (2.0.0).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1129
#1129 Allow column defaults of form "(1)"
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
A table definition I extracted from a MS SQLServer database had columns specified with the form
but when I try to run these into an in-memory hsql database, it gives an "unexpected token" error at the opening brackets.
I assume the "literal" definition in
(http: doesn't allow for this. It would be handy if it did, at least when
SET DATABASE SQL SYNTAX MSS TRUE
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1130
#1130 ClassCastException when casting to longvarchar
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm getting a ClassCastException from the hsqldb driver when executing this SQL statement:
This doesn't work anymore with the newly released version 2.1.0. It worked with 2.0.0. The stacktrace from the HSQL Database Manager:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1131
#1131 Cannot have procedure parameters of type longvarchar array
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This used to work with version 2.0.0.
With version 2.1.0 it doesn't anymore, I get an error:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1132
#1132 SQL DATA access clause is now mandatory
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm not sure if this is wrong now in 2.1.0 or if it was wrong in 2.0.0 and fixed now. But it's not really documented here:
I didn't find a comment about that in the release notes either.
This used to work in 2.0.0:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1133
#1133 Cannot create CHAR[ACTER] LARGE OBJECT type
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
According to the documentation, I can use CHAR[ACTER] LARGE OBJECT as synonyms for CLOB when creating tables:
This doesn't work in 2.1.0, though:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1134
#1134 LONGVARCHAR seems to default to LOB now
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The newly added setting seems to be defaulted to TRUE (maybe after upgrading from 2.0.0 to 2.1.0? I'm not quite sure...):
SET DATABASE SQL LONGVAR IS LOB TRUE
It would be more backwards compatible, if it were set to FALSE by default.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1136
#1136 LIMIT 0 returns one record
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
These two queries return the same record:
I would expect "LIMIT 0" to return nothing (like MySQL, Postgres. SQLite, H2) or raise an error (like Derby). Is this the expected behaviour or is it a bug?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1137
#1137 DatabaseManagerSwing won't start with -noexit
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
won't start and throw exception "No value for argument".
In main(String[] arg),
throws exception when there is no arg for -noexit.
This will fix the issue:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1138
#1138 DatabaseManagerSwing throws exception
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In Rev 4201,
This will also check for the case DatabaseManagerSwing --user --password. The addition won't take --password as the argument of --user.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1139
#1139 rolling back to unnamed savepoint rolls back entire txn
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
i'm using hsqldb 2.1.0GA. in testing savepoint functionality, i found out that rolling back to an unnamed savepoint (created via Connection.setSavepoint()) seems to roll back the entire transaction. when i run my test with a named savepoint it works as expected.
my test is essentially:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1140
Clarify LOCK TABLE behavior
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The behavior of LOCK TABLE statement as described in the documentation does not specify whether READ lock is shared and WRITE lock is exclusive. Is it an implementation of "readers-writer lock" as exists in Oracle/PostgreSQL "lock table {table_name} in share|exclusive mode" statement?
If not, please adjust the documentation, if yes, there is a problem with taking locks (assume 2 concurrent transactions):
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1141
#1141 Array and wasNull-Call
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I figured out that the resultSet.wasNull() does not work correctly while using SQL arrays.
example:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1142
#1142 The SQLSyntaxErrorException while createArrayOf in 2.2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Any call of the createArrayOf("TIMESTAMP", new Date[] {new Date()}) method for a JDBCConnection in HSQLDB 2.2 throws an exception.
The same thing for HSQLDB 2.1 is OK.
My systems were:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1143
#1143 Table and Schema information missing from metadata at random
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It seems that under certain conditions, Hypersonic will return empty schema and tableName metadata even when such data should be available. For example, this query:
Results in metadata only for the invoice_invoice, prod_product, and invoice_element tables. The rest have schemaName and tableName values that are left blank. Shuffling the joins around does affect which tables are missing metadata, but not in a predictable way. For instance, sticking the two outer joins in the query above at the very end of the list of joins results in all tables except prod_type having metadata. I can provide a database for examination upon request.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1144
#1144 NullPointerException when ordering by NULL
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This is a minor issue. When I run this:
I get a NullPointerException:
This works:
This doesn't
I think the NPE should also have an associated error message. On the other hand, why is 1 permitted, and no other value is?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1145
#1145 offset/limit broken with unique-index
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It looks like, a unique-constraint breaks the behavior of offset/limit. I attached a sample db, which was stripped down from our production table. Decoupled from our project, it doesn't make a lot of sense, but it shows the bug.
The following query lists the rows I'm interested in:
Now using offset/limit, I would expect to be able to get the same result piece by piece. For example, the following query should return the rows with rid 82 and 83, but it returns 88 and 89:
Or from offset 4, I would expect rows 86 and 87, but I get 90 and 91:
With offsets 6, I get the same result as with 4. Same for 2 and 8. Same problem with other offsets.
I then observed that after removing the unique-constraint for the four columns SCHEMEIDENT,CLIENTTYPE,CLIENTID,VARIANTID it works as expected.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1146
#1146 Comparing DATE column to DATETIME produces error in 2.2.2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Comparing a column of type DATE to DATETIME values no longer works in HSQLDB 2.2.2. The build made on May 17, 2011 to address the metadata issue does not have this problem. The expected results for the below queries that produce errors are the same as the results of the previous query.
Sample script:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1147
#1147 JDBCClobFile length() bug for fixed width enc > 1 byte
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm just posting this here so that it is a known bug between releases 2.1 and 2.2.2.
Basically, the bug results from failing to provide a sufficiently rigorous unit test, togther with an inadertent typo where the file length is multipled (*) by the fixed character byte width, where it should be divided (/).
This will only manifest for truely fixed width encodings of over 1 byte per character, such as "UTF-16BE" and UTF-16LE" (which are hopefully rather uncommon)
This will not manifest for UTF8 or UTF16, since they are not fixed width encodings, and will not manifest for latinxxx or any other single byte encoding, since multiplication and division by one both yeild the same value.
The fix and updated unit tests will be commited today.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1148
#1148 Exception getting connection
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
org.hsqldb.jdbc.JDBCDataSource.getConnection(String, String) checks field user for not being null instead of corresponding accepted argument.
It leads to exception "invalid argument: user" if user was not set previously explicitly with setUser(String)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1149
#1149 Create Temporary Table as select WITH DATA
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
if you create a table from a select statement, and the data is to be published into the created table, doesnt work.
This only solution is to create with no data and then insert the the rows.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1150
#1150 session hangs with invalid use of table function
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This can be reproduced most easily with the HSQL DatabaseManager. Create this function (or probably any function returning a table):
The session hangs. The database server is not affected, i.e. new connections are possible. Also, it doesn't seem to matter whether the database is run in in-memory mode, or server-mode. I have observed this with both versions 2.2.1 and 2.2.4. Here's a thread-dump of the hanging thread:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1151
#1151 INFORMATION_SCHEMA.COLUMNS regression or new feature?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
There seems to be a regression between versions 2.2.1 and 2.2.3/2.2.4 in the INFORMATION_SCHEMA.COLUMNS view (I didn't check 2.2.2). If it's intended, then I didn't see it in the release notes:
I think this is quite an important change between such minor releases for it not to be mentioned, that's why I thought it might be a regression. Here's what I observed. For this table:
I used to get these values in 2.2.1:
Now I get these values in 2.2.4:
What do you think about this?
Cheers
Lukas
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1152
#1152 INFORMATION_SCHEMA.ELEMENT_TYPES doesn't work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In your release notes, I found that you have added this new INFORMATION_SCHEMA.ELEMENT_TYPES view in 2.2.3:
I have tried selecting from it and got an unhandled exception:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1153
#1153 java.lang.Bug on query handling
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
java.lang.ArrayIndexOutOfBoundsException encountered on a queries to a particular view structured as demonstrated by the attached test program
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1154
#1154 Error in handing update/insert/delete on a view
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Thanks for the prompt response. However, it looks as though the fix for 3354244 had introduced new bugs. It is no longer possible to prepare statements to update/insert/delete on that particular structured view.This is demonstrated by the attached test program
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1155
#1155 union and character padding
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I do the following:
create table tab (n integer)
insert into tab values (1)
select t from(
(select 'aa' t from tab) union
(select 'bbbbb' t from tab)
) b
then the first result row is not 'aa' as expected, but 'aa ' with three trailing white spaces.
This is easier to see with
select concat(t,'x') from(
(select 'aa' t from tab) union
(select 'bbbbb' t from tab)
) b
I used
Bundle-Version: 2.1.0
Bnd-LastModified: 1300071888199
Specification-Version: 2.1.0
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1156
#1156 RuntimeException with ORDER BY CASE...
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This is how to reproduce it (with 2.2.5):
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1157
#1157 ClassCastException with < ALL operator
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Here's another one of those highly unlikely queries that I tend to come up with for my own integration tests :-)
This doesn't
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1158
#1158 NullPointerException when calling char_length
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Casting again.
This works fine:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1159
#1159  Statement.RETURN_GENERATED_KEYS in Batch broken sine 2.2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We have Batch insert operations that works with generated keys
The flow that used to work in hsqldb 2.1.0 is now broken in 2.2.5 (and 2.2.4) (BTW the same code is on in Oracle and MySQL)
The error I'm getting now is:" java.sql.SQLException: invalid cursor state: identifier cursor not positioned on row in UPDATE, DELETE, SET, or GET statement: ; ResultSet is empty"
JDB code is as flowing:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1160
#1160 incompatible data type in operation for call parameter
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
prepareCall("select (X'cafebabe' || ?) from dual") fails with 'incompatible data type in operation'
adding a cast for the parameter eliminates the error, but should not be required because the LHS of the catenation is unambiguous (binary type)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1161
#1161 Doubts about new ELEMENT_TYPES table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi Fred,
I'm trying to get a hang of all the new equijoins between dictionary views that you impose on me :-) I have found an inconsistency in version 2.2.5 (or maybe I'm misunderstanding something?). Consider these three simple stored procedures:
I can use the DISTINCT keyword as a workaround, for now... What should be the correct behaviour? Because if duplication is correct (due to explicit matching by parameter ordinal), then I'd be missing the ordinal information from the INFORMATION_SCHEMA.PARAMETERS view
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1162
#1162 Bad link on hsqlUsing.html
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
On this page here:
...the link to ObJectRelationalBridge - OJB is "broken", i.e. the OJB project has been retired in favour of Apache's OpenJPA implementation:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1163
#1163 Duplicate Column Names even with Alias
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Select both fields and a third using substring of one of them with an Alias.
HSQL throws an exception "Duplicate Column name in derived table"
I think it doesn't take the alias into consideration as the new name of the field.
To reproduce the bug: execute the following 2 statements
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1164
#1164 Aggregate Function - unsupported internal operation: Type
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After updating HSQLDB from 2.1rc4 to 2.2.5, I can no longer perform the following SQL query anymore (calculate weighted average with 2 columns)
... 29 more
Could you take a look of it?
Thank you
jacklty
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1165
#1165 NullPointerException when dropping schema
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hsqldb 2.2.5
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1166
#1166 subselect doesn't work correctly with a view
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
when using a view in an inner select as described in the following example, no results are returned.
when the view usage is replaced with the table or tvo.val in the where clause is replaced by a hard-coded string, results are returned as expected.
The query is a simplified example of SQL generated by Hibernate-Envers so modifying the SQL is no option for us. We use a database view to support legacy code.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1167
#1167 Identity colum: Inconsistency between documentation and code
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The documentation suggests at http states the following syntax for column definition
The key (for this issue) is the comma (,) between START WITH <n> and INCREMENT BY <m>. When I do this I get an exception "saying unexpected token: , required: )". Basically saying it is expecting a space instead of a comma. If I remove the comma between start with and increment by it works. So it would seem that the documentation needs updating, right?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1168
#1168 BooleanType.convertToDefaultType() does not support numbers
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
BooleanType.convertToDefaultType() does not properly convert a number object to a boolean for insert/update into a database. It seems to only handle Boolean and String types.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1169
#1169 Exception over an HTTPConnection on huge records insertion
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The Java test file is uploaded here.
The test results are below:
My systems were:
1) Mandriva 2010.2 i586, Oracle JRE 6 Update 26 (home computer)
2) OpenSUSE 11.4 x86_64, Oracle JRE 6 Update 26 (office computer)
The results are same.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1170
#1170 Create table hangs after writing to another table
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using HSQLDB 2.2.5, if I do the following:
1. Create connection1
2. Create table1 using connection1
3. Create connection2
4. Write to table1 using connection2
5. Create table2 using connection1
The call to create the 2nd table hangs. Is this expected behavior? See the attached test case.
Using HSQLDB 2.1.0, the attached test case passes.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1171
#1171 parameter of type char(1) in merge takes 32K of memory
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After upgrading from HSQDBL 2.2.4 to 2.2.5 we noticed a dramatic increase in HSQLDB's cache's memory consumption. After some investigation we discovered that a parameter on a CHAR(1) value in a MERGE statement was being padded out to CHAR(32K). For a cache size of 50,000 rows this translates into roughly 1.5GB of additional memory usage.
Here's the table definition and the merge statement:
The problem does not reproduce if we change the data type in the merge statement to a varchar. I've also attached a JUnit test that shows the problem and will lead to OutOfMemoryErrors when run with a max heap size of 1GB.
It looks like this behavior was introduced in r4345.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1173
#1173 error parsing a parameterised SQL
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following query is legal
The above SQL is accepted by both Oracle and DB2.
Attached is a test program for this
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1174
#1174 Missing caused-by exception
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When an IOException occurs during the JDBCPreparedStatement.setBinaryStream() the causing exception is lost because not given as cause to Util.sqlException().
should become:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1175
#1175 VARCHAR is space padded or trims trailing spaces
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
VARCHAR appears to be padded with trailing spaces (or trailing spaces are trimmed), which means that VARCHAR columns with a unique constraint can't have 2 entries differing only by trailing spaces. I would expect this behavior from CHARACTER but not VARCHAR. See the attached test case.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1177
#1177 DISTINCT crashes in 2.2.6-SNAPSHOT
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Attached test case blows up on SVN 4417 + 4421(HEAD)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1178
#1178 2.2.5 Memory Leak on 'MERGE INTO' with sample code
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Scenario:
1) I start with no existing database on the file system
2) After 'INSERT INTO' 10000 records in a table using an prepared statement, and closing down the connection, about 10MB is in use in the Java VM after Garbage Collecting 3x with 2 second sleeps in between.
3) I remove the database files from the file system, and change the prepared statement to use 'MERGE INTO', and shoot the same 10000 records into the database. Result: about 700MB(!) are in use after 3x GC
Other trial run scenarios:
- I executed on both Java 1.7 and 1.6 and the results are similar.
- I executed the prepared statement in batch mode, as described in the manual: Still same results.
- I changed the scenario to 5000 records, and the 'in use memory' after closing down the connection varies significantly (to me this denotes a second kind of memory leak, as it applies both to the INSERT and MERGE scenarios.
Here are the numbers.
Attached is the sample code.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1179
#1179 JDBC 2.0 ResultSet updateRow is not working as expected
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have simple ResultSet with update of current row (if required by application).
It works in MySQL and Oracle. But Breaks in HSQLDB.
Can you please be so kind to look at it?
Environment tested hsqldb v 2.2.5 and any before. Java 7 or Java 6
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1180
#1180 Upload 2.2.5 jar to maven repository
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
please deploy the latest version to central maven repository
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1181
#1181 Integrity constraint violation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Summary how to represent situation is in attached SQL script.
First we create schema with 3 tables (dictionary, temporary table, storage table) and 1 view to select data.
Then we create procedure that merges data from temporary table into storage table.
When there is data in a temporary table, all is ok.
But when there is no data in a temporary table, call statement:
Fails with:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1182
#1182 ORA compatibility:rownum misunderstood in DELETE query
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In a DELETE query, ROWNUM can be used to restrict the number of records to delete in one query, like for example:
DELETE FROM hugetable WHERE ROWNUM < 1000;
The above statement should delete 1000 rows from 'hugetable'. However, in HSQLDB this statement has no effect if the total number of rows in the table exceeds 999 rows (0 rows deleted). ROWNUM seems to be interpreted as the total number of rows in 'hugetable', which is not correct. ROWNUM (in Oracle) means the current row it is acting upon, meaning it will run from 1 to the maximum rows in the table.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1183
#1183 SQL Routine NOT FOUND handler not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
From the doc, it said whenever update affected 0, should go to not found handler, but it did not do that.
In below example, you can see table B, msg is empty always.
Example:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1184
#1184 Trigger+Merge do not see variables in Merge statment
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Similar to an old bug:3107413 - PSM (PL/SQL) routines do not see variables in MERGE statement.
This time is a trigger. The trigger cannot compile, it said cannot see newRow pointer for the NEW data.
Thanks Fred!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1185
#1185 Allow for omitting parentheses when calling procedures
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
According to the Javadoc of CallableStatement (and to some SQL dialects, especially that of Oracle), stored procedures without any arguments can be called without parentheses, e.g. these two should be the same
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1186
#1186 Calling org.hsqldb.Server.main with the argument  "--props"
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Whensoever you call "java -cp ../lib/hsqldb.jar org.hsqldb.Server --props ../bin/config/server.properties" following exception is thrown:
But the argProps has still the property named system.props=../bin/config/server.properties and this property is for the server an invalid property, so in the method Server.setProperties(HsqlProperties) the calling of HsqlProperties.validate is thrown the org.hsqldb.HsqlException.
The patch removes the server.props property from the argProps if the propsPath is not null.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1188
#1188 Trigger "Instead of update" fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Update is not working on views with "instead of update" trigger.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1189
Can't drop view if any table is being locked
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Even though the view and table are unrelated
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1190
#1190 bug in parsing create table statment
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
As diescribed in the doc a create table statment with auto incremented key should have following syntax:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1191
#1191 bug in parsing create table statment
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
As diescribed in the doc a create table statment with auto incremented key should have following syntax:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1192
#1192 Cascaded Views + Sub-Select  ArrayIndexOutOfBoundsException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following schema cannot be imported into 2.2.6-SNAPSHOT
I'll keep my fingers crossed, that you'll find a solution to this problem. ;)
Cheers,
Andy
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1193
#1193 getGeneratedKeys() multiple row inserts does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Inserting multiple rows with PreparedStatement (sql, "generatedColumnName") and addBatch()/executeBatch does not return generatedkeys with HSQLDB 2.2.5, 2.0.0.0 works fine with no code changes, just changing the HSQLDB jar.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1194
#1194 DbBackup missed command line option for AbortUponModify
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I saw DbBackup has the setter/getter for AbortUponModify, but don't know how to change it's value from just command line.
When backup a database, will do below.
$JAVA_HOME/bin/java org.hsqldb.lib.tar.DbBackup --save --overwrite ./backup/$CUR_TIME.tar ./data/fix
But if I want to AbortUponModify=false, i cannot pass value in command line, Seems have to write code.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1195
#1195 Referenced views are absent in VIEW_TABLE_USAGE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It stated that VIEW_TABLE_USAGE should provide information on TABLE and VIEW objects that have been referenced in the query expressions of the VIEW objects, but it provides only tables:
CREATE TABLE table1 (col1 int);
CREATE VIEW view1 AS SELECT * FROM table1;
CREATE VIEW view2 AS SELECT * FROM view1;
select * from INFORMATION_SCHEMA.VIEW_TABLE_USAGE
Result is:
VIEW_CATALOG VIEW_SCHEMA VIEW_NAME TABLE_CATALOG TABLE_SCHEMA TABLE_NAME
PUBLIC PUBLIC VIEW1 PUBLIC PUBLIC TABLE1
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1196
#1196 One SqlTool function definition variant broken
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Needs to be verified, but I think that creating functions from buffer fails if an "appendage" string is not given.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1197
#1197 doc mistake re. raw mode terminator
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I think that online docs (and maybe Util guide) state the wrong characters to terminate raw input. The examples or user guide fragments must be correct, because I know some people are using the feature successfully. Another possibility is that the incorrectly documented usage applies only to either interactive or non-interactive usage so that not all users are affected.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1198
#1198 INFORMATION_SCHEMA.SYSTEM_COLUMNS field name typo
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In the table INFORMATION_SCHEMA.SYSTEM_COLUMNS field header for "table_schema" is missing trailing "a" so it stands as "table_schem".
See query:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1199
#1199 HSQLDB 2.1 fails with a core dump
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Please see the attached core dumps.
OS: Windows NT 6.1 , 64 bit Build 7601 Service Pack 1
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1200
#1200 statement.execute executes multiple sql statements
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The statement.execute(String) method executes multiple sql statements separated by semicolon. This might allow sql code to be injected into a sadly programmed application.
Please do not understand me wrong: There might be some space for interpretation in the jdbc specification of the Statement interface. So this behavior might be ok in the sense of the specs. BUT: an app that uses code like this:
is vulnerable for sql-injection if the user inputs some thing like this:
As this is bad programming it might not be the problem of hsqldb either!
BUT: for example the oracle jdbc driver does not allow mutliple statements in the Statement.execute method. This adds an additional level of security that hsqldb is not having.
I stumbled across this behavior when working with owasp's webgoat intrusion example webapp and was quite shocked to find a way to add update/insert statements into a read-only application. I had not thought that this was possible.
So if this is intentional behavior, please think about its usefulness in comparison to the security impact. If it is a bug, please fix it :-)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1201
#1201 Failing NULL value check with decode in ORA SYNTAX mode
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
Detected in versions
Decode function fails to identify null value in ORA SYNTAX mode (sql.syntax_ora=true)
Thanks for your help.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1202
#1202 Table / column aliasing doubts in nested selects
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have found some weird behaviour when aliasing tables and fields. This here is pretty clear:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1203
#1203 NullPointerException when unnesting empty array literal
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
These statements result in a NullPointerException:
These statements wor
Is this a syntax error? According to the documentation, it seems to be. At least one value expression has to be provided between the brackets. But then how to construct empty arrays?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1204
#1204 object not found when fully qualifying it in nested JOINs
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Here's a funny thing I have stumbled upon. Create this database:
Now, this query with a somewhat odd syntax works well:
It looks like the tables b and c are somehow implicitly renamed and no longer form a part of the "PUBLIC" schema. I'm guessing this is a bug, as I couldn't find any explicit reason in the documentation around this definition that would justify this behaviour:
Also, HSQLDB is the only database I know that has this behaviour.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1205
#1205 HSQLDB breaks my logs
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I establish a connection to HSQLDB, logging no longer works.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1206
#1206  current value for sequence is null !!!
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1207
#1207 NOT IN predicate and NULL values not SQL standard compliant
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Maybe I have not grasped some subtlety in how this really *should* be according to the SQL standards... In my understanding, the following should return an empty result set:
The rationale taken from the SQL 1992 standard chapter 8.4 <in predicate>:
And then, further down:
In the case of my example:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1208
#1208 Regression in TRUNCATE statement after upgrade to 2.2.8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have detected a regression in my jOOQ integration tests for the TRUNCATE TABLE statement after upgrading from 2.2.6 to 2.2.8. The following is specified in the documentation:
"If the table is referenced in a FOREIGN KEY constraint defined on another table, the statement causes an exception"
Most other databases also behave this way. But HSQLDB 2.2.8 seems to silently execute such table truncations. Note, I'm not using the NO CHECK clause
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1209
#1209 Cannot unnest arrays resulting from nested selects
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I seem to have a regression when unnesting arrays that result from nested selects. Create the following schema:
Run this query:
I have observed the issue in HSQLDB 2.2.6 and 2.2.8. I think I had seen this working before, but maybe it's just a syntax confusion?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1210
#1210 Error when unnesting arrays from "preceding tablereferences"
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
See also possibly related issue here:
I'm running a test against this schema:
The test query looks like this:
According to the documentation of UNNEST the above should work:
The documentation reads:
"The array expression can contain references to any column of the table references preceding the current table reference."
In the example, "STRING_ARRAY" is a column of "T_ARRAYS", which has been referenced precedingly. Nevertheless, I get the following exception:
It does work like this, however:
I think I had seen this working in a previous version of HSQLDB, but I'm not 100% sure.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1211
#1211 select distinct * from ( select...) doesn't work any more.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following select worked until 2.2.6-snapshot (2.2.6 pre-release). Since then, it returns:
Error: user lacks privilege or object not found: C8
I think it's something with the 'distinct': if we ommit him, it works.
Thank you,
Lucio I.P.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1212
#1212 DECODE fails as of 2.2.7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Starting with 2.2.7, this statement started failing for me:
I've tracked the change that broke it to SVN revision 4878 in ParserDQL. With OpTypes.MATCH_FULL passesd in, flow ends up going into case "OpTypes.MATCH_FULL" in ExpressionLogical.getValue(). This results in a call to ExpressionLogical.testMatchCondition(), which throws an exception of "row column count mismatch". Prior to 2.2.7, flow goes to case "OpTypes.EQUAL" in ExpressionLogical.getValue(). This results in a correct evaluation of the data.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1213
#1213 update LOCAL TEMPORARY TABLE leave COMMIT in *.log
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I used an local temp table. Then I frequently insert/delete record from it. Yes, it will not write those SQL into *.log, however, it left many COMMIT in the log and cause it huge. In fact, I've disable the auto check point, because I found that the auto check point will cause HSQLDB hung. So the *.log file will be very big and full of only "COMMIT". Any idea?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1214
#1214 Unnecesary SQLWarning:cursor updatability mismatch
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
prepareStatement for update created unnecessary warning "cursor updatability mismatch" W_36502
The code example is in attachment, All executed just fine in HSQL, Oracle and MySQL.
Only warning are annoying when executing Unit tests using HSQL.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1215
#1215 EOFException when database grows past ~5GB
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Running 2.2.8, we are randomly hitting an EOFException when our database size goes over ~5GB. We run our database with the settings:
I poked around a little bit in ScaledRAFile, specifically looking at how the fileLength member is maintained. It seems like the logic in the write() method "if(!extendLength" might not be correct. I ran some test with the logic reversed "if(extendLength" and that may clear up the problem (although, i can't guarantee that since the issue is not 100% reproducible).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1216
#1216 Pathological Query Performance in 2.2.8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
My company is trying to integrate hsqldb into our product and while trying it out with some test data, we hit a class of queries that result in pathological query performance -- about 5 minutes on my machine. For comparison, the same query runs against MySQL in about 300ms. Here's the query:
An "explain plan" showed that some joins used some indices, but at some point, it did a full table scan of the ec_job table. The MySQL explain resulted in no full scans and a very different plan. See attached file.
I've also attached a zip file of the hsqldb we were testing with. How to connect to it:
I was running on an 8GB laptop with 64-bit IntelliJ IDEA with 2GB heap. I used the database console to issue the query.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1217
#1217 NullPointerException concatenating empty blob
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Precondition:
Table containg an empty blob value
Attachment:
The attachment contains a maven project with a junit-test reprocuding the error.
The test can be executed using "mvn test" in the project root directory (assuming maven bin-directory on the path)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1218
#1218 Double.NaN equals any DOUBLE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HSQLDB supports NaN, INF and -INF in its DOUBLE datatype. Equality for NaN appears to be broken.
The result is one row one column containing NaN. It appears that (NaN=x) is TRUE for any x, including NaN. I believe that's exactly the wrong way around. (NaN=x) should be FALSE for any x, including NaN itself.
HSQLDB 2.2.8 release, Java 6, OS X, observed on in-memory database.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1219
#1219 Exception when comparing empty (zero-length) BLOB or CLOB
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1221
#1221 OutOfMemory error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Revision # 4531
In heap dump (512 megabytes allocated for java machine):
OS: Linux, 32-bit
Problem didn't happen instantly. System worked for some days then this situation happened.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1222
#1222 OutOfMemory error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Problem has been too quickly closed but I think resolution is not correct.
Revision # 4531
In heap dump (512 megabytes allocated for java machine):
Problem didn't happen instantly. System worked for some days then this situation happened.
Last resolution:
Well, this dump indicates you have one million rows in MEMORY tables. These
tables are stored in memory and quickly fill up large memory allocations.
If you can manage with a larger heap, you can continue using MEMORY tables,
otherwise you need to convert some of these tables to CACHED tables.
But that's not right. That's initial script in attachement of database scheme.
Application (3 threads) does insert in *_IMP tables and then calls PR_UPD_*_STAT(). Then an insert in TM_DATAPROCJOURNAL is made and then commit.
Also 3 threads perform select of all rows from *_STAT tables ordered by date to export into text file.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1223
#1223 BINARY field display as [B@12345 in Database Manager
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Other data types are represented in roughly the same syntax as their literals. The current display of Binary types is unusable.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1224
#1224 Server fails with ClassCastException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Create custom function with declared result NUMERIC, but actual result is TABLE. When function is called, server fails.
Example:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1225
#1225 Inaccurate documentation on the datetime TRUNC() function
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I've tried using the TRUNC() function as specified here:
It says
The <char value expr> is a format string for YEAR, MONTH, WEEK OF YEAR, DAY, HOUR, MINUTE or SECOND as listed in the table for TO_CHAR and TO_DATE format elements (see above).
But that's not correct. I've tried using 'DAY' as a literal, but that doesn't work:select trunc(current_timestamp, 'DAY')
This works, though:
The same seems to apply for ROUND()
I'm not sure if this is an engine bug, or a documentation bug
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1226
#1226 Datetime TRUNC() truncates to UTC, not to local timezone
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This query here:
The same applies for ROUND()
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1227
#1227 NPE in CharacterType.substring
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Getting a null pointer exception in CharacterType.substring
Here's the SQL to reproduce, the error and the stack trace:
Not very convenient but does the trick
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1228
#1228 missing hsqldb.init
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The hsqldb.init script is not to be found at locations given in Appendix D: those are dead links.
Also, FWIW, it seems arbitrary to paste the hsqldb.cfg and hsqldb.rc scripts verbatim into the docs but not hsqldb.init.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1229
#1229 foreign key constraint name not shown in GUI db manager
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If I create a named foreign key constraint, say with name FK1, in the left pane under indices I can see, e.g., SYS_IDX_10037 instead of SYS_IDX_FK1_10037 as expected but in INFORMATION_SCHEMA.TABLE_CONSTRAINTS I do see my FK1. It looks like the GUI tool has wrongly used "" as the constraint name.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1230
#1230 procedure returns updateCount even with READS SQL DATA
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
A CallableStatement's execute() method will always return false when calling a stored procedure; that is, the first result will always be the update count and getMoreResults() must be called to obtain the first ResultSet, if it exists.
While this is expected behavior for a procedure capable of modifying data (e.g. when the procedure has the parameter MODIFIES SQL DATA), the update count should not be returned when the procedure has the parameter READS SQL DATA; such a read-only stored procedure will always return an update count of 0 and is therefore redundant and non-standard API.
Currently, statementRetType in JDBCPreparedStatement is always set to StatementTypes.RETURN_COUNT regardless of whether the stored procedure is MODIFIES SQL DATA or READS SQL DATA. Solution: when READS SQL DATA is specified, statementRetType should be changed to StatementTypes.RETURN_RESULT so that getMoreResults() inside the JDBCPreparedStatement.fetchResult() method is automatically called (on line 4667).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1231
#1231 ClassCastException when doing simple select
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We have a table definition:
Where the sql is:
resuting in the following error:
Why do we get this ClassCastException?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1232
#1232 Servlet server mode fixes for 2.2.8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
For me the Servlet server mode operation was not working, and Iooking at some postings on the net it has been broken for quite a while. There were 3 distinct issues causing servlet more to fail:
1) The servlet as is would only work in the root of the webserver
2) The client-code would not correctly set the content-length of the post-request, causing an EOFexcpetion on the server when reading the payload
3) A similar issue to 2) on the reponse being generated on the server: The response content-length was not set correctly by the server, causing EOFEXceptions on the client
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1233
#1233 HSQLDB 2.2.8 server not shutting down cleanly
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HSQLDB 2.2.8 server is not shutting down cleanly when given the "shutdown compact" command. The server begins the shutdown sequence, but never exits. I have a small sample database that I'm using (I can make this database available if necessary).
The process I'm following is:
I have also used a java class to create a JDBC connection (rather than using DBVisualizer) and get the same results.
I have attached the output from the server process on the command-line. Let me know if there's anything more you need.
Thanks.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1234
#1234 WITH RECURSIVE returns only first level children
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Query using WITH RECURSIVE returns only first level children, even for query in docs:
As I see union/unionAll methods in RowSetNavigatorData cleans result which is used later.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1235
#1235 2.2.8 Hangs with select
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm having trouble with an update where I have the following:
The initial state of the database is a charge with a decision attached.
The action perform is a replacement of the decision on the charge with a new decision.
I've attached full logs of update, and result of "SELECT* FROM INFORMATION_SCHEMA.SYSTEM_SESSIONS"
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1236
#1236 Unsupported WITH clause in subselect
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Usage of WITH works fine in HSQLDB 8.2, but apparently not in a subselect.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1237
#1237 COUNT(DISTINCT(ID)) returns 1 instead 0 for VARCHAR NULLs
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Given the table:
Reproduced on 2.2.6, also reproducible with 2.2.8 and latest snapshot.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1238
#1238 Caused by: org.hsqldb.HsqlException: expression not in aggre
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
The simple request below doesn't work whith HSQLDB 2.2.8 but works fine with MySQL.
Is this a bug in HSQLDB ?
Anyone knows a workaround ?
Here is the stack trace :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1239
#1239 "set table source ..." fails since 2.2.6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It seems there was a bug introduced in release 2.2.6 in the area of text tables. Up until 2.2.5 it was possible to set the java system property textdb.allow_full_path=true and then attach CSV files using the SET TABLE statement as follows:
Note the path in the SET TABLE statement differs from the path actually to be opened. I checked the docs and it seems the behavior of release 2.2.5 and earlier is consistent with the docs.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1240
#1240 Incorrect result sets when using WITH and aggregates
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I think the following 3 queries should be equivalent and return the same
results, however, only the 1st is correct. I originally found this as
queries referencing a CTE in a WHERE clause were giving incorrect results.
PostgreSQL returns the correct result for all 3.
Returns empty result set
ps. I was going to submit this several months ago :( Anyway, finding
HSQLDB fantastic so far.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1241
#1241 JDBCCommonDataSource isn't Serializable
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Since JDBCCommonDataSource isn't serializable and the subclass is, Java doesn't store the values from the class JDBCCommonDataSource when the datasource is serializaed. Therefore when deserializing the url is null.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1242
#1242 incorrect results for subqueries within a view
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It appears that hsqldb (v2.2.8) is botching queries against views
defined with more than one subquery. The following SQL test case
will explain this far more clearly than English:
In general, the first subselect in the view is evaluated correctly,
and the rest are not.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1243
#1243 DDL session is never freed by TransactionManagerMVCC
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
version 2.2.8
The problem is probably in MVCC and in the following piece of code:
So txManager.commitTransaction - never happens in the session A and cannot release TransactionManagerMVCC.catalogWriteSession
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1244
#1244 PreparedStatement keeps references even after closed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
JDBCPreparedStatment.close() does not nullify the session variable. The session variable hangs on to a cache object. This means that PreparedStatements take up much more memory than needed after they are closed. In my application, this is adding up to about 42 megabytes of RAM. I know that I can fix the problem by nullifying my PreparedStatements when I'm done (and that's what I'm doing now), but I thought it would be a good idea to fix this by nullifying the session variable in PreparedStatement.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1245
#1245 Text table: Delimiters not encoding-aware
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using hsqldb 2.2.8.
Text tables can be declared with a text encoding parameter, if the encoding of the underlying file is not ASCII. Unfortunately, this encoding is not applied to field delimiter and line separator characters. This fails with encodings that are not strict supersets of ASCII, most prominently UTF-16.
Example:
As you see, the fs (tab) and the CRLF at the end have not been properly encoded as UTF-16.
Attempting to configure a text table with a properly encoded text file which already contains data also fails because HSQL does not recognize the delimiter/separator characters.
The manual states that "[t]he default is encoding=ASCII and the option encoding=UTF-8 or other supported encodings can be used". I have not been able to find a list of "supported encodings".
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1246
#1246 a UNIQUE constraint does not exist on referenced columns
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Tested on: v2.2.8, and the latest snapshot of 2.2.9 posted on the hsqldb homepage
Also tested on MySQL, and works correctly there.
Minimal SQL to reproduce error:
Expected outcome:
The add foreign key works as expected.
Actual outcome:
Error message:
If you need more info please let me know.
Thanks
Chris
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1247
#1247 Select for varchar with new line does not work [2.2.x]
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Example col in DB with following content:
attempt to query like this:
NOTE: on hsqldb family 1.8 such selects worked perfeclty - on 2.2.x they are not working
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1248
#1248 The precision of BLOB column types is ignored
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Trying to save a BLOB value longer than the maximum length allowed for a column will always succeed, effectively ignoring the specified column maximum length. A check similar to what's done in {{BinaryType.castOrConvertToType}} should be used here as well.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1249
#1249 Exception when a value exceeds the precision of CLOB column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Normally, CLOB values should be truncated to the maximum allowed length (precision) of the target column. However, because org.hsqldb.Session.performLOBOperation(ResultLob) doesn't have a switch case for REQUEST_GET_TRUNCATE_LENGTH, an exception is thrown, causing the transaction to fail.
The solution would be to add a switch case for REQUEST_GET_TRUNCATE_LENGTH which would call database.lobManager.getTruncateLength(id)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1250
#1250 Cannot create a VARCHAR type
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After I've created a type with:
in an instance of hsqldb server, I shutdown that instance with SHUTDOWN command. When I try to restart I got the following exception:
The guilty line is this:
If I modify the line removing the information_ schema part or removing the collate part, the script seem to work, bringing up instance
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1251
#1251 NullPointerException on subquery with group by
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Given the following definition:
I am using the latest version (2.2.9) as an in-process, in-memory database.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1252
#1252 Unquoted identifiers handled differently from PostgreSQL
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am using HSQLDB in PostgreSQL compatibility mode and it appears that unquoted identifiers are handled differently between PostgreSQL and HSQLDB.
In PostgreSQL unquoted identifiers are converted to lower case whereas in HSQLDB unquoted identifiers are converted to upper case.
This seems like it should be a relatively simple configuration option to include and would help improve compatibility between HSQLDB and PostgreSQL.
I am currently using version 2.2.8.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1253
#1253 Defrag does not trigger properly at checkpoint
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I repeat the process of adding a million rows to a table, then truncating the table, the data file continues to increase in size when I have hsqldb.defrag_limit=5. My output is the following:
After the first run, the size of the data file is 67108kb, which should be the max size needed at any time because I am only adding a million rows and deleting them. After the second run, the size of the data file is 134217kb - twice the size of my data, which means 50% of the db file is wasted, so a defrag should be triggered at the next checkpoint. However as you can see, the defrag is not triggered.
When I reduce hsqldb.defrag_limit to 1, the defrag is triggered and the file size stays at 67108kb. But it should also trigger when the limit is 5.
Test code is attached. This was run against hsqldb 2.2.5.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1254
#1254 wrong shutdown from sqltool
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The bug seems to be appeared at 2.2.9 release. It reports an error, when shutting down non-interactively from sqltool, i.e.
Enter password for SA:
Disconnected from JDBC Data Source
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1255
#1255 cannot attach text tables (CSV files) with absolute path
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi there
I reported bug ID 3535299 against 2.2.6 on June 14. You reported the bug as fixed in 2.2.9 - thanks for paying attention to the bug report! However, I just upgraded to 2.2.9 and I still get the same error as I reported in 3535299. (I just copied the stack trace from bug report 3535299):
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1256
#1256 Cannot store strings with leading blanks in TEXT tables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This bug report applies to 2.2.9 and probably to earlier versions too.
HSQLDB cannot store strings with leading or trailing blanks in TEXT tables. I do the following:
When I look at the csv file, it contains one record:
I conclude that HSQLDB can read Strings with leading and trailing blanks correctly from the csv file when they are quoted. Unfortunately HSQLDB does not quote strings when they contain leading or trailing blanks.
I suggest the following bugfix
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1257
#1257 ORDER BY NULL Logic Not Consistent
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We use HSQLDB for testing to mimic Oracle.. Just recently I upgraded HSQLDB and was surprised that our test cases were failing when checking if NULL was first or last.
This is when I first learned about NULL FIRST/ NULL LAST. Unfortunately we use Hibernate which doesn't support those options.
So then I wanted to verify the behavior for Oracle and I found this:
In HSQLDB, by default it's either always first or always last. There isn't logic to have a best guess. I would recommend that by default it uses the "best guess" in order to prevent surprises on upgraders.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1258
#1258 DbBackup overwrite not working
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When DbBackup attempts to overwrite an existing file, it does not delete the file before renaming the -partial file to the desired file name.
As a result, the rename fails and the -partial file persists.
When another attempt to backup the database to the same file is performed, an IOException is thrown when the -parital file is found and the exception incorrectly suggests that something or someone may be writing to the file.
The resolution to this is simple.
If overwrite is specified, delete the pre-existing file prior to renaming the -partial file.
If the delete fails or if the rename fails, an exception should be thrown.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1259
#1259 Using Schemas doesn't seem to work with Connection Pools
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
We've been using HSQLDB on the XWiki (http://xwiki.org) open source project for years now and it's great. Thank you for such a wonderful DB :)
Now XWiki supports having one DB per wiki in a farm mode and I'm adding support for HSQLDB (this mode works with some other DBs already).
My problem is that even though I set the schema correctly (SET SCHEMA "myschema") the statement is executed on the wrong schema. I've been debugging this and I think I've found the problem:
However the problem is in recompileStatement(). It creates a new statement but uses the old schema in the cs as the session schema instead of using the current schema:
I don't understand why the "session.setSchema(schema.name);" line is needed and this line seems to be causing my issue...
So I see several solutions to my problem:
WDYT?
Is this something that could be fixed in HSQLDB?
Thanks a lot
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1260
#1260 Inserting blob in a db with "res" connection string NPE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When calling an insert statement with a blob parameter: this exception is thrown (the same query is successful when connecting to the sam database using "file" url):
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1261
#1261 Type Coercion Needed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When running a query that coalesces a bit field, you should be able to supply a default value as follows:
SELECT COALESCE(aTable.someBitField, 1) FROM ...
In the above example, if 'aTable.someBitField' is NULL, 1 should be returned in the result set. Currently, an exception is thrown with the message 'incompatible data types in combination'. In a similar vein, the following query should return the same result set:
SELECT COALESCE(aTable.someBitField, TRUE) FROM ...
Both queries ran and failed against HSQLDB v2.2.9, running in memory with the following connection string:
jdbc:hsqldb:mem:test-DB;sql.syntax_mys=true
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1262
#1262 Querying CHAR fields using MySql Syntax
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If a table is created with of field of type CHAR of a defined length as follows:
create table myTable (id char(36) not null, version integer, creationDate timestamp not null)
And the CHAR field is defined in a simple SELECT query as follows:
SELECT id FROM myTable
Only the first character of the field value is returned. In essence, the field is treated as a CHAR, not a CHAR(36).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1263
#1263 Versions 2.0.0 and higher fail on Sonar
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When running Sonar analyzer with Maven (mvn sonar:sonar), JUnit tests using HSQLDB fail with segmentation fault.
Regular Maven build (mvn clean install) pass without problems.
My guess would be that Sonar's byte code manipulations somehow interfere with HSQLDB.
When using HSQLDB version 1.8.0.10 the build passes both with Sonar and in regular mode.
Please see dump log attached.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1264
#1264 NullPointerException on shutdownWithCatalog
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1265
#1265 WHERE condition on ROW_NUMBER does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Where conditions on the generated row number generated by ROW_NUMBER() OVER() does not work for lower bounds > 0.
Assuming 5 records matching the SELECT criteria...
SELECT * FROM
(
SELECT ROW_NUMBER() OVER() as row_num, inner.* FROM
(
SELECT ...
) AS inner
) AS foo
WHERE row_num > 0 and row_num <= 5;
returns 5 records. 5 records were expected.
Changing the WHERE condition to
WHERE row_num > 2 and row_num <= 4
returns 0 records. 2 records (row_number 3 and 4) were expected.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1266
#1266 UNION in ROW_NUMBER sub-select does not eliminate duplicates
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Assuming 5 unique rows match the SELECT criteria of a ROW_NUMBER sub-select:
SELECT * FROM
(
SELECT ROW_NUMBER() OVER() AS row_num, inner.* FROM
(
SELECT ...
UNION
SELECT ...
) AS inner
) AS outer;
The expected output is 5 rows. In the attached example I get 10, every row duplicated once.
OS is Ubuntu 11.04
JDK is Oracle jdk1.6.0_31
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1267
#1267 SELECT with a subquery in WHERE sometimes returns nothing
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Having a non-empty table Q, with ID as the primary column, running this query will return no rows:
SELECT * FROM Q o WHERE ID = (SELECT MAX(ID) FROM Q i WHERE o.ID=i.ID)
If ID is not a primary column the query would work. Replacing "WHERE ID = " with "WHERE ID IN " will also work.
The bug is reproducible with 2.2.9 and with the latest snapshot. It is not reproducible with 2.2.8, so it seems to be a regression.
Unit test attached.
The query is actually of the type which is frequently generated by Hibernate Envers, so any project using Envers with HSQLDB 2.2.9 might have some problems doing so.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1268
#1268 Error while performing connection to previous version of DB
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello, Fred!
I remember about your mail but had no time to perform database tests.
Today I've checked out latest revision of source tree (5106) and tried to connect to database from 5106 build.
So I'm very disappointed about trying to test engine because I get such error:
This I get when I connect via Swing interface:
The database dump is in attachement.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1269
#1269 "incompatible data types in combination" with TRUNC function
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HSQLDB version 2.2.9
JAVA version 1.6.0_30
Executing prepared statement including TRUNC function with timestamp parameter, HsqlException with "incompatible data types in combination" message is thown.
For example, this prepared statement doesn't work: SELECT COUNT(ID) AS TOTAL_ROWS FROM TABLE1 WHERE OPERATION_DATE = TRUNC(?)
Instead this prepared statement works: SELECT COUNT(ID) AS TOTAL_ROWS FROM TABLE1 WHERE OPERATION_DATE = TRUNC(TIMESTAMP'2012-11-01 01:00:00')
In attach junit test for both cases.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1270
strange behaviour of exit / continue handler
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
A procedure loops through a table and updates each record in a begin / end block. The update sometimes fails due to unique key violation. With having an exit or an continue handler the record to be updated is DELETED! With an undo handler everything is as expected.
After my opinion deleting the record in such cases is unacceptable.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1271
#1271 SQL / PSM: aggregate function do not see variables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The problem seems to be similar to 3107413.
Find complete example in attached file, essence is:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1272
#1272 Resource bundle name causes assertion failure in gwt-dev
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Reproduction scenario: unzip the attached zip-file, and run "mvn test" in the folder which has the pom.xml file. This results in a test error: "Could not initialize class org.hsqldb.error.Error". (Alternatively, from the zip-file import the Eclipse project, with m2eclipse installed, and run the provided JUnit launch configuration.)
The error only occurs when assertions are enabled during the unit test run.
In gwt-test-utils (specifically in com.googlecode.gwt.test.internal.GwtClassLoader.GwtClassLoaderWithRewriter.GwtClassLoaderWithRewriter), a call is done to gwt-dev's com.google.gwt.dev.util.Name.BinaryName.toInternalName("org/hsqldb/resources/sql-state-messages"), which fails on an 'assert' which checks that there are no '/' characters in the name.
As far as I can see, the cause is that org.hsqldb.resources.BundleHandler.prefix, which contains '/' characters, and org.hsqldb.error.Error.errPropsName, which contains '-' characters, together result in the call java.util.ResourceBundle.getBundle("org/hsqldb/resources/sql-state-messages", ...). HOWEVER, this method specifies that this String argument must be "the base name of the resource bundle, a fully qualified class name" (see http://docs.oracle.com/javase/6/docs/api/java/util/ResourceBundle.html#getBundle%28java.lang.String,%20java.util.Locale%29).
Therefore it seems to me that both BundleHandler.prefix and Error.errPropsName must be changed, so that ResourceBundle.getBundle() receives a funlly qualified class name; the 'assert' in gwt-dev will then automatically succeed.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1273
#1273 clob AsciiStream read returns arg length everytime
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
in Clob type
method read of InputStream returned by getAsciiStream evrytime returns this arguments array length
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1274
null pointer exception while executing stored procedure
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
the next example gives a null pointer exception (HSQLDB 2.2.8, server version):
this occurrs only when the do_update block is run inside a loop.
This error occurred inside a more complex procedure and was really hard to find. It would be much easier, if there was a print option as requested (by me) in 3584054. Is there already an internal / unofficial mechanism that helps debugging stored procedures?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1275
#1275 exception handler problems with stored procedures
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi Fred,
I use latest HSQLDB snapshot from Dec. 2nd 2012 and have several problems with exception handlers. Find attached a file with some tests. Maybe some things are only due to misunderstanding of the documentation (which is quite brief)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1276
#1276 Bad behaviour of the NULL predicate for row value expression
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The NULL predicate is not implemented correctly for row value expressions. Both the HSQLDB documentation and the SQL 1992 standard specify the same behaviour as specified in 8.6 of the SQL standard documentation paper:
However, HSQLDB 2.2.9 returns a value for this query here:
select 1
from information_schema.system_users
where (null, null) is not null
... whereas it doesn't return a value for this query here:
select 1
from information_schema.system_users
where (null, null) is null
It seems that IS NOT NULL always returns TRUE, whereas IS NULL always returns FALSE for row value expressions with degree > 1
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1277
#1277 Stored Procedures Can't Have NUMERIC Parameters
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If I create a function with a BigDecimal argument and declare it as a stored procedure with a NUMERIC parameter then HSQLDB can't find the function - but if I declare the same static function as having a DECIMAL parameter then it works. I think the patch below fixes this:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1278
#1278 upper() or lower() function limits
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When using multiple upper() or lower() functions for comparisons, receive a "java.sql.SQLException: java.io.IOException: Access is denied" error.
For instance, the following works:
But this will throw the error:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1279
#1279 MERGE with constant in <search condition> fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Given a simple table like this:
And trying to run this MERGE statement:
I receive this error: "user lacks privilege or object not found: SYSTEM_SUBQUERY".
Exactly same query used to work with HSQLDB 2.2.8 and before, but is not working both on 2.2.9 and latest 2.3.0 snapshot.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1280
#1280 Wrong result for left outer join with subselect
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We have a problem with a select statement containing a subselect for a left outer join (see attachment)
It worked fine in version 2.0.1 (previous version we used). Since the upgrade to 2.2.9, this statement returns wrong results.
I guess this is an error in the engine? Im not sure though in which category to put this issue.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1281
#1281 No view results even though query has results
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have just upgraded from version 2.0.8 to 2.2.9, and I am having a problem with certain views not returning results. When I run the actual query that the view is defined as, it returns results, but as a view it does not.
Please open the attached DB - there will be an error at startup about a missing Java stored procedure but this is not relevant.
If you run the query:
then it returns 18 rows.
if you now do
and
then 0 rows are returned.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1282
#1282 No view results even though query has results
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have just upgraded from version 2.0.8 to 2.2.9, and I am having a problem with certain views not returning results. When I run the actual query that the view is defined as, it returns results, but as a view it does not.
Please open the attached DB - there will be an error at startup about a missing Java stored procedure but this is not relevant.
If you run the query:
then it returns 18 rows.
if you now do
and
then 0 rows are returned.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1283
#1283 UUID() built-in
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
It seems that the UUID() built-in function, called without parameter generates an UUID built with only 8 bytes which are really random.
For example, here is an UUID generated with the function UUID() : 3ddf4f9e3ddf4f9e1bac4a331bac4a33
If we look closer we can see that the first 8 bytes are repeated and the third 8 bytes are repeated.
The same UUID printed differently :
Would'nt it be better if the 16 bytes were randomly generated ?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1284
#1284 Natural Join returns duplicate columns.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
As recommended in a previous issue I tried the latest snapshot jar. With the attached DB please try this query:
The result has two columns called 'TEMPLATE_GROUP_ID', so I am unable to make a view out of this query.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1285
#1285 More natural join problems
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
On the latest snapshot (26/2), please open the attached DB.
select top 1 * from ((SELECT USER_NAME, LIST_NUMBER, LIST_LEVEL, APPEND(TEXT) AS TEXT
FROM USER_OUTLINE_LIST_LEVEL_TEXT_AND_NUMBER GROUP BY USER_NAME, LIST_NUMBER, LIST_LEVEL)
NATURAL JOIN USER_OUTLINE_LIST_LEVEL);
gives several duplicate columns in the result.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1286
#1286 JDBC ResultSet deleteRow problem
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When deleting rows on a JDBC ResultSet any following rs.next() on that result set returns false.
This is contrary to any other JDBC implementations (uploaded test code works on both MySQL and ORACLE). Currently, the ResultSet javadoc for HSQLDB does not state this difference from the JDBC standard explicitly.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1287
#1287 expression not in aggregate or GROUP BY columns
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
I have defined 2 tables:
This problem appears only on version 2.2.9, on 2.2.8 it works fine. Also, when I run similar query on mysql it also works correctly.
Maybe this problem is somehow related to changes concerning bug with id 3534936.
Regards,
Adam
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1288
#1288 HTML export not XHTML compliant for sqltool
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Perhaps as a future enhancement, it would be nice to have the tags used for this export/report externalized so I could pick my own tags for rows and columns.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1289
#1289 Bug in sub-select in aggregated query
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
We migrated from version 1.8 to 2.2.9 and experienced bug described below.
Look at the query below and notice NEWRATE sub-select query. This sub-query always returns no data and therefore I get null
as a result of this column. I checked this in latest snapshot and (as of 18-03-2013) and it doesn't work there either.
If I remove SUM(...) and Group by query works fine.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1290
#1290 Bug in Transaction manager causing NPE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I discovered bug in newest snapshot (as of 18.03.2013 revision 5210 of trunk. We experience NPE in some cases of LOB usage.
NPE occurs when limit of 32 (or N * 32) is reached for size of rowAction.elementData.
Scenario is given in execution flow and solution patch (tested and works) is attached.
Please apply patch given or fix this bug in some different way in next release.
If we have lob column and we reach limit in element data following scenario occurs.
Therefore we need to copy array instead of making reference to it.
Regards,
Ognjen Milic
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1291
#1291 TRUNC function causing java.lang.IllegalArgumentException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello. I'm quite new to databases, so forgive me if I use the wrong words.
I've a simple table with a column TIME_STAMP and a column COUNTER. TIME_STAMP contains a timestamp expressed in milliseconds.
I need to group data by week and I use the following query:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1292
#1292 data exception: string data, right truncation in CASE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi!
I have two string columns with some 'compressed' data in following format: <tagNum>=<value><delimiter><tagNum>=<value><delimiter><tagNum>=<value><delimiter>...
Use-case is two extract some value by tag number that may by in any of two columns.
Below is an example where time is retrieved from date value.
Here delimiter is char 0x1, not displayed in browser unfortunately. I've attached the same in file just in case.
I'd not be wondered if such exception occurred when columns of this size (Integer.MAX) are concatenated, even though size of their content is small.
But concatenation is executed successfully.
All these expressions are executed successfully individually, but not in this CASE together.
Other two weird things I noticed playing:
1. If in WHERE right argument in comparison is 0x1, then query is executed successfully
2. If ELSE returns any explicit value instead of SUBSTRING function, query would is executed. I repeat, this SUBSTRING itself is executable successfully.
And two related questions with our permission:
1. Is it planned two support kind of VARCHAR(MAX) format to avoid explicit huge sizes defined?
2. Is it planned to implement POSITION in CLOB?
Thanks in advance!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1293
#1293 NullPointerException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Example below is pretty senseless, but I cut off all insignificant data and expressions to ease debugging.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1295
#1295 TIMESTAMP and AT LOCAL behavior
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm puzzled by the results I get when using TIMESTAMP and AT LOCAL when it comes to time-zone/daylight saving conversions.
I'm not sure if this is a bug or not. Maybe I'm just doing the wrong assumptions. In case please help me to better understand what is wrong.
In one of the columns of the table I have to query there's a number representing a timestamp. For example: 1228086000
I'm located in Italy, current timezone is GMT+2 (due to daylight saving, otherwise it would be GMT+1)
According to website http that timestamp corresponds to:
GMT: Sun, 30 Nov 2008 23:00:00 GMT
My time zone: 01 dec 2008 00:00:00 CET GMT+1
It is important to notice that the local time is GMT+1, because that date belongs to the range in which no daylight saving is applied.
Now I tested the following statements with the corresponding results (-->):
When "AT LOCAL" is not used, I get the same result for A,B,C.
I would expect A1,B1,C1 to be different from A,B,C and all look the same, but it is not the case.
As it regards A1 and C1 it seems that "AT LOCAL" has no effect.
B1 is different, but still it is wrong. In fact I would expect B1 to return 2008-12-01 00:00:00.0, because on 1st december there's no daylight saving (GMT+1).
Instead it seems that the timestamp is converted to the current time zone (GMT+2).
Am I doing something wrong? If yes, what should I do to go from that number (1228086000) to the correct local date-time (2008-12-01 00:00:00.0) ?
Thank you.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1296
#1296 Jdbc Batch Selects doesn't work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When executing a batch of select queries, HSQLDB 2.2.9 reports an error:
The problem can be reproduced with the Junit Test in the attachment, or with the
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1297
#1297 session leaks on initialize
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm using hsqldb in combination with hibernate (3.6.8)
I tried to upgrade from hsqldb 1.8 to 2.2.9 and ran into the problem that I could not change the transaction isolation level initially. This did work after I ran my first transaction. When digging a bit I found out that it seems that when reading the metadatadefaults somehow a stale transaction exists which prevents the change of isolation level. Also when checking the getAllSessions().length it still has a session open.
When I put the property "hibernate.temp.use_jdbc_metadata_defaults" to false I do not encounter this issue. The change of isolation level works from the beginning and no session is open.
I made a simple test to expose this problem. See code below:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1298
#1298 NPE in StringConverter.byteArrayToSQLHexString
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
On version 2.2.9 with sqllog set to 2, HSQL tries to log an insert statement on a table with a BLOB column. For the logging, it tries to convert the BLOB to a hex string. At the time of logging, the field value is of type BlobDataID. As the BlobType.convertToSQLString calls BlobDataID.getBytes() which returns null, the following call to StringConverter.byteArrayToSQLHexString fails with a NPE.
Table format as created by HSQLDB: create table SomeTable (id bigint not null, version varchar(32) not null, data BLOB not null, primary key (id));
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1299
#1299 LEFT JOIN (SELECT...)  <-> LEFT JOIN .... non-equivalence
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Running the following script:
This is extremely non-intuitive, to say the least.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1300
#1300 subselect and indexes
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We have a problem with subselects on same table. Table has indexes.
It works fine in 2.2.8 but in 2.2.9 rusult is a empty list.
Our initial analysis: For subselect is another plan used. In 2.2.8 'full table scan', in 2.2.9 'indexes' used
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1301
#1301 Create view based  on selection from another view
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Failed to create view which based on selection from another already created view.
If it is based on selection from existent table - there is no any problem, it works well.
Environment:
- HSQLDB 2.2.9
- PostgreSQL compatibility mode enabled
- java 1.6
Script:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1303
Get a Concurrent exception while close connection
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1304
#1304 Error if JRE1.7 is installed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Doesn't matter which JRE you use to compile the code below, if you run it under JRE1.7 you will get the error.
The code:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1305
#1305 Implementation of java.sql.Statement is not conformant
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Your implementation of the interface java.sql.Statement is not conformant to the contract.
The JavaDoc reads:
[quote]
By default, only one ResultSet object per Statement object can be open at the same time. Therefore, if the reading of one ResultSet object is interleaved with the reading of another, each must have been generated by different Statement objects. All execution methods in the Statement interface implicitly close a statment's current ResultSet object if an open one exists
[/quote]
But with HSQLDB you can actually use two ResultSets from the same Statement concurrently without a problem. This almost made me report a bug against the MySQL Connector because there it didn't work, I tried with HSQLDB and it worked and thus thought it is a bug in the MySQL Connector until I found that piece of documentation which actually explains what the problem with my code was and that the MySQL Connector behaves exactly as expected.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1306
#1306 Incompatible data type in conversion with EXTRACT function
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We are getting an error "incompatible data type in conversion" when doing a setTimestamp on a PreparedStatement where the parameter value will be inside an Extract function call. Here is an example of the query and attached are a code sample to reproduce easily the problem:
PreparedStatement stmt = conn.prepareStatement("select EXTRACT(HOUR FROM DATE_TS - ?) from MY_DATES");
Timestamp ts = new Timestamp(new Date().getTime());
stmt.setTimestamp(1, ts);
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1307
SSL Server Doesn't Work On IBM JVM Implementation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Configuring HSQLDB Server to use SSL results in a java.lang.NoClassDefFoundError: sun.security.ec.ECParameters when running with IBM JVM 1.7.0 on AIX 6.1. I verified the missing class is not available in the IBM implementation. I am able to configure an SSL server using a Sun/Oracle JVM on other operating systems.
I think the problem lies in the constructor for HsqlSocketFactorySecure. It looks like it is adding the SunJSSE security provider if it is not available. When I comment out the block of code, I am able to start an SSL server with what I assume is the IBM JSSE implementation.
You should consider removing the code forcing the SunJSSE provider to be available and/or allow it to use other JVM implementation security providers.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1308
#1308 3609454 reproduced on 2.3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm still getting NPE on 2.3 in this case https.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1309
#1309 Datatype Not Resolved for Parameterized Escape Character
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When specifying the escape char following a LIKE condition as a parameter, the data type is never resolved and results in exception "data type cast needed for parameter or null literal" when preparing the statement. This problem surfaced using HSQLDB 2.3 within a Spring-Data/Hibernate environment using JDK 1.7.
Code:
Fix:
From what I can tell, it appears that data types are resolved from their associated table columns. In this case, no table column applies to an escape character, so the data type is left null, thus failing the condition in StatementDMQL, line 559. I am able to build a patch with the following addition to ParserDQL following line 4083 that resolves the problem:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1311
#1311 JDBCPool not supported in JDBCDataSourceFactory
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
From the documentation of the factory method JDBCDataSourceFactory#getObjectInstance(Object, Name, Context, Hashtable), it seems that the JDBCPool is one of the four supported data source class names:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1312
#1312 Count Function not working as expected
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Team,
I am facing an issue with the following query.
It is throwing following exception."Caused by: org.hsqldb.HsqlException: Column not found: COUNT"
But when I visited the HSQL Documentation, it was mentioned that count was supported. Currently using HSQLDB 2.3.0
Please help me to sort this out.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1313
#1313 SET MESSAGE_TEXT cannot take expression
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In a PSM I am trying to set the message text for exceptions. This works when I use a simple string constant, e.g.
However if I want to elaborate this with specific information on the error using either a local variable or string concatentation, it doesn't work, e.g.
There is a similar error when using local variables in the PSM.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1316
#1316 Wrong doc Chapter 8. SQL-Invoked Routines (JAVA static SP)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The SQL signature composed from 2 'string' parameters, while the JAVA signature composed of 3.
This will lead to 'unresolved method' SQL exception if one will try this code as a template for its SP
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1317
#1317 NullPointerException in getColumnNames if no columns defined
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
V.2.3.0:
If you create a table without any columns, then simple "select * from <table-name>" would throw Null Pointer Exception.
It was working in V.2.2.5
The stack trace:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1318
#1318 HSQL JDBC driver crashes when calling JAVA SP with Array arg
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Issue observed under 2.2.9 & 2.3.0
0. Compile the following class and add it to the classpath of the DB (it contains JAVA stored procedure implementation)
1. Create simple DB using the following script
create PROCEDURE get_data(IN ids VARCHAR(128) ARRAY, IN query LONGVARCHAR) MODIFIES SQL DATA LANGUAGE JAVA DYNAMIC RESULT SETS 1 EXTERNAL NAME 'CLASSPATH:Foo.getData';
2. Call the get_data SP
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1319
#1319 HSQL JDBC driver crashes when calling JAVA SP with Array arg
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Issue observed under 2.2.9 & 2.3.0
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1320
#1320 JDBCPreparedStatement.setParameter may throw HsqlException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The JDBCPreparedStatement.setParameter() method may throw HsqlException for columns of type VARCHAR if the parameter value is not convertible according to the definition of CharacterType.convertToDefaultType().
It appears the intention of JDBCPreparedStatement.setParameter() is to catch any instances of HsqlException that may be thrown and re-throw them as
Performing a similar call on a non-VARCHAR column results in an SQLException being thrown, which I believe is the expected behavior.
The attached patch (against 2.3.0) should fix the problem, although I haven't tested it.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1321
#1321 No Result for SELECT with Comparison > and DESC order
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This select statement will produce no result set, which is wrong:
The data is attached as csv-file.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1322
#1322 Server#handleConnection still called after #shutdown
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After shutting down an instance of org.hsqldb.server.Server and creating a new one with the same connection settings, sometimes the old instance still tries to handle jdbc calls (and of course fails to do so). The problems does not occur always but only in rare case and on some of our test machines.
Version is 2.2.9
I'd guess the cause of the problem is that the change of field socket to null (via method #releaseServerSocket) is not visible in the thread that executes the #run method and the while loop condition "while (socket != null)" remains true. This is because there's no synchronization when accessing the value of field socket and field socket is not marked as volatile. I'd think the field should be made volatile.
Log output (adapted to output System.identityHashCode of the server instance as well) attached.
A new instance 2008761310 gets started immediately afterwards (at 01:22:04,162) on the same thread (main).
When used via JDBC (again from same thread 'main') the #handleConnection method of the old instance 428822417 is called - and of course leads to an error because the database has been closed (database alias=testpersistence_alias does not exist)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1323
#1323 Quoted CSV Import Merges Fields
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
There is an off by one error in SqlFile.preprocessCsvQuoting that causes it to lose commas and merge fields under certain circumstances. The attached SQL and CSV files demonstrate the issue, and the Java file fixes it.
Specifically, if two quoted fields are separated by a comma with no surrounding whitespace, that comma disappears. That situation results in a segment of length 1 containing just the comma, but the condition tests for "segLen > 1" instead of "segLen >= 1".
Example:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1324
#1324 call database_version() returns wrong version
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
call database_version() still returns "2.3.0" - it should return "2.3.1"
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1325
#1325 driver hang after OutOfMeoryError
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It look like that the hsql need to copy all data of a row in the memory multiple times.
I have 26829272 bytes free before executeUpdate(). I want save a InputStream with 9170069. I think 26 MB should enough memory to save a single row with 9 MB,
But the fatal error is that it hang after the OutOfMemoryError occur. executeUpdate() never return. See the both stacktraces for details
Blocking Stacktrace:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1326
#1326 Can`t use sysdate as default column value
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I tried to create table:
It was created successfully, but then, after reconnect to database was throwed exception:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1327
#1327 Cannot use table dual in stored procedure
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I create a database like this:
In the resulting script, the procedure is created as follows:
CREATE PROCEDURE PUBLIC.TEST_PROCEDURE(IN P_NAME VARCHAR(50)) SPECIFIC TEST_PROCEDURE_10096 LANGUAGE SQL NOT DETERMINISTIC MODIFIES SQL DATA NEW SAVEPOINT LEVEL BEGIN ATOMIC INSERT INTO PUBLIC.PROCEDURE_TEST(ID,NAME)SELECT TEST_ID_SEQ.NEXTVAL,P_NAME FROM SYSTEM_SCHEMA.DUAL;END
Trying to connect to the database complaints about invalid schema SYSTEM_SCHEMA:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1328
#1328 Oracle - Limiting rows with "order-by" and "where ROWNUM"
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This is with regard to oracle compatibility. Limiting the number of rows with "order by" and "where ROWNUM" is returning wrong result set.
This is on Ubuntu 12.04 with Oracle JDK 1.7. Attached is the sql file with the DDL and DML statements required to create the test table.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1329
#1329 Rollback issue v2.3.0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi, have some strange behaviour after upgrading to v2.3.0.
When I insert a new record, then rollback and try to select it, it is gone as expected. But after shutdown and restart it is inserted into the table.
This happens with MVCC and cached tables. Please see the attached example code.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1330
#1330 Process hangs at 100% CPU usage in server mode
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
We are using HSQLDB as the primary database in server mode for an open source project (a RSS feed reader, here https).
Sometimes, when no heavy task is performed on the database, the hsqldb-server process hangs and it takes 100% CPU. I've attached a jstack if it can helps you.
The database is quite big, around 500Mo. You can find the structure here :
Thank you for this great project :)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1331
#1331 trigger execution problem after dropping column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
i found this problem when trying to test my DAO implementation.
Example
We have a create table statement in the first script:
In the second script we create trigger to auto set modification time column:
In the last script we have a drop column statement:
Ita apears, that droping a column doesn't infulence on trigger definition (trigger sees 5 columns, insert statement sees 4 columns).
After changing order of the second and third script it works just fine.
Here's java stack:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1332
#1332 DATEADD does not work as expected (documented)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1333
#1333 ArrayIndexOutOfBoundsException [HSQLDB 2.3.1]
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Queries:
Details:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1334
#1334 Use NULL as DEFAULT for INSERTs
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Assume the following table:
When I use the following syntax:
INSERT INTO t_identity DEFAULT VALUES;
I usually get the following error message:
DEFAULT keyword cannot be used as column has no DEFAULT
The same happens when I use the DEFAULT keyword for single columns that do not declare a DEFAULT in their DDL. E.g
Pretty much all other databases use NULL as a DEFAULT, when no explicit DEFAULT is defined. I'd say that this is correct according to the SQL-92 standard:
2) The default value of a column is
Case:
a) If the column descriptor of a column includes a default value
derived from a <default option>, then the value of that <de-
fault option>.
b) If the column descriptor includes a domain name that iden-
tifies a domain descriptor that includes a default value
derived from a <default option>, then the value of that <de-
fault option>.
c) Otherwise, the null value.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1335
#1335 DBMS ignores redefining of already defined alias.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Sample:
We can expected errors in above query, but the db engine successfully fulfills the request.
hsqldb.jar version 2.3.1
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1336
#1336 Excessive memory use with "Delete from table" & "Truncate table"
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have recently upgraded hsql db from 1.8 to 2.3.1 and started having memory heap space issue.
Before the upgrade max 500 mb memory needed and there was enough heap space available during the entire test.
After the upgrade 6GB space was not enough so I tried it with every version from 2.0 to 2.3.1 and I had the same problem.
Now I am dropping tables and recreating the database fresh from scratch and the issue disappeared.
Using "Delete from table" also "Truncate table" cause the memory issue hsql cannot clear the memory. I can provide more info if needed.
Tested with
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1337
#1337 "Duplicate column name in derived table" for non-derived, top-level tables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following query raises a "duplicate column name in derived table" error:
select table_name, * from information_schema.tables
But there is no derived table, and pretty much every database I know of allows for such a query. For example, PostgreSQL:
This query will produce the same error:
select table_name AS x, * from information_schema.tables
These queries, however, are a workaround for the problem:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1338
#1338 write_delay can lead to data-loss
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I believe I have traced a problem in HSQLDB v2.3.1 which is arguably
either poor documentation or a bug -- either way, I only discovered
what was happening by looking at the source-code.
While using v1.2.8 of HikariCP with HSQLDB, I noticed my application
(which makes changes to a database and exits quickly) was suffering
from data-loss: information added as committed transactions were
missing on subsequent runs.
The problem seems to be a combination of two things: v1.2.8 of
HikariCP has a 'shutdown' that does nothing and that HSQLDB, by
default, relies on an orderly shutdown to guarantee data integrity.
I should point out that newer versions of HikariCP shutdown method
close all open connections, which (along with 'shutdown=true' in the
jdbc connection string) eliminates the data-loss problem.
The HSQLDB documentation for WRITE DELAY states:
If the property is true, the default WRITE DELAY property of the
database is used, which is 500 milliseconds. If the property is
false, the WRITE DELAY is set to 0 seconds. The log is written to
file regardless of this property. The property controls the fsync
that forces the written log to be persisted to disk. The SQL
command for this property allows more precise control over the
property.

This strongly suggests that data is written into the OS' file-system
cache (VFS-layer, for Linux) after a COMMIT, and write_delay controls
the periodic calls to fsync. Calling fsync is meant to guard against
certain hardware failures (e.g., power-cut), if the data has been
written to the VFS layer then it will be written to disk after the
application closes (with extremely high likelihood, if not actually
guaranteed), so committed transactions are safe from software
failures.
In practise, org.hsqldb.scriptio.ScriptWriterBase shows that output is
written through a BufferedOutputStream. The forceSync method first
flushes this buffer prior to calling fsync(). This behaviour is not
at all described above.
There are two issues here:


It seems that HSQLDB requires applications to close any open
Connection (and, perhaps shutdown the database) before
guaranteeing transactions have been processed. This doesn't
appear to be mentioned in HSQLDB documentation and IMHO isn't part
of the JDBC spec.


The documentation for WRITE DELAY is misleading: it suggests that
it is limited to delaying calls to fsync and neglects to mention
that there is an internal buffer that is also flushed.


I cannot say whether the performance benefits outweighs the risks
associated with buffering the output in Java; I suspect this is a
decision that can only be made on an application-by-application bases.
If I may, I would suggest two actions:
a. update the WRITE DELAY documentation to better reflect current
behaviour,
b. consider adding an option that removes the write buffer so that
HSQLDB can write data on commit but continue to support delayed
calling fsync.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1339
#1339 Silent corruption
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am running a simple HSqlDB (v. 2.3.1) application on top of ext3 file system (write-back mode). My application just inserts 100 rows into a memory table. I see that HSqlDB writes the data to a log file before checkpointing the data to the actual script file. In ext3 write back mode, a system crash while writing to the log can result in a state where some part of the log file contains garbage data. In some cases, when the database is reopened from such a crashed state, HSqlDb returns garbage data to the application without detecting the corruption. Please note that this does not happen always - i.e., from most crashed states, HSqlDb can realize a corruption and recover to the old state of the database.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1341
#1341 Regression in 2.3.2: Trigger introduces false object dependencies
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following leads to an error:
CREATE TABLE t1 (a bigint, b bigint);
CREATE TABLE t2 (c bigint);
CREATE TRIGGER t AFTER INSERT ON t2 REFERENCING NEW AS new FOR EACH ROW UPDATE t1 SET b=b+1;
ALTER TABLE t1 ALTER COLUMN a RENAME TO d;
I would expect the rename to succeed since the trigger does not refer to the column being renamed and is still valid afterwards. Yet HSQLDB 2.3.2 fails with:
SEVERE SQL Error at '/tmp/sql' line 4:
"ALTER TABLE t1 ALTER COLUMN a RENAME TO d"
dependent objects exist: PUBLIC.T in statement [ALTER TABLE t1 ALTER COLUMN a RENAME TO d]
SEVERE Rolling back SQL transaction.
org.hsqldb.cmdline.SqlTool$SqlToolException
The problem does not exist in version 2.3.1.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1342
#1342 unique index behavior change in 2.3.2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
creates unexpected "integrity constraint violation: unique constraint or index violation"
the behavior was different in hsqldb version 2.3.1. (was working fine)
As well in PostgreSql and MySQL
It is not exactly specified here http://hsqldb.org/doc/guide/ch02.html
"Example 2.1. Column values which satisfy a 2-column UNIQUE constraint"
But I think this breaks the SQL standards
Test:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1343
#1343 Recursive query regression in 2.3.2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The example from the docs for recursive queries (with a slight modification) causes an infinite loop in 2.3.2. The same query works fine in 2.3.1. The key difference from the documentation is using "UNION ALL" instead of just "UNION".
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1344
#1344 NullPointerException org.hsqldb.index.IndexAVL.delete
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We have seen a null pointer in the IndexAVL.delete method which occurs infrequently but with different types of SQL (always update or delete SQL). We cannot replicate it. At the time of running the SQL there may be multiple threads performing reads or writes on the DB. We also have some large varchar fields, in this case one that was abount 380,000 characters long.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1345
#1345 GROUP BY does not support parameters
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Then you can do:
What does not work with HSQLDB that however does work with for example MySQL is using a prepared statement like
where both parameters use the same value.
That will produce org.hsqldb.HsqlException "expression not in aggregate or GROUP BY columns"
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1347
#1347 HSQLDB server mode limited to 2GB ResultSets
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When working with large data sets I noticed that if the ResultSet returned was over a certain amount of rows the server would go into an infinite loop and never return the ResultSet. Upon further investigation the problem was tracked down to the HsqlByteArrayOutputStream class. In server mode HSQLDB creates the ResultSet and passes it over the network as a large byte array. Unfortunately the code in HsqlByteArrayOutputStream uses System.arraycopy() to resize arrays which is limited to using int for the length of the array. The code that ends up getting into an infinite loop is:
In the while loop above newsize overflows and eventually went to 0 causing an infinite loop in the while loop.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1350
#1350 Servlet mode with WEB-INF=true does only work with Tomcat prior to Version 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi!
Using hsqldb-svn trunk.
I'm using HSQLDB's Servlet mode in a JavaEE Webapp with Tomcat 8.0.3.0 and hsqldb.server.use_web-inf_path=true.
The problem with this is, Tomcat 8's output of getServletContext().getRealPath("/") is without a trailing slash (/), so the path where our database is saved is not blabla-1.0-SNAPSHOT/WEB-INF/database/db but instead blabla-1.0-SNAPSHOTWEB-INF/database/db ...
With any version prior to Tomcat 8 it runs flawless, but since Tomcat 8 it doesn't. Also other application servers like WebSphere do handle this like Tomcat 8, as this is a correct behavior.
It might be better to change this according to the API.
I attached a working (but probably dirty) patch.
Thanks in advance!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1351
#1351 Foreign Key constraint issue
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Foreign key constraint should not be applied if parent table don't have ref data but HSQLDB is applying the constraint and didn't throw any exception. Below is the sample script.
Thanks,
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1352
#1352 JDBC compliance SOURCE_DATA_TYPE in getColumns() result set
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In v2.3.2 the 'getColumns' metadata method documented here:
returns a string in the penultimate column SOURCE_DATA_TYPE instead of a short, as documented both in the HSQLDB spec, and the JDBC interface it inherits from. Currently it returns the literal name of the data type.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1353
#1353 HSQLDB not working with some IBM Codepages, HSQLDB is not binary compatible between different plattforms
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Because in the Class ScriptWriterText the platform (System.getProperty("line.separator") specific line separator is taken, the files are not portable and the database don't start anymore.
The real problem at the end is, that the LineReader Class only detect 0x0A and 0x0D.
On IBM Code Page 500 for example, the line separator is 0x0F.
Resolution:
Define a permanent Hex Code for the line.separator (platform independent).
I have changed the code in the ScriptWriterText Class which resolves the problem.
This version runs now on our system (Linux and IBM USS)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1354
#1354 Unique constaint does not work correctly if the nullable column is first in the list
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following example should throw an exception, but does not:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1355
#1355 SYSTEM_TEXTTABLES id_quoted difference
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In the SYSTEM_TEXTTABLES routine of DatabaseInformationFull, when sysTables[SYSTEM_TEXTTABLES] returns null, IS_ALL_QUOTED is created as column 10, IS_QUOTED as 11. If not null, textFileSettings.isQuoted is stored in row[10] and textFileSettings.isAllQuoted is stored in row[11] (through the index variables iiq and iiaq). The Javadoc comments indicate the latter is correct.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1356
#1356 Results being overridden
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In the 2-line for loop at line 270 of ArrayType.java, the result of the first line's call to convertJavaToSQL() is being overridden by the result of the second line's call to convertToTypeLimits(). Possibly the result of the first line s/b passed as a parameter to convertToTypeLimits()?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1357
#1357 exception on union
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
executing the queries below, hsql crash :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1358
#1358 Unable to use decode function in order to transform datatype
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In Oracle database, I can use the decode command to transform the return datatype. For example, I can use in my PreparedStatement decode(:myVar, 'toto', 1, myColumnInNumber). When myVar equals toto, then it returns 1 otherwise, it returns the value of myColumnInNumber.
This syntax is not allowed in hsqldb 2.2.8
I tried to convert myColumnInNumber in String using the to_char function but it doesn't work too.
The stack trace for the decode fucntion :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1359
#1359 Prepared statement with placeholders: SQLSyntaxErrorException: incompatible data types in combination
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Create a table:
and try to prepare the following statement via JDBC:
This only seems to happen when the columns in the SET clause belong to different types, here SMALLINT vs. VARCHAR. The following statement compiles without problem.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1360
#1360 Problem with JDBCPreparedStatement when using "with" clause that include a recursive subquery
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi, I am putting a DB2 application on HSQLDB, but I have a strange behavior. (I am using Hsqldb2.3.2)
When I run a statement with all the values in it (in squirrel or in a java statement), the result set is feed with all the expected values.
But
When I run it with a preparedStatement with the same values, the resultSet is empty
So, if I execute this statement with all value set in a sql client, records are returned:
with produit_adhesion as
BUT,
If I use it in a prepare statement with the same values, the result set is empty.
After debugging a lot, I found that the block in bold below is the problem. If I set it with fix values, the prepared statement will return expected values otherwise, no value will come back from the database
The cause seems to be in the pre-compilation of the prepared statement.
HSQLDB get mixed up with the subquery using the previous one (folio_non_retire_cons using value from folio_non_retire)
So, if I remove the ? in that block and put fix values, the prepared statement will work just fine :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1361
#1361 Corruption during system crash and power failure scenarios
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This issue is about what happens during a sudden power failure, if a SHUTDOWN statement is being executed. It is hard to reproduce this scenario normally, but we found it by using a tool that can reproduce such scenarios (we believe the chance of it occurring in a real world is small).
When the SHUTDOWN command is executed, HSQLDB updates the properties file finally, using an unlink(properties) call followed by a rename(properties.new, properties). Before this, HSQLDB unlinks the log file, and updates the script file using an unlink and a rename.
File systems such as btrfs can buffer directory operations, and send them to disk out-of-order. Consider that a file system buffers the unlink(log) call, and instead sends the unlink(properties) call to disk first. In this case, if a system crash happens immediately after the unlink(properties) call, trying to read the database after reboot reveals a corrupted database. Similarly, if the rename(script) call is buffered instead, trying to read the database results in an error.
A slight disclaimer: I am involved more in file-system research than in using databases, so there is a chance that I did not use HSQLDB properly. Please let me know if you suspect that is the case. Also, the exact problematic buffering (sending out-of-order) discussed above might not happen with btrfs; I am not sure. I am, however, sure that btrfs does buffer and send some directory operations out-of-order [1], and it is possible future file systems (or other current file systems I do not know about) will do the problematic buffering.
More details: HSQLDB version is 2.3.1.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1362
#1362 Class cast exception with FOR Select
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using 2.3.2 as a server engine, within a stored procedure I am receiving class cast exceptions "String cannot be cast to Number". The example below will fail when trying to execute. If you remove the two varchar's (, VP_PATTERN, VP_CODE) from the select clause of the FOR statement, the code executes.
The example is cluttered because I was trying to reproduce a problem in my development code where the chkpts table showed that in the course of entering the for loop, the variable your_counter jumped in value between chkpts yourCounter 1 and 2 by the number of records that should be returned by the select statement. Thus the VP_LOOP never executed a second time since your_counter exceeded howMany. Very bizarre. If you need this demonstrated I can provide an even more complex example. So two problems here.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1363
#1363 Modulus function does not return decimal result
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The HyperSQL User Guide says of the Modulus function:
But mod(8.5,3.00) yields '2' not '2.50'.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1364
#1364 Column names are case sensitive when returning Generated Keys
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I'm new to using HSQLDB. I've found what may or may not be a bug, depending on your point of view, but I do know that this behavior is inconsistent relative to SQLite, Oracle, and Postgres.
When you execute an insert that will return generated keys, if the String array of column names is not capitalized, HSQLDB will fail to find the columns for that table and will fail to execute the statement (and return the new primary keys).
I've tried to
By making "FolderId" upper-case, it works fine, aka:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1365
#1365 Invalid HAVING expression when having a parameter
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I saw an old bug where having a parameter in a having expression generates an invalid HAVING expression exception. It's marked as fixed but I'm having the exact same issue on 2.3.2
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1366
#1366 ArrayIndexOutOfBoundsException in UNION ALL
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The query:
It works fine if the 'ALL' option is removed from the UNION and also when the two clauses being unioned are swapped. On its own, the first clause with the GROUP BY returns an empty result set; the second clause does not.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1367
#1367 problem with materialising subquery in exists-Clause when view is involved
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HSQLDB version 2.3.2.
problem with materialising subquery in exists-Clause when view is involved.
It seems the join-column-expression is not evaluated if it is not in the from-clause of the select.
It was working with hsqldb version 1.8.0.10 and earlier :-)
table setup
-- not working (gives no result, should return the view-record)
-- working (gives expected result)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1368
#1368 The NPE raised if there's a subquery in the where cluase
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a query with a subquery in the where clause,say:
but I got the following NPE when executed:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1369
#1369 Query with filter of NaN characters failed.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I have an hsqldb table with -Inf, Inf, NaN and null. Using a select statement to filter using that value is confused. E.g. if using where sqrt(0), it returns 0 and NaN. Using where -Infinity, it returns -Inf and NaN etc. Use these samples to test:
Thanks.
-Aston
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1370
#1370 PostgreSQL Style Syntax fails on rename column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Greetings.
With HSQLDB 2.3.2, the following "postgres syntax" doesn't seem to work.
alter table eat_arch_status rename LAST_PACKAGED_DATE TO LAST_DIGITAL_PACKAGED_DATE;
FWIW
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1371
#1371 User defined function. Exception when hsql is starting
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I have the actual hsqldb running (2.3.2). I created a function via dbvisualize and tested it succesfully. After stopping hsqldb and starting it again, I got the following hsql exception.
line 247 is exactley the function definition that should be executed.
I attach the db script without data and the sql for creating the function.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1372
#1372 Can't call callable with JDBC{?=call proc()} pattern
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1373
#1373 SELECT query is not working if aggregated column have alias and that column also appear in ORDER BY clause.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
However above select query work fine if alias not give for aggregated column. e.g Select sum(id) from Sample order by sum(id).
Please check and resolve the issue ASAP.
Thanks,
Tahir Akram
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1374
#1374 add_months in combination with extract loses bracket
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm using version 2.3.2, the datasource has 'sql.syntax_ora=true'.
When I'm using the query:
However, when I make a combination:
The recompiled SQL shows a missing part ',1)':
I tried wrapping things with additional brackets but havent found a working solution yet. The query does run fine in Oracle.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1375
#1375 Time zone change the date or hour in a database
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I think the bug is in HSQL but I'm not shure.
The bug comes up from LibreOffice or OpenOffice.
(I'im sorry for my bad english)
To reproduce it...
1. create a new database in LibreOffice (or OpenOffice).
2. create a table with tree fields: DATA, TIME and TIMESTAMP.
3. put some datas in the table, save the file and quit LibreOffice
4. change the time zone of your operating system with a farway zone from yours
5. open again the table
The date or/and hour changed
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1376
#1376 manual entry for constraint incorrect
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm looking at the CONSTRAINT section of the manual (http://hsqldb.org/doc/guide/databaseobjects-chapt.html).
According to that information I should be able to do
ALTER TABLE messaging_messages ADD CONSTRAINT fk_address_person FOREIGN KEY (queue) REFERENCES messaging_queues (id) ON UPDATE RESTRICT ON DELETE CASCADE INITIALLY DEFERRED;
I'm currently getting "Error: unexpected token: INITIALLY".
I've also tried
I'm using hsqldb 2.3.1.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1377
#1377 Trigger not fired on every single row when using insert select or merge
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I defined a BEFORE INSERT trigger for a table and it works as expected for single INSERTstatements, but not for INSERT ... SELECT nor MERGE statements.
These are my database objects (simplified):
If single INSERTstatements are executed, everything works as expected, the D is fetched from the sequence. But if I execute something like
INSERT INTO employee (company_id) SELECT id FROM company;
the I get an error:
integrity constraint violation: unique constraint or index violation: "EMPLOYEE_PK"
which could propably mean that it tries to insert the same key from the sequence twice.
I'm using the latests version 2.3.2 of HSQLDB.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1378
#1378 DDL 'LIKE TABLE" does not copy NULLable property
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In the documentation (Ch 4) it says:
"All NOT NULL constraints are copied with the original columns, other constraints are not."
However, this doesn't appear to be the case. All columns appear to have the nullable attribute set. Looks like it's down to this (ParserDDL.java:readLikeTable)
The penultimate line overrides nullable indiscriminately.
Is this part of the SQL 2008 spec? Either the docs or code should be changed, I think.
I've tested this with 2.3.2, using a temporary table as the source table (though this shouldn't make a difference, I think).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1379
#1379 Queries are not escaped with `
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi there,
I'm using hibernate and some of my entities have attributes like order, language or catalog. I noticed these are reserved words so having a query like this:
will fail in HSQL but succeed in Mysql. The reason is that Hibernate will translate the query to something like this:
and the same query gets translated like this when using HSQL:
which fails. I believe the problem is in the hsql driver, so now I have to rename my datamodel and put some unintuitive attribute names, only because HSQL does not escape with.
I hope you will fix that. Thanks a lot and keep up the good work :)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1380
ScriptReaderDecode locks filehandle on exception
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
There is a problem in the ScriptReaderDecode constructor class. If the first constructor is called with a "String fileName", it creates an FileInputStream with the call to "openInputStreamElement(..) and then delegates to this(....)
In our usecase the "new GZIPInputStream(...) throws an exception upon construction (because the DB File is corrupted). HSQLDB then holds on to the filehandle xxxx.script until the finalizer of FileInputStream finalizes that inputstream and closes the filehandle. During that time its not possible to delete the corrupted database.
A solution is needed that frees up all resource if the HSQLDB cannot be started. Below is the current code of HSQLDB 2.3.2 that we use....
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1381
#1381 Incorrect results from query on temp-tables when spilled to disk
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After upgragding to 2.3.2 I see a problem where I lose results in correlated sub-queries on temp tables that are spilled to disk. Here is the scenario:
Connect and create a database : jdbc:hsqldb:file:C:\Users\DNICOD~1\AppData\Local\Temp\hsDNTst1\hsDNTst1
SET SESSION RESULT MEMORY ROWS 1398 -- This causes the failure, but 1399 and above avoid the problem, presumably by not going to disk.
I've attached a stand alone program (including data) that reproduces the problem. If invoked with an argument >=1399, then it returns the correct results
1398 causes the query to return incorrect results. I can narrow it down further or provide more info if that will help.
Thanks for your time,
Dave N
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1382
#1382 NullPointerException when unnesting an array
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following SQL statement causes a NullPointerException to be thrown:
Perhaps, the statement is invalid per se, but even if that's the case, there shouldn't be a NullPointerException. The full exception stacktrace is:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1383
#1383 Wrong calculation of MEDIAN
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HSQLDB calculates the MEDIAN() value as PERCENTILE_DISC(0.5), rather than PERCENTILE_CONT(0.5). For instance:
The above yields 2, instead of 2.5.
Consider for instance the Wikipedia article on the Median:
If there is an even number of observations, then there is no single middle value; the median is then usually defined to be the mean of the two middle values [1] [2] (the median of {3, 5, 7, 9} is (5 + 7) / 2 = 6), which corresponds to interpreting the median as the fully trimmed mid-range. The median is of central importance in robust statistics, as it is the most resistant statistic, having a breakdown point of 50%: so long as no more than half the data is contaminated, the median will not give an arbitrarily large result.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1384
#1384 MEDIAN() should ignore NULLs
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following query yields 1:
This flaw is particularly interesting when emulating the SQL standard FILTER clause (as supported by PostgreSQL), e.g.
In HSQLDB, the FILTER clause would be emulated as follows
In this emulation, again, NULLs are not ignored by HSQLDB, which leads to the MEDIAN() being wrong.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1385
#1385 RuntimeException when using bind values in FILTER clause
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I found another bug related to these aggregate functions. When using a bind variable in an aggregate function's FILTER clause:
I'm getting:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1386
#1386 sys_extract_utc not recognized in create table statements under Oracle syntax mode
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Executing the following statement produces
But selects work just fine.
Select sys_extract_utc(systimestamp) from dual;
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1387
read
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Even though my connection string has readonly, sqltool tries to expand the data file and throws following exception:
The connection string in question is:
I should probably mention that the data file is in use in "read-write" mode in a separate jvm while i'm trying to open the file using sqltool. Any idea what could be going wrong here or if this is expected?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1388
java.lang.ClassCastException: with count(*) and union
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
When the following SQL requests, a classcast exception occurs on the server side, for instance using sqltool:
On the hsqldb server logs:
I am using the last snapshot from svn, but it is reproducible with the 2.3.2.
Best regards,
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1389
Oracle syntax mode, case statement datatype not resolved
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Running the following set of sql statements against HSQL results in a stacktrace, while running them against Oracle successfully executes.
SET DATABASE SQL SYNTAX ORA TRUE;
It works in both HSQL and Oracle.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1390
ArrayIndexOutOfBoundsException in org.hsqldb.DatabaseManager.getDatabaseObject
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The application has 8 threads opening and closing connections to hundreds of HSQLDB databases. The application is running with the 2.2.9 hsqldb.jar.
I suspect the issue is that the fileDatabaseMap isn't synchronized consistently.
In addDatabaseObject, getDatabaseObject, and lookupDatabaseObject, the synchronization is on the DatabaseManager class. In removeDatabase, it's on the map.
I glanced at the 2.3.2 code and it didn't seem to differ here.
Thanks.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1391
setObject with UUID throws excepton
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We work with UUIDs. Every time when I call statement.setObject(parameterIndex, java.util.UUID.randumUUID().toString()); it throws an SQL exception. The column is generated with VarChar(100). Though there should be no Error. I solved it like this:
I modified in method setObject of org hsqldb.jdbc at line 1055:
It would be nice if Objects would be converted (when possible) to the Type of table. This may could be manged by an boolean.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1392
Wrong results from inner join with subquery in join condition
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following code was extracted from
On Hsqldb 2.8.2 it produces two rows ('a', 1) and ('c', 3). PostgreSQL, MySQL, H2, SQLite and Derby all agree with this result. Hsqldb 2.3.2 produces an empty result set.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1393
Values scalar sub-query causes NullPointerException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
A query that uses VALUES inside a scalar sub-query gives a NullPointerException.
Here is the query:
FYI, I am developing on Apache Calcite, on https.
I have a workaround so I don't consider this problem to be urgent. The workaround is as follows:
select (select null from (values 1) union all select null from (values 1)) from "foodmart"."department";
(Yes, the desired effect is to produce a cardinality violation.)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1394
Oracle syntax mode, update with sub query aggregation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following query runs successfully on Oracle but fails on HSQL
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1395
Wrong constant used for indexing
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In the TABLE_PRIVILEGES function of dbinfo.DatabaseInformationMain.java, the TABLE_PRIVILEGES constant is used to index into sysTables (line 3347) and sysTableHsqlNames (line 3350), but uses SEQUENCES to index into sysTableHsqlNames at line 3362/3.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1396
"Error: Incompatible data types" thrown during CASE Statement with Sub-Query
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Procedure to replicate,
1) Run the DDL.sql to create the Schema and Tables.
2) Run the DML.sql to populate data into the Tables.
3) Run the SQL as decsribed above.
Note,
1) This may be related to https://sourceforge.net/p/hsqldb/bugs/1393/ as I am currently looking into the https://issues.apache.org/jira/browse/CALCITE-259, to rewrite queries and push it down to the underlying data source (in my case HSQLDB).
Thanks!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1397
Order by log(column) changes the type of column in select list
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In the following example o_orderkey and o_customer_key are integers, but in the 2nd query o_orderkey is returned as a double:
This seems like a bug. It looks like the o_orderkey field in the select list received the type of the order by log(o_orderkey).
Attached program to reproduce the problem, compile and run as follows
Output for me is :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1398
Where exists using an temp table with join has no results
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In a specific case, when using a 'where exists' clause the hsqldb should return results, but it doesn't. The next query is used:
Strange thing is: when the 'left outer join' clause on TABLE2 is removed, the results are correct.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1399
Cannot assign sequence value to a variable in a stored procedure
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It is not possibile to assign the next value for a sequence to a variable, in a stored procedure.
Below the example:
the error is: "sequence expression cannot be used in this context"
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1400
closing and immediately opening a DB fails
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After writing a medium sized DB (about 100MB) i experience the problem that opening the same DB immediately after closing it fails.
Is there a way i can wait until the DB is finally closed so i can it open again?
This is the sequence as seen in the log file:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1401
Maven snapshot version numbering
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Currently all snapshots are
in a directory SNAPSHOT.
I recommend naming this directory based on the version number. E.g. 2.3.3-SNAPSHOT
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1402
View using "with" clause cannot be selected
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
A view using a "with" clause can be created successfully but cannot be selected.
The following runs successfully on 2.3.2 but not on SNAPSHOT
Simply executing the select outside the view runs successfully
with w_clause as (select * from a_table) select * from w_clause;
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1403
Grant examples fail
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We have a project, where we need to invoke some Stored Procedures. Doing so via a Trigger is working, but when trying to access them directly, we got the error:
As the Stored Procedure is working via the Trigger, we tried to look at the permissions, and from the documentation: http://www.hsqldb.org/doc/guide/ch09.html#grant-section
We can read how to do it. However, the example there:
GRANT ALL ON CLASS "java.lang.Math.abs" TO PUBLIC;
Is causing the following error:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1404
#1404 Missing class 'StatementSignal'
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Good day,
I think the class 'StatementSignal' is currently missing on the trunk.
The ParserRoutine is referring it on line 1988
Kind regards,
Richard
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1405
#1405 Table alias fails for INSERT INTO in Oracle syntax compatibility mode
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When using an instance of HSQLDB started with Oracle syntax compatibility (sql.syntax_ora=true), the usage of table alias in INSERT INTO statements will fail with an unexpected token error.
Example:
-- create the example table# create table mytable (my_id varchar2(40 char) not null,my_name varchar2(1000 har),constraint my_id_pk primary key (my_id));
-- this insert will work fine
insert into mytable (my_id,my_name) values ('item1','First item');
-- but this will fail claiming unexpected token for mt
insert into mytable mt (mt.my_id,mt.my_name) values ('item2','Second item');
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1406
#1406 NPE when a unique constraint is violated
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
A null pointer exception is thrown in IndexAVLMemory when a unique constraint is violated:
The old and correct behavior was to get a SQLIntegrityConstraintViolationException in such a case.
Test case provided.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1407
#1407 incompatible data types in combination when using COALESCE in case of DATE type
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In case of DATE type this does not work:
This workaround works:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1408
#1408 Problem with column labeling in java ResultSet
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Reading a db in locally from file to test a SQL query-generating API. I run the following Natural Join in java using the jdbc:
where the ActorTeams join table includes an ActorID and a GroupIP, GroupBundles join table includes GroupID, BundleID, and a policy string, and BundleResources join table includes a BundleID, ResourceID, and ResourceType string. All fields are strings.
I notice in the ResultSet that the columns are correctly matched to the data in the columns. However, the column labels are mixed up; instead of the columns "ActorID, ResourceID, and ResourceType" I get GroupID, policy, and ResourceID (which are not the columns that I selected). I'm thinking this is a bug.
I ran the same query on the same schema in a different relation DB and it worked as expected.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1409
#1409 unexpected duplicate rows in result of GROUP BY primary_key on implicit cross-join
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
commands issued on a new in-memory database:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1410
#1410 General error since version 2.3.2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Since version 2.3.2 something is broken and I get a "General error":
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1411
#1411 index misses rows "where column < VERY_LARGE_VALUE order by column desc"
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
-- querying without index matches the row.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1412
#1412 index causes null values to match "column >= LARGE_NEGATIVE_VALUE"
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1413
#1413 Illegal SQL is persisted for table valued function
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Consider the following table-valued function:
When this function is created via jdbc:hsqldb:file:C:/data/hsqldb/test.db, it is effectively stored as
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1414
#1414 NullPointerException regression in 2.3.3 on INFORMATION_SCHEMA queries
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I believe this is a regression in 2.3.3 as I can reproduce it only after upgrading. Consider the following table:
And now this query against the INFORMATION_SCHEMA:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1415
bug with queries with more than one "right outer join" since the version 2.2.9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It seems that there is a bug with queries with more than one "right outer join" since the version 2.2.9.
It looks that the problem is not direct in the database but in the jdbc connector.
I test them in the Server Mode and connect with SQuirrleL.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1416
#1416 indexed null comparison gives wrong answer
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Fetched 0 rows.
-- same query gives wrong answer when using index:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1417
#1417 Connection does not become invalid after HD loss
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
With HSQLDB 2.3.2 on Windows 7.
Create/Connect to a db on a network drive (usb stick).
Disconnect (I suspect a similar behavior occurs with a full hard disk)
You can keep CIUDing without any problems and even close your db without running into any exceptions.
It is very graceful behavior to be able to continue working even if the hd is lost (temporarily), but in my usecase too graceful (a user can work for hours against the database without realizing his changes are never saved).
I'd expect the Connection to become invalid after encountering such a problem, or some driver specific way to detect/notify me of this?
maybe Connection.getWarnings()?
or a Connection.setNetworkTimeout ?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1419
#1419 executeUpdate does not return the correct number
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I found a bug during the execution of executeUpdate() of CallableStatement.
The return value of executeUpdate is equal of the number of row inserted/deleted/updated as written in
I created a small test - the attached file - that creates a very simple scenario:
a table with 2 columns, 4 store procedure for each operation and a test that:
1) Insert a tuple (with data = value), select that tuple
2) Update that tuple (with data = value2), select it
3) Delete it, select it (result = null).
As result of the test, the tuple is inserted, updated, selected and deleted correctly but the value of RowCount is always equal to ZERO!
For this test I used the same types I used in my code to find this bug: CallableStatement, HikariDataSource.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1420
#1420 Error on Cascade/No Action delete when I have 3 tables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I found a bug in the current release.
I have 3 tables with these parameters (PGS Syntax):
I use these alters:
When I would like to delete from 'example_class_b3', I get constrain validation error as expected.
But when I delete from 'example_class_a3' table, the result is 'updated count 1' and my expectation is a constrain validation error. This worked in 2.3.2 release.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1421
#1421 "call current value for ..." returns null string, if called before "call next value for ..."
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
"call current value for mycounter", if executed before "next value for mycounter" returns a null string. It should return the current value, I assume. This is through the DBManager GUI on 2.3.3.
If executed after a 'next value for ...', in the same DBManager session, it appears to work correclty.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1422
#1422 NullPointerException on add constraint
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
if I try to execute a alter table statement I receive an NPE. I used HSQLDB 2.3.3.
My create statement:
Please fix this error and use e.g. Sonar to detect NPE before a release is performed.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1424
#1424 ArrayIndexOutOfBoundsException from XEvent in 2.3.3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Problem
Reproduced with 2.3.0 and 2.3.3, but NOT 2.2.9. Running Java 1.8.0_60.
When running an application with HSQL on the classpath I see the following error in the console now and then:
After a lot of searching It turned out to be a problem with a xbindkeys mapping I'm using to map my scrollwheel to ALT+Left and ALT+Right (navigate back and forth in browsers and eclipse).
Steps to reproduce
Comment
I have only been able to reproduce it with the xvkbd. If the -xsendevent argument is used the exception is not thrown.
Potentially related issue:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1425
#1425 Sequence generated only once during insert.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a table using nextval('test_seq') as DEFAULT. It works, as long as I insert only 1 row. With more than one row it complains that the primary key contraint is violated. Is this behaviour intended or is this a bug? I'm attaching code to reproduce the problem, tested on HSQLDB 2.3.3. In PostgreSQL such sequence works as I expect - it generates a unique value for every inserted row.
--this is ok
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1426
#1426 ENGINE getFromFile failed 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
we had this problem when we started up the HSQLDB database in server mode:
Please take a look below to our database properties configuration:
Do you believe this problem is related to the cryptography module or not?
Can you help us giving a solution?
Thanks for your attention.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1427
#1427 BigDecimal precision is lost in NUMERIC(x.y) columns
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HSQL seems to lose precision of the inserted BigDecimal values in NUMERIC(x,y) columns. Fractional part has always y digits - regardless of what the original value was or whether all these digits are zeros or not. For instance if I have NUMERIC(5,2) and insert 1, I get 1.00 back.
Reproducible with 2.3.3. Seems to be a regression, works with 1.8.0.10.
Test case attached.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1428
#1428 Session.abortTransaction is set on InterruptedException, never cleared
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I was trying to track down a random unit test failure that was associated with this stack trace:
I finally tracked it down to the fact that some of the unit tests were testing corner cases around an interrupted thread, and that the interrupted exeption was picked up by hsqldb instead of my code it set Session.abortTransaction flag, which in turned caused the exception I was seeing. That is fine, but the flag is never cleared and subsequent commit/rollback operations on that connection will always fail, leaving that connection only semi-functional. Functional enough that connection pool validation still thinks it's a good connection, but make the connection useless for updates.
See attached for code InterruptBug.java for sample code that duplicates the problem.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1429
#1429 HsqlException: unexpected token: SELECT
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I am using @Formula("select .....") annotation for property in the entity class.
Calling the method with Query that returns my entity leads to following exception:
Using @Formula annotation is working in 1.8.0.10 version but not in higher versions.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1430
#1430 ClobInputStream.read(char[], int, int) does not comform to definition by Reader.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The code will actually put "this is a " in ch[] from 2-11.
basically when currentPos+len <= availableLength, the code will do wrong.
Proper implementation should be something like below.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1431
HSQLDB setting wrong input parameter name for procedure when preparing callablestatement
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am using HSQLDB as in memory database in one of my junit tests for a generic set of classes that I wrote for calling stored procedures.
Then at some point inside the class that prepares the statement I use setObject method to register the input parameter:
By using the debugger, I was able to find that there is a package private method in org.hsqldb.jdbc.JDBCCallableStatement class which is HSQLDB's implementation of CallableStatement, the method name is findParameterIndex which is called inside the setObject method, which checks if the provided parameter exists in the map of parameters from the procedure:
by exploring that map using the debugger I was able to see that the parameter name is wrongly set by hsqldb, at least in this map:
I was able to verify by changing the name in the setObject method call:
After that it worked fine.
The weird thing is that if I retrieve the metadata of that procedure using DatabaseMetaData.getProcedureColumns method, the name of the param is correct from the jdbc metadata perspective:
Note that the name is coming in upper case, I already check that. I changed the registering of the parameter name to upper case in the setObject method to see if that helped, but didn't make a difference.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1433
#1433 Wrong update when using subselects
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Consider the following setup:
Then run the following UPDATE statement to swap the two values in the column data:
The row with ID = 1 is updated correctly, but the row with ID = 2 contains a NULL value in the data column.
This happens with 2.3.3 and the 2.3.4 RC jar file
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1434
#1434 MVCC and deletes leaking RowActions
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
My version is 2.3.3. I'm experiencing unbounded memory growth using MVCC, cached=true, autocommit=true, and deletes. This is not a case of unclosed sessions or statements (AFAIK). What I am seeing is that the rowActionMap in TransactionManagerCommon has RowActions in it that are never cleaned up. I am determining this by looking at a YourKit memory dump. I see many RowActions in the rowActionMap that are for closed sessions. The RowActions are of type ACTION_DELETE_FINAL and deleteComplete is marked as false. The delete code I'm using is:
As you can see the connection is called withing a try-with-resources statement which closes the connection. I have observed close being called. If I put the prepared statements in the try (and explicitly commit -- I have not verified that both changes are required), the issue still occurs.
It is my understanding the all of the statements and associated results should be cleaned up when the connection is closed. There is a high degree of concurrency in my application. I can provide the YourKit snapshot if needed.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1435
HyperSQL Database Engine (HSQLDB) / Feature Requests / #328 ROW_NUMBER() OVER(ORDER BY) Results in Error (HSQLDB 2.3.3)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
We are trying to execute the following SQL command in HSQLDB and are receiving the error below. Our understanding of 2.3.3 (our current HSQLDB version) is that it should honor SQL Server's ROW_NUMBER() OVER() syntax. It does, up until the point we add an ORDER BY clause.
Can you please let us know what we are doing wrong?
Thanks in advance,
Chad
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1436
#1436 Current Statement not Canceled on ALTER SESSION
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Looking for a way to cancel long running Statements on a HSQLDB Standalone Server,
I stumbled upon this Stackoverflow Question and Freds Answer to it.
Using the current Release Candidate and the svn version from the Date of Freds Answer, I tried to release or close a session in order to abort the Statement using another connection.
I used
select session_id from information_schema.system_sessions where current_statement='some_sql'
to get the corresponding session id an then called ALTER SESSION <id> RELEASE.
The Statement, however, is not canceled instantly. It seems that HSQL waits for the statement to finish and then returns an rollback:serialization sql error to the connected client.
Is this the way hsql should behave in this case?
If yes, is there another way to cancel a statement immediately?
Thanks in Advance,
Christian
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1437
#1437 JDBCPooledConnection with allow_empty_batch
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
JDBCPooledConnection uses constructor JDBCConnection(JDBCConnection, JDBCConnectionEventListener) to create its connections. This causes the property isEmptyBatchAllowed, and others, to be left on the default setting, thus ignoring the URL properties.
I think you should add the following statement and its brethren in there as well:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1438
#1438 Inconsistent case sensitivity in JDBCConnection.prepareStatement(String, String[])
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am using the method JDBCConnection.prepareStatement(String, String[]). As you well know, the last String array argument tells the PreparedStatement which columns to return in the ResultSet from getGeneratedKeys().
The column names in that array are case sensitive while in the previous queries, there is no case sensitivity: select id from users is valid, resultSet.getInt("id") is valid, but using new String[]{"id"} is not valid. I would have expected the same case-insensitive column names here.
Here's an SSCCE that demonstrates the situation:
The output shows the first two id's, but crashes at the prepareStatement:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1439
#1439 Assignment to method parameter in JDBCPreparedStatement
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Version: 2.3.4
The result of c.nativeSQL(sql) is assigned to parameter sql instead of this.sql. In the end this.sql = sql is executed on line 3955 for toString() as the comment points out, so this might be intentional?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1440
#1440 unexpected token: , required: (
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This procedure worked in all previous versions of 2.3, it is objecting to the comma after Period
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1442
#1442 Bnary Data Type TableMetaData
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
First I would acknowledge the work that has been done with the HyperSQL
database. Being a fellow developer of a open source project I appreciate
the work involved.
In reviewing changes between v2.2.9 and v2.3.4 my Application MyJSQLView
is reporting a change in Precision and Size for Binary types. Given the
defined table below v2.3.4 is now giving a zero precision and size for
a standard Binary declaration with no length. The documenation indicates
this should result in a single byte Binary field.
Instead of being a single byte field, it appears to be cabable of
receiving a unspecified number of bytes. I'm able to put a larger number
of bytes then one in the field. In additional this is causing an error in
the application output of defining the table, field as Binary(0).
Dana M. Proctor
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1443
#1443 HSQL + Hibernate 5.1 (JDK 8) + JPA unable to insert due to column order and parameter order not aligned
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
I have a Spring 4.2, Hibernate 5.1 application which uses Liquibase to create a table. If I switch the application from MySQL 5.x to HSQLDB 2.3.4 it is unable to insert a row into a table (in the meantime it has inserted two rows into Liquibase control tables). The error is "Invalid argument in JDBC call" and the Hibernate-generated insert has the columns listed in alphabetical order but the parameters appear to be in the order that the table was created. If I switch back to MySQL all is fine but back to HSQLDB and it fails. I am not using Hibernate hibernate.hbm2ddl.auto by the way (which would cause the table columns to be created in alphabetical order). Apologies if this has nothing to do with HSQLDB but I can't think of what else it could be. I have also tried it with MS SQL Server and it works. The behavior seems to be Hibernate generates the insert statement with column names in alphabetical order. The parameters in the case of HSQLDB appear to be getting generated in the order the columns are in the table physically.
Thank you
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1444
CallableStatement::prepareCall doesn't work in 2.3.4 for stored function with (at least) one parameter
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I've just updated the version from 2.3.3 to 2.3.4 and I discovered a bug in 2.3.4 that was not in 2.3.3.
The snippet below can be used to reproduce it.
Basically, what happens in 2.3.4 is that there is an exception thrown in the call to connection.prepareCall("{call return_inparam(?)}"). If I replace the question mark with an actual value, and remove the statement.setString(1, "sometext"); line it works fine also in 2.3.4.
The exception call stack is the following:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1445
create table fails with exists error, but table does not exist
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We are using HSQLDB in an in-process (Tomcat/WebSphere) multi-thread situation where tables are being created, populated, queried and dropped very rapidly -- usually less than 5s in total. Each table is named uniquely, guaranteed. Each table also has a single compound index applied, and each table has a primary key auto generated via identity.
Infrequently, the create table fails with an "object already exists" error. In the catch block of that create, executing the identical create table script succeeds, in all cases.
In a sample run of 900 table creations (being generated by 10 threads in the pool), this occurred only 3 times. However, we can find no programmatic reason for it to occur at all.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1446
ResultSet#getObject not returning UUID
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
For a column of type UUID, a SELECT returns org.hsqldb.types.BinaryData as return type for ResultSet#getObject, instead of java.util.UUID.
PreparedStatement#setObject() does accept an UUID as type, so ResultSet#getObject should also return one.
See the attachment for a demo.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1447
Joining table with BigDecimal/NUMBER as PK fails when value > MAX_LONG
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi.
Attached are three Java files that describe my simpel model. I use Hibernate to store the data. Later (using Hibernate or straight SQL over JDBC connection) I get a variable success when joining depending on the value I instansiate the model with (the column "parentkey"). Joining (using WHERE or JOIN) works OK if the value I give to the Java BigDecimal is small, but fails if it is over 9223372036854775807 (which is the MAX_LONG_VALUE).
The actual SQL that silently fails is:
So I think this is this a bug, or ?
Regards.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1448
DB appears corrupt after INDEX DROP/CREATE on column with a UNIQUE constraint.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We had a schema creation script that drops and creates an index on a column with a unique constraint.
The bug is reproducible with the HSQLDB GUI tool using an in-memory db. Affected are at least 2.2.9 and 2.3.4 versions.
The script to reporduce the problem at the bottom of the post.
After the script is executed, table CLIENT_DETAILS has three entries, however, they are only
availiable to an unrestricted select query:
If one executes a query restricted on column CLIENT_ID, ony one entry is available:
The entries inserted after DROP/CREATE INDEX are missing in the result set.
The documentation menstions that one should not create custom indices on columns with UNIQUE constraint for performance reasons, since an index is created automatically on such columns. However, in this case the undesired effects seem to go far beyond poor performance.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1449
DROP CONSTRAINT causes SQLIntegrityConstraintViolationException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We use hsqldb (2.3.4) inmemory mode for testing and all test uses the same db.
When we start a full test at first we load data from xml to db as we drop constraints -> load data -> add constraints.
It works properly but some tests load some data again and one of them (and following tests) failed when tried to drop constraints.
It doesn't make sense because of a constraint (foreign key) dropping cannot cause "SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation"!
I know whcih test methods cause the problem just I don't know why.
Theese tests cointain failed transactions but other ones too and they don't cause any problem.
I attached the log and in debug I found that "olddata" (RowStoreAVL.java:540) contains primary key twice but obviously I found just once in db (with select).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1450
metadata for datetime types incorrect
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In 2.3.3 The table INFORMATION_SCHEMA.SYSTEM_COLUMNS, which just shows the result of the jdbc introspection getColumns has some problems with 'DATA_TYPE' and 'TYPE_NAME' of datetime types. There are two issues I can see:
2) When declaring a DOMAIN of TIMESTAMP WITH TIMEZONE CHECK VALUE IS NOT NULL, and then declaring a table column with this alias, 'DATA_TYPE' is again 93 as in issue #1, but also, the 'TYPE_NAME' is 'TIMESTAMP' (i.e. without time zone). Again, I suspect a similar problem with other date/time types but haven't checked.
Here is the result of an example, query on SYSTEM_COLUMNS. The first row is declared directly as TIMESTAMP WITH TIME ZONE as in #1, whereas the others all use an alias DOMAIN as in #2 above.
I guess that not many people care about this, but looking into rising populatrity of Java8 time, the driver should distinguish between these clearly to help the user to cast these as e.g. Time or OffsetTime.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1451
HSQL under-relative JEE stress leads to a shower of SQLNonTransientConnectionException: connection exception: closed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
While running weblogic 12.1.2 with NON-JTA (as the XA - JTA HSQL db driver seems to be quite problematic), our application systematically would go into connection closed errors "blood-bath" when runing agianst HSQL 2.3.4 stable release.
The type of exceptions that were encountered are listed in the text file uploaded in attachment, with the stac trace with company related code removed.
In this particular case, the HSQL DB 9001 server was running locally, on the exact same server running weblogic.
Nor firewalls, no network communication going exernally.
The connections to HSQL seemed reliable only while the system was not under stress.
As soon as the the testing would create heavier load, suddenly many of the threads executing read or write queries would get pummeled by exceptions such as the one in the stack trac.e
In the end the HSQL in version 2.3.4 even without JTA transaciton was dimmed unsuable.
We had to re-create the domain to run agains Oracle or Microsoft Sql server and all issues were gone.
Of course now we have the network latency in a test server that could very well do with testing on local HSQL db.
I did not try reverting back the HSQLDB JDBC driver and HSQLDB server to version 2.3.2, which is the current stable version of hsql we primarly use for integration tests, but I suspect the HSQLDB non JTA/typical jdbc driver under 2.3.2 as well as the server, is rather more stable.
This situation of weblogic detecting closed connections I had never encountered before on HSQL.
Weblogic will then reconnect and recreate new connects to the DB, but of course then many of the logically correct DB transactions have already be damanged by the previous errors ... so even if connections to HSQL will self-heal, this runtime exceptions are unacceptable, and the server/client driver combination for 2.3.4 had discarded as the only datbase giving us such trouble.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1452
ArrayIndexOfBounds Exception in RowSetNavigatorClient
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
S2R:
1. Create Statement with fetch size = 1
2. Execute select query from table which contains more than 3 rows.
3. Call next() three times on result set
4. Call resultSet.getObject(int)
Expected:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1453
NullPointerException parsing recursive CTE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This bug occurs reliably with HyperSQL 2.3.4. When parsing the following query, a Java NullPointerException is thrown:
The stack trace is as follows:
The problem appears to be that the recursive name ("foo" in the example) is referenced twice in the query. Although the minimal example query above does not really require require recursing twice, it's useful to be able to use a construction like this to halt the iteration based on some condition on the result of the previous iteration.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1454
Null Pointer in CASEWHEN and CASE... WHEN use case
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The last instruction of the script below results in a null pointer exception:
FROM Test
Same if CASEWHEN is replaced by CASE facet WHEN ...
This is happening with hsqldb 2.3.3 (I'd like to test with 2.3.4, but it needs extra effort due to bug https://sourceforge.net/p/hsqldb/bugs/1441
Thanks
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1455
PreparedStatement: setObject(..) throws SQLSyntaxErrorException when binding Arrays to MERGE statement
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I try to bind an Integer[] to a PreparedStatement that uses MERGE... I get a SQLSyntaxErrorException.
INSERT/UPDATE works fine.
I attached a test case that triggers that behaviour on my system with HSQLDB 2.3.4
Is that a bug or faulty user?
Harald
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1456
Silent option does not always prevent Server to write on stdout
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am starting a HSQLDB server programmatically. Everything is working well, except the silent option, which in despite of being the first method that is invoked on the Server object, one line is written on the standard output:
A quick look at the code associated to the class org.hsqldb.server.Server let me think it is a bug. The field isSilent is initialized to false by default. Then, its value can be changed with a call to setSilent or setProperties. However, these last methods make use of printWithThread to print a few things before changing the value of the field isSilent. Since value passed to printWithThread is written on the standard output if isSilent not set, there is always at least one line displayed on the standard output.
Is there any other solution to prevent output on standard output with the Server class?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1457
org.hsqldb.HsqlException: incompatible data types in combination
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I get following exception with v2.3.4:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1458
unexpected token: SELECT
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm using a formula to execute a select statement and I'm getting the following error: unexpected token: SELECT
Here's the relvant part of the stack trace:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1459
#1459 Commands work on embedded, not on server.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a sequence of legal SQL commands that work fine on HSQLDB embedded, but fails when running HSQLDB in server mode. In server mode the last command in the attached file results in a java.sql.SQLException: statement is invalid.
See attached file for SQL statements, including setting up the database and creating tables, server logs and config.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1460
Parser error when RECURSIVE CTE contains parenthesised UNION query
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This works:
This doesn't:
Only recursive queries seem to be affected. The following works:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1461
colon not working as fs separator
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
A text table named hello.txt containing only "hello:world" returns "Access is denied" or "constraint violation" when
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1462
table created via subquery not like a view
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
http says"An <as subquery="" clause=""> used in table definition creates a table based on a . This kind of table definition is similar to a view definition." Yet after creating a (non-text) table from a text table via an as-subquery clause, and then DROPping the source (text) table, I still have my data in the newer, non-text table. So the latter table is not similar to a VIEW onto the former, dropped table.I don't know enough of HSQLDB's functionality to suggest fixed verbiage.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1464
#1464 using clob instead of varchar uses lots of memory with mem database
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
For junit tests in hibernate I use hsqldb mem database for performance. As soon as I switch a column to use @lob annotation in hibernate the junit tests persisting object graphs fail because out of memory. Obviously the clob type uses lots of memory for the mem database compared to varchar and exhausts available memory.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1465
TO_CHAR not work into view
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I create a view on a table, if I use TO_CHAR into the view definition, HSQLDB doesn't find the column.
With an example :
I create a table MY_TABLE
If I create the view where I do a TO_CHAR on NUMBER_COL
I have this error :
But, if I create the view without TO_CHAR, it work :
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1466
#1466 java.util.UUID[] type cannot be used as bind variable
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The java.util.UUID type is already supported as a valid bind variable type for HSQLDB's UUID type, but it cannot be used for arrays. For instance, the following code fails:
The exception I'm getting is this:
The logic in the failing method is this:
There should also be a section like "if (a intanceof UUID[])".
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1467
Recursive query runs forever
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This simple recursive query runs forever:
Here are a few thread dumps from jstack:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1468
#1468 CallableStatement cannot handle expressions containing bind variables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a procedure like this:
I can call the procedure easily using a CallableStatement like this:
I can also call the procedure using a constant expression for the IN parameter:
However, I cannot use any expression that contains a bind variable, e.g.:
This results in the following exception:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1469
#1469  regression 2.3.4 ADD CHECK CONSTRAINT keyword handling problem
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
While upgrading to 2.3.4 from 2.3.3 a new error started to apper in our tests:
Full error:
the word period is not mentioned in reserved keywords. http://hsqldb.org/doc/guide/lists-app.html
Also when period is esaped (double quoted) the same statemt dod not work (with different error)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1471
#1471 ArrayIndexOutOfBoundsException in RowStoreAVLHybrid#getAccessor()
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Attempting to upgrade from HSQL 2.3.2 to 2.3.4 we observed lots of our integration tests (more than 900) randomly failing with an ArrayIndexOutOfBoundsException in RowStoreAVLHybrid:
If I add diagnostics to RowStoreAVL and RowStoreAVLHybrid and let the tests fail again, I see the actual length of accessorList was 1 at the moment the call was made, and the RowStoreAVLHybrid instance has been accessed by 40+ different threads.
This looks like a inconsistent synchronization issue to me, despite all methods which change the array reference (not the array contents itself) seem to be synchronized.
The exception is thrown during both SQL INSERTs and SELECTs, but always from the same (single) place.
The issue is hard to reproduce in a lab, so I'm not providing any code here.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1472
#1472 ArrayIndexOutOfBoundsException for array_agg(distinct col)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This seems to occur because list.size() is used on line 343 instead of array.length. List size and array length can differ if distinct clause is used..
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1475
#1475 Can't check for index existence by selecting from INFORMATION_SCHEMA.SYSTEM_INDEXINFO
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
As of r5720 (2.4.0-SNAPSHOT), I can no longer check whether a particular index exists.
This is a regression compared to v2.3.4.
I'm pretty sure this is a regression compared to r5711, too.
The testcase which demonstrates the issue is attached.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1477
#1477 SQL batch issue in concurrent  memory DB
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I run the following code in servlet
when I do the performance test, there may be a batch error occur, the batch fail and the table will not be created. I wont get the exception if I remove the "mem" for connection.
I also update my code, if I catch batch error, I will execute the SQL one by one, then the tables are created successfully.
I think there may be some concurrent issue in HSQL, can you help fixing the issue?
thank you!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1478
#1478 OdbcPacketOutputStreamTest can't be compiled in some environments
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If LANG=C and LC_ALL=C are exported, org.hsqldb.server.OdbcPacketOutputStreamTest can't be compiled:
So
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1479
#1479 preprocessor.jar can't be built with gradle
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Unlike the ant preprocessor command, gradlew preprocessor fails with:
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
FAILURE: Build failed with an exception.
For some reason, Ant runtime (BuildException) can't be found on the class path during the Gradle build.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1480
#1480 test.xml doesn't make any use of junit38.lib property
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When building hsqldbtest.jar (declared in build.xml), it is possible to specify an external junit.jar location via -Djunit38.lib=... .
This is not the case for make.test.suite and run.test.suite targets from test.xml, however, so one always needs to copy the junit.jar to lib/ first.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1481
#1481 Provide support for dependency management
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
HSQLDB requires 3 external libraries:
Gradle has been providing automatic dependency support for ages. Please add support for dependency management via Gradle, so one doesn't need to manually download junit.jar any longer.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1483
#1483 https connection problem
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm trying to use hsqldb using https protocol. But I found a problem when the URL is generated for https.
At this line the s variable receive an http url
at this line the https url is generated using the "s" variable with has a full http url.
The attached diff shows one solution for this problem.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1484
#1484 Docs Typo: Timstamp
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
while reading the docs on Java Language routines (http) I stumbled upon a typo in the table mapping SQL types to Java types. It says 'TIMSTAMP WITH TIME ZONE' but should say 'TIMESTAMP WITH TIME ZONE'
Regards,
Florian
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1485
#1485 Invalid character value for cast
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
Previously with HSQLDB 2.3.4 it was possible to execute an SQL query like this one:
select cast(to_char(created, 'IYYYIW') as int) from TemporalEntity
But now with 2.4.0 we get this error:
After some testing I found out that there seems to be a problem with the "to_char" function because if I test the same SQL query but without the casting part then I get this: 2017'18
There is an apostrophe that gets inserted between the date and week values!
Please let me know if you need more details and/or a test case...
Thanks,
Christian
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1486
#1486 TarGenerator.write outputs progress in stderr
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We are using hsqldb to, among other things, periodically backup our hsql database. When doing so, our service produces log messages marked as ERRORs (since it is coming from stderr) that just show the progress of the backup. This introduces noise in our log monitoring system, and is also misinformative.
The problem is located at src/org/hsqldb/lib/tar/TarGenerator.java and a patch fixing it is attached.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1488
#1488 StackOverflowError in Value Pool mechanism
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
StackOverflowError in Value Pool mechanism
Since HSQLDB version 2.3.4 there is a bug in the area of the ValuePoolHashMap which will eventually cause HSQLDB to crash with a StackOverflowError. In our test environment the crash will reproducibly occur after 5 days of continuous operation. In real operational scenarios the occurrence may be more or less frequent depending of the frequency of updates performed on existing table records.
Characteristics:
The occurrence is characterized by a StackOverflowError, with a stack trace of the following constitution:
Further, in theory, the bug may occur in the methods: org.hsqldb.map.ValuePoolHashMap.getOrAddDate, org.hsqldb.map.ValuePoolHashMap.getOrAddDouble.
After the occurrence, HSQLDB will fail to execute any further query through JDBC with the following error:
A restart of the HSQLDB is required to get it working again. No database corruption was observed after restart.
Analysis:
The ValuePoolHashmap serves as a mechanism of de-duplication to reduce memory usage when the database contains several entries with equal value. For this purpose it provides a number of type specific methods like getOrAddString() or getOrAddInteger() with accept a primitive type value or object value, respectively and returns a unique, singleton object instance with equal value. Its implementation relies on the bespoke hash map implementation class BaseHashMap, which behavior is modified in ValuePoolHashmap to achieve a cache-like behavior in order to not endlessly keep value objects that are no longer part of the database entries. At the same time the implementation attempts to keep a balance between holding enough value objects for repeated equal values in the database but not waste memory by keeping value objects in the internal hash map that only occur once of few times in the database. This is realized by access counting.
The typical logic of the de-duplication methods consist of computing a hash index based on the value and attempt to find a matching singleton instance, possibly after several re-hashes. If a matching object is found, the method returns that object right away. Otherwise, if no matching value object can be found in the hash map, a size check of the internal index data is performed:
When this condition holds, an internal re-organization of the hash map is performed, after which the method invokes itself recursively in a repeated attempt to find a matching value object. When the bug occurs, the recursive invocation of the method does still not find a matching value object and the condition hashIndex.elementCount >= threshold still holds, which leads to an endless recursion.
Related:
On 2015-05-29 a similar bug was discovered and reported by Jesse Barnum in the SF forum, which affected the method getOrAddInteger of HSQLDB in the development trunk for version 2.3.4. That similar bug was then reported to be fixed on 2015-05-30 with commit r5476, which made it to the release of version 2.3.4. The commit however only affected the methods getOrAddInteger() and getOrAddLong(). The other methods like getOrAddString(), the culprit of this bug, was not changed.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1490
#1490 File size increase with upper on clob
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If you execute a select with in upper on a clob column in the where clause the size of the lobs-file increases. A upper copy of the clob will be created and stored. But if you execute the same statement again the already copied data won't be reused. Further the space is not freed again. Only if you manualy execute a CHECKPOINT the unused data in the lobs-file is removed and the space is available for other data.
If you don't know this behavior the file increases until the disk is full. The size of the lobs-file cannot be shrinked as it was before the execution of the selects with an upper.
It would be nice if this would be documented and that there is a possibility to shrink the size of a lobs file to the current used defragmented size.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1491
#1491 NPE in ConnectionDialogSwing.actionPerformed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Launching the Swing GUI (via java -jar hsqldb.jar) and pressing the "Clear Names" button results in an NPE thrown in the AWT event thread. The backtrace is attached.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1492
#1492 WHEN clause ignored using Java based triggers
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Trigger:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1493
#1493 hsqldbmain missing Main-Class in jar file.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Build script for hsqldbmain is missing a main class in the mainfest like
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1494
#1494 Problem with database files initialization in volume of windows docker container
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
While running app in windows docker container with Oracle Java 1.8 within mounted volume HSQLDB library fails to rename files with ".new" prefixes
When it's running in regular directory it succesfully rename files:
According to the code in the org.hsqldb.lib.FileUtil#renameElement method result of org.hsqldb.lib.FileUtil#renameWithOverwrite method was not checked, so it silently ignores it.
It seems that could be implemented workaround for cases when rename fails.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1495
#1495 VALUES expression and TIMESTAMP WITH TIME ZONE literals not working
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Potential bug with VALUES expression and TIMESTAMP WITH TIME ZONE literals
The following highlights the bug.
Is this a bug?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1496
#1496 PreparedStatement#setObject with java.time.LocalDate saves incorrect values to DATE column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When using PreparedStatement#setObject with a java.time.LocalDate object to update a DATE column, the stored value appears to deviate from the actual value by 999 days for each day that the actual value differs from the epoch (1970-01-01). That is, a LocalDate of 1970-01-01 works, but 1970-01-02 is stored as 1972-09-27 (999 days after 1970-01-02) and 1970-01-03 is stored as 1975-06-24 (1998 days after 1970-01-03).
Test code:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1497
#1497 unique constraint or index violation: SYS_IDX_10094 when IN predicate contains array larger than 288 elements
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When issuing SELECT (through JDBC with using technique described in HSQLDB guide) in which IN predicate value is an array of 299 lements or larger, exeption "(java.sql.SQLIntegrityConstraintViolationException) java.sql.SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation: SYS_IDX_10094" is thrown.
Test-case code:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1498
#1498 IndexOutOfBoundsException from Table.getColumn() while running recursive queries
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Running of recursive query causes IndexOutOfBoundsException since 2.3.2. Works fine with 2.3.1 version.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1499
#1499 Merge - Insert does not produce auto-generated keys
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
"Merge-Insert" does not produce auto-generated keys.
Java Code:
Auto-generated keys will produced if it is not a merge statement. Only a If-Statement ist working fine.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1500
#1500 TEXT table source parse issue
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Fixed issue with incorrect parsing of text table data when the first field of the first line contains a quoted comma.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1501
#1501 SELECT ... INTO issue with subquery
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Inside a routine, a SELECT ... INTO statement that relies on a subquery does not work correctly. Example below was reported in the Open Discussion Forum
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1502
#1502 order by does not work in for loops in SQL-invoked routines
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The order by clause does not work in for loops in SQL-invoked routines. It will be ignored.
This issues exists since before version 2.3.1.
My Example:
A "get_test_vars()" call returns "ABC" but it should return "CBA".
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1503
#1503 CHECKPOINT DEFRAG raises IllegalArgumentException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
I trying to migrate a table containing 140 millions rows using liquibase.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1504
#1504 Delete with inner join not working
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Delete with inner join is not working.
For example:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1505
#1505 How to reset identity column?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have many tables which will created and fully deleted from the user.
I couldn't found a way to reset identity column, because the normal command:
Thank you
PS: Do you have a plan to release 2.4.1? There are already many bugfixes which are important for me.
Thank you
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1506
#1506 "natural left outer join" bug
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When I input following SQL statements:
It's supposed to return
But HSQLDB returns:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1507
#1507 ALTER TABLE .. ALTER COLUMN doesn't adhere to the documentation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Try the following script:
According to the documentation, the above should be perfectly fine
The relevant section is:
This form of ALTER TABLE ALTER COLUMN accepts a columnDefinition as in a CREATE TABLE command, with the following restrictions.
Restrictions
The NOT NULL attribute will be that of the new definition (similar to previous item).
However, the parser rejects the query with "unexpected token: NOT in statement". The same is true when adding a DEFAULT clause and other clauses.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1508
#1508 DECLARE LOCAL in Stored Procedure
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using "declare local temporary table" within a stored procedure will not compile. In a simple statement it will, however. See proc below.
If schemaless temp tables are not supported then perhaps the docs should be updated to clarify this as the documentation doesn't suggest such a limitation.
Thanks.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1509
#1509 SPs and Cursors : cannot be cast to org.hsqldb.result.Result
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Exception thrown when executing a procedure that returns a cursor on an in-memory db. I have read anything I could find on the subject including this post and I am pretty sure I am following your lead as to how ref cursors are supported.
Thanks in advance!
Monte
The procedure:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1510
#1510 Exception after altering a varchar to clob
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The alter table command seems to execute properly, however on the next launch of the application, I see this stack trace. Any suggestestions would be greatly appreciated.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1511
#1511 Exception when ordering distinct array_agg
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1512
#1512 Collate clause from domains lost in DB script file
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Using v2.4.0, I have created some domains with collate clauses as per the guide:
However I discovered my SELECT queries were not working as expected as the collate clause was not being applied during table creation.
Demonstration of issue using sqltool:
Fetched 2 rows.
Alternatively, shutting down the database then manually adding "COLLATE SQL_TEXT_UCC" to the "CREATE DOMAIN" definition in the DB's script file also works.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1513
#1513 Discarding decimal places using Oracle syntax with column type NUMBER
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We use hsqldb with sql.syntax_ora=true.
Switching version from 2.3.4 to 2.3.5-2.4.1 datatype NUMBER shows different behaviour.
With 2.3.4 storing a floating point value in column of type NUMBER the decimal places are preserved like in Oracle (3.141 -> 3.141).
With version >=2.3.5 storing a floating point value in column of type NUMBER the decimal places are lost (3.141 -> 3).
In the changelog appears a fix which may introduced this behavoir:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1514
#1514 WHERE id IN ( UNNEST(?) ) only returns first matching row
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
only returns the first matching row when the column is NUMERIC(5) andthe array is int or smallint.
However, if we change the column type to INT
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1515
TRANSACTION_SIZE() broken? Or at least doesn't track information_schema
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1516
#1516 Case-insensitive English collation does not work with LIKE clause
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The docs at http say (regarding collation strengths) "The value 0 indicates least sensitivity to differences. At this strength the collation is case-insensitive and ignores differences between accented letters." To me, this means that "English 0" should be a case-insensitive collation, and so equality or LIKE clauses should match case-insensitively.
This works as I expect with SQL_TEXT_UCC collation, but with "English 0" collation an equality check is case-insensitive, but a LIKE clause is not. This is inconsistent between those two collations, because I would expect them to be the same in terms of case-insensitivity. I expect the collations to work the same whether it's an equality or a LIKE clause.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1517
#1517 Handling of pos./neg. infinity values in HSQLDB 2.4.1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
in an open-source project, quantile values of the F-statistic are calculated and stored to a HSQLDB (version 2.4.1). The F-distribution needs two degree of freedoms (dof). One of the dof can be +Infinity (in this case, the F-distribution is equal to the Chi2-distribution).
The log file of HSQLDB contains the right value for +Infinity, i.e. 1E0/0
If I close the database by shutdown, the sign of the infinity value is changed to -Infinity, i.e. -1E0/0. The corresponding script file contains
For validation of my problem, I create a database, insert one row to the TestStatistic table, and kill the application. Thus, the database is unclosed and the log file still exisits. You can take a look to the log file to validate the Infinity-value. If you open the database, HSQLDB flushs the entries of the log file to the script file. In script file, the sign is now negative. Why?
What is the reason of the sign-change? You can download the short example here.
Tested on Win7, Java 10 using HSQLDB 2.4.1.
Thanks in advance.
Micha
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1518
#1518 "incompatible data types in combination" for case when statement
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Here are the tables I was creating:
Here is the SQL query I need to execut:
FROM CustomerOptions co INNER JOIN Customer c ON
It complains about "incompatible data types in combination / Error Code: -5562 / State: 42562"
I think mainly because the PrimaryPhone column is bigint while others are varchar.
a vast system is already built on the above mentioned tables, and changing the column definition isn't really an option.
the same SQL query executes fine on MYSQL (production database).
we are using hsqldb to run unit-tests
so our unit-tests are not passing, but production is working fine.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1519
#1519 13.28 "String comparison with padding" documention typos
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This is a very minor ticket, but I ran into the 'silent character padding issue' referenced in ticket #1175.
I found the documentation regarding setting "no pad" in section 13.28 but couldn't get it to work until I realised there was a typo!
The corrections needed are:
to become:
and
By default, when two strings are compared, he shorter string is padded
to become:
By default, when two strings are compared, the shorter string is padded
Set to priority 9 as I assume this is the lowest.
Thanks!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1520
#1520 In DB script file, CREATE COLLATION with CREATE TYPE broken
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have columns that use a custom defined TYPE via "CREATE TYPE" statements. These columns, in fact the whole database, needs "NO PAD" or "sql.pad_space=false" property.
For non-custom-type columns I can use SET DATABASE COLLATION SQL_TEXT NO PAD and/or the connection property "sql.pad_space=false".
For the custom-type columns, the corresponding CREATE TYPE statement ends with "COLLATE SQL_TEXT_UCC". However there's no way to specify "NO PAD" with "CREATE TYPE" so I had to create a new collation with CREATE COLLATION SQL_TEXT_UCC_NO_PAD FOR SQL_TEXT FROM SQL_TEXT_UCC NO PAD and use this new collation in my CREATE TYPE statements.
This worked the first time I created/opened the database but after shutdown the database cannot be reopened.
The first issue is that in the script file the "CREATE COLLATION" statement is placed after the "CREATE TYPE" statements that use it so I get a "user lacks privilege or object not found: SQL_TEXT_UCC_NO_PAD" error.
If I edit the DB script file and move the CREATE COLLATION statement to just before the CREATE TYPE statements I still get "user lacks privilege or object not found: SQL_TEXT_UCC_NO_PAD" error.
I noticed the script's version of CREATE COLLATION has "PUBLIC.SQL_TEXT_UCC_NO_PAD" so I changed one of script's CREATE TYPE statements so that the collation name was similarly qualified:
But now I hit a new error: "unexpected token: PUBLIC" which suggests CREATE TYPE isn't able to deal with qualified collation names.
I'm now stuck with no way to reopen my database or no way to perform non-padded, case-insensitive string comparisons.
Any ideas for a workaround?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1521
#1521 stored procedure with output parameter can't call another stored procedure directly
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have a stored procedure 'testDummy1' with output (or input/output) parameter.
I can't create another stored procedure 'testDummy2' with output (or input/output) parameter. Calling 'testDummy1' passing the parameter directly, got error:
Errore SQL [42603]: dynamic parameter or variable required as INOUT or OUT argument
Below the sample code:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1522
#1522 SqlFile throws error on first statement when script file has a BOM
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm using SqlFile to execute a SQL script that was generated by SQL Server Management Studio (SSMS). My intention is to run the script against another instance of SQL Server to re-create an entire database.
Because SqlFile does not seem to like the GO statements that SSMS intersperses through the file, I pre-process the file to replace the GO statements with semi-colons. However, I still get an error on the first statement because the file is encoded as UTF-16LE.
SqlFile does not have a problem with subsequent USE ... statements, nor does it have a problem with USE [master] per se, as illustrated by the following test script:
When I execute that script using SqlFile from SqlTool 2.4.1 via
it produces
Further testing revealed that if I convert the file to UTF-8 with no BOM the error goes away. If I convert the file to UTF-8 with a BOM (as Windows seems to prefer) then the error comes back.
So, it looks like SqlFile has a bit of a problem with Unicode files that have a BOM, even if the file really does need one (e.g., UTF-16LE).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1523
#1523 Casting string to TIMETZ and back to VARCHAR yields wrong results
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Running this query:
I would expect to get the original value back, i.e. '00:00:00+02:00'. Instead, I'm getting '24:00:00+02:00', which cannot be parsed using java.time.OffsetTime.parse(). I'm not too skilled deciphering the SQL standard text, but in any case, the behaviour is inconsistent with this, which raises an error (as expected):
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1524
#1524 System generated constraint name breaks log file
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Procedure:
Server fails to rebuild from log file because create constraint statement with no name specified generated a different name (always or under specific circumstances?). When the drop constraint statement in the log file is reached, the name specified in the drop constraint statement does not exist and the entire log file is thrown out.
Solution: include the system generated constaint names in create constraint statements recorded in the log file?
Excerpted from broken log file:
alter table cycle_counts drop constraint sys_fk_14655
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1525
#1525 Sequence next_value invalid after database restart (from scriptfile)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When creating a table column with GENERATED BY DEFAULT AS SEQUENCE, while restarting the database using the scriptfile, any sequence automatically gets set to MAX() of the corresponding columns.
If the sequence is defined with a MIN/MAXVALUE outside this MAX(), working with the sequence is not possible anymore. Also, another restart (scriptfile contains an invalid RESTART WITH now) is not possible (org.hsqldb.HsqlException: number out of the valid range for sequence generator).
Expectation:
The sequence must use the next-value given in the scriptfile. It should never automagically change these database definitions on restart.
How-To-Reproduce:
All correct so far, also the scriptfile is ok:
As some can see, the NEXT_VALUE automagically got set to some value, which is max(id)+1.
And, it's outside MINVALUE/MAXVALUE range now. This is what gets written to the scriptfile then:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1526
#1526 Illegal reflective access warning on JDK11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
From the warning emitted by the JDK:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1527
#1527 Misleading error message when timestamp literal is illegal
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Consider the following SQL
select
from information_schema.system_users;
It contains an illegal timestamp literal. The error message I'm getting, however, is:
This is quite misleading
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1528
#1528 sql.sys_index_names doesn't seem to work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If I execute the statements below the index gets created as SYS_IDX_13915 (or something like this).
I expected a different name, according to the docs.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1529
#1529 Possible typo in  missing data impact clause error message
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If execute this:
I get:
I would expect READS SQL DATA instead of only READS SQL.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1530
#1530 Exception when creating foreign key
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The last of these three statements:
causes this exception:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1531
#1531 ALTER TABLE loses ON UPDATE clause
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The table works as expected after being created with:
However after changing it with something like:
the ON UPDATE clause stops working.
If it's any help this also happens when adding a foreign key or dropping a column.
I didn't try anything else but I suspect it's tied to ALTER TABLE, no matter what kind of change is done.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1532
#1532 Can't change padding for string comparison
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After executing these statements:
sql.pad_space is still set to true.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1533
#1533 Extra whitespace in .script file
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When creating a new database specifying the connection property hsqldb.digest the .script file contains a line like this one:
with an extra space between DATABASE and PASSWORD.
Even if it's removed manually, a SHUTDOWN COMPACT puts it back in its place.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1534
#1534 Data file size doesn't seem to be as expected after enabling files space
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
On a newly created database if I execute these statements:
The .data file is 4 MB and I would have expected 2 MB since by default hsqldb.cache_file_scale is 32.
The file size remains the same even if I change the first statement to SET FILES SPACE 1;, where I would have expected 1 MB.
The size is still 4 MB even if before executing the statements above I set hsqldb.cache_file_scale to 256, in this case I would have expected 16 MB.
Maybe this is not a bug but after reading the docs this isn't the behavior I would expect.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1535
#1535 ClassCastException when using ora_syntax=true and Union ALL
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi there!
I'm getting an EOFException (connection closed) with some sqls, like the one below (ora_syntax = true):
If you need more info, please let me know.
Thanks!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1536
#1536 CountUpDownLatch.java: String.format typo $d -> %d
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1537
#1537 alter session close not working with sessions that have an open transaction
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Steps to reproduce:
I've tested another options (RELEASE and END STATEMENT), but the end result is the same.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1538
#1538 Can't create java.sql.Array with type name TINYINT
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following test errors:
But I think it should succeed just as this one:
The exception thrown is:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1539
#1539 SSL connections via hsqls not working on JDK 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I've discovered a strange problem regarding SSL/TLS encrypted connections on JDK 11. I was able to reduce the problem to this short application:
The program creates two connections to the database server via hsqls. Of course the server was configured properly and the truststore / truststore password is correct.
Running this application on OpenJDK 8 & OpenJDK 10 leads to the expected output:
First connection established...
Second connection established...
But running this application on OpenJDK 11 leads to an error on the second connection:
First connection established...
Second connection failed!
Obviously the second connection attempt should not fail, if the first connection was properly established.
I was able to reproduce this problem with HSQLDB 2.4.1 and SVN trunk on the following systems:
I hope there is a solution before HSQLDB 2.5.0 is being released. Maybe you have an idea about this?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1540
#1540 hypersql incompatible data type
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
can someone pls tell me why this error ?
Derby shut down normally
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1541
#1541 Cannot connecto to the Database after renaming PUBLIC schema
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I have discovered a strange behaviour of the database after a certain sequence of operation. I was able to reduce the problem with this code snippet:
the program renames the PUBLIC schema and then creates a table and inserts into it a value. After the SHUTDOWN IMMEDIATELY I am not able to recreate a connection to the database again. The error that I have obtained is the following:
The program was executed with the Oracle JDK8 on Ubuntu 18.04 using both 2.3.6 and 2.4.0 version of HSQLDB maven dependencies.
If I execute a SET SCHEMA TESTSCHEMA immediately after the ALTER SCHEMA PUBLIC RENAME TO TESTSCHEMA everithing works, as well as if I do not insert values into the db between the table creation and the shutdown.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1542
#1542 Interruption flag is cleared during statement execution
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I encountered an issue with an application that was occasionally leaking active threads from thread pools that were shutdown. I traced this problem back to HSQLDB suppressing interruption flags during its statement execution. I saw this to be a pattern in for example the Session class where Thread.interrupted() is invoked after catching an interruption exception what clears the flag.
As a consequence, a thread that is currently executing a statement will not be able to shut down as the interruption signal is only sent once. If the thread's event loop is checking for the flag to be set, it will have been cleared by HSQLDB. This is breaking the contract of thread interrupts making ordered shutdown impossible.
Is there a particular reason for you to clear that flag? If it should not be set during the session execution, it would be important to self-interrupt the thread before returing to invoking code.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1543
#1543 Exception with ON UPDATE CURRENT_TIMESTAMP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If I try to execute this statement:
I get this exception:
I don't think this is the expected behavior but of course I could be wrong.
For what it's worth if I remove WITH TIME ZONE then the statement works.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1544
#1544 data exception: invalid interval format
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If I try to execute query like
I get exception
IntervalType.getIntervalType() contains nothing about milliseconds. Seconds, minutes or another time field works good.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1545
#1545 bad line numbers in recent jdk8debug Maven artifacts
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm trying to see what's inside prepareStatement(), but with recent Maven artifacts for jdk8 the line numbers don't match. See the attached screenshot.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1546
#1546 MEDIAN on TIMESTAMP Version 2.5.0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Congratulations to Fred and Blaine and HSQLDB team on the release of HSQLDB Version 2.5.0 .
I have noted an item ( bug ) where the documentation does not match real world use.
According to the documentation Chapter 7 Aggregate Functions:
The SUM operations can be performed on numeric and interval expressions only. AVG and MEDIAN can be performed on numeric, interval or datetime expressions. AVG returns the average value, while SUM returns the sum of all values. MEDIAN returns the middle value in the sorted list of values.
While AVG works as described with a datetime expression, MEDIAN returns an error incompatible data type in operation / Error Code: -5563 / State: 42563.
The following code example can be used to re-create the issue, NOTE comment line to have it work:
Sliderule
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1547
#1547 When trying to access a running HSQLDB process, a NPE ocurs
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
Using HSQLDB 2.3.3 (our current production version), there is no problem.
Our server creates a separate java hsqldb process. When our process is abrupdly stopped, it may not have killed the hsqldb process using a SHUTDOWN sql. The java process then runs alone. To avoid problems when starting again our process, we then try to connect to a potentially running hsqldb process. During this attempt, we now always get a NPE while we never did get one before (using 2.3.3). The NPE is below.
Thank you for having a look.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1548
#1548 Error with UNDO and EXIT handlers in procedure creation
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If I try to execute this:
I get this error:
If I change EXIT with UNDO there's a similar error.
CONTINUE works, however.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1549
#1549 Error when overloading procedures
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This executs properly:
However if after that I try to execute this...
...I get this error:
This doesn't seem to be an issue with functions because after succesfully executing this...
...this executes successfully too:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1550
#1550 rs.getObject(x, LocalDateTime.class) returne incorrect values prior to 1582-10-15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
repro code:
Interestingly, if the column is DATE then rs.getObject(1, LocalDate.class) returns the correct date.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1551
#1551 Error with UNDO handler in trigger execution and deletion
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm using r6016.
To reproduce create a new in-memory database with DatabaseManagerSwing.
Then execute these:
I'm not sure if that's an error or expected behaviour.
However with CONTINUE or EXIT instead of UNDO I get this:
Which is what I thought (maybe erroneously) that I would get also with UNDO.
Moreover if I try to delete the trigger:
I get this:
and the trigger isn't deleted.
This I'm fairly sure is a a bug, please note that this only happens after executing the trigger, for example with the INSERT above.
The trigger can be deleted just fine after creation, before it's used.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1552
#1552 EXTRACT ( WEEK_OF_YEAR FROM ... ) does not adhere to ISO 8601 anymore
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Starting with hsqldb 2.5 the statement:
This problem happens, if the Locale.getDefault() is set to en_US. It disappears, if Locale.getDefault() is set to de_DE.
This problem happens on hsqldb 2.5.0 only, it does not happen on 2.4.0 and 2.4.1.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1553
#1553 Invalid exception "numeric value out of range" at converting Double in NumberType
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I want to insert a double value '1.0E+38' to HSQLDB, but I get an error:
When I insert value '1.4E-45', i don't get this error.
I guess method toDouble(..) in NumberType does wrong comparing (see JPG attachment).
I don't know why author of this code wrote such comparing.
I get this error using UCanAccess driver loading access db with such values to HSQLDB.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1554
#1554 Wrong result for quantified ALL predicate
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following query returns NULL as expected:
But for the following query the expected result would be FALSE, but HSQLDB evaluates it to NULL again:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1555
#1555 Minor Doc Error hsqldb.cache_size
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
"Table 13.53. Size of Rows Cached in Memory " contains the following seemingly contradictory text:
The value can range between 100 KB - 4 GB. The default is 10,000, representing 10,000 kilobytes. If the value is set via SET FILES then it becomes effective after the next database SHUTDOWN or CHECKPOINT.
It's 100KB to 4GB but the default is 10KB?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1556
#1556 Function creation before sequence creation in script file causes problems when function references sequence
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If you create a sequence, and a user defined function that accesses that sequence, then the statement to create the function is put before the statement to create the sequence in the script file. This means that, although no errors are reported when creating the script, it will not reopen as the sequence is not recognised.
results in:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1557
#1557 Is hsqldb 2.3.7 available somewhere?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
My dears, it is not effectively a bug, but a doubt.
We need HSQLdb to run with Java7, and, so, we need version 2.3.7, not 2.5.0. Is this version available somewhere? Thank you very much!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1558
#1558 Incorrect domain constraint violation when using 'instead of' trigger.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Please load the attached DB.
If you then execute:
Then it incorrectly reports a not-null violation on a domain 'money', for which there is a derived attribute 'unit_cost' in the view 'order_line_ext'.
However if you insert directly into the base table:
insert into order_line(customer_order, product, quantity) values ('ORD_0', 'PRD_4', 7);
Then the violation is correctly not reported; querying from order_line shows that the value for attribute 'unit_cost' is not-null.
On top of this, the 'instead of' trigger for inserting into 'order_line_ext' simply does the insert into order_line which succeeds if executed outside the trigger.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1559
#1559 Insert INTO .. SELECT .. UNION ALL SELECT..  fills up varchar with whitspace
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
INSERT INTO with select union will fill up all varchars(x) with trailing whitespaces.
Example:
This will end up with a entry for 1 = "Ben "
despite its declared as varchar2.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1560
#1560 Is PreparedStatement.setFetchSize() works or not?
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I get an JVM OutOfMemory error on large dataset select.
PreparedStatement.setFetchSize() seems does nothing. Is it a bug or just not implemented feature in driver?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1561
#1561 Issues with set table source parameters.. (ignore_first, fs , all_quoted etc..)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello there,
It seems text file interaction still requires refinement .. please see below..
Above file is not opened anywhere else. Just FYI.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1562
#1562 Rejected update still commits data
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Please see the attached DB - I'm running in shared mode with
Please try the following:
This correctly gives the error:
This is raised by a procedure called by a trigger fired after updates on ORDER_LINE, which is updated by the 'instead of' trigger for view ORDER_LINE_EXT
But now....
Which is clearly not correct as the update has been committed instead of rejected.
Seems like it's another problem with the 'instead of ' triggers like #1558. If I update the base table ORDER_LINE directly then the problem does not occur.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1563
#1563 The value stored in the Timestamp column differs between INSERT and MERGE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The value stored in the Timestamp column differs between INSERT and MERGE.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1564
#1564 JRT mapping for INTERVAL types is invalid
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
According to documentation, INTERVAL types correspond to Period and Duration Java types.
Java mappings for INTERVAL SECOND and INTERVAL MONTH types are invalid in org.hsqldb.types.Types:624.
Here is the opposite mapping in org.hsqldb.types.IntervalType:152.
My guess is that HyperSQL Code Switcher directives are messed up somewhere and that's what causes the issue.
Minimal example presenting the issue (HSQL 2.5.0, Java 8):
package hsqltest;
Also see this StackOverflow question.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1565
#1565 Recursion never stops
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Consider this recursive query:
When running this on HSQLDB 2.5.0, the recursion never stops until we hit an OutOfMemoryException. It works perfectly fine on PostgreSQL. This query works fine on HSQLDB as well:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1566
#1566 Inconsistent type inference in CTEs with bind variables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I've noticed a few interesting inconsistencies when using bind variables and depending on "type inference" of those bind variables when using JDBC. For example, the following logic yields 12, instead of the expected 3:
At first, I was assuming this is because the bind variables are inferred to be of type VARCHAR rather than INT, so the + operation is really concatenation instead of addition. But the result is very different when I do this, instead:
Now, I'm getting 11 as an output, which I cannot explain. No luck either when casting the bind variables to int explicitly. This yields 2, not 3:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1567
#1567 'duplicate column name in derived table' when using wildcard
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Please create a simple table such as
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1568
#1568 String truncation in derived view column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Please see the attached DB with a text table and a CSV with some large values for certain columns (>32K).
When you do select * from v_view, you get 'data exception: string data, right truncation'. It seems there is an intermediate VARCHAR of length 32K being used in the view calculation which gets exceeded (the same happens if you just do the view definition query on its own).
Is this perhaps related to, as you state in the guide, "a VARCHAR column declaration that has no size, is given a 32K size"? [but there isn't any way of hinting to the view that the derived column will be >32K]
The table column definitions were LONGVARCHAR in the table definition, but I've tried various combinations, and also doing a e.g LEFT(xxx, 1024) for each column as part of the view definition, but nothing seems to work.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1569
#1569 Sequence not found when allocation size is above 16383
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I want to set a sequence allocation size of 20.000 ("twenty thousand") within a JPA entity. The parameter allocationSize will be managed by JPA (eclipselink in my case).
For any reason my tests with hsqldb (v2.5) are failing when the allocation size is greater than 16383. The tests are failing during a sequence restart (see below). Following the documentation (http://www.hsqldb.org/doc/1.8/guide/ch09.html#alter_sequence-section) there nothing special here. Up to 16383 the tests run successfully.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1570
#1570 Document capability of aliasing INSERT tables
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This seems to work (just like in Oracle or PostgreSQL):
But it isn't documented here:
Aliasing DML target tables is documented for UPDATE, DELETE, and MERGE statements, so I'd say this is merely a documentation bug
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1571
#1571 Small typo in manual
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The manual contains a small typo in the syntax example for "OVERRIDING SYSTEM VALUES"
It says "ORERRIDING"
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1572
#1572 Parser error when using FOR SYSTEM_TIME BETWEEN syntax
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm playing around with the temporal table syntax in HSQLDB. Given this table:
I'd like to run a query like this, but it fails:
SELECT *
FROM public.t_sys_time_period
The error is:
The other syntaxes work, including:
As a workaround, this syntax also works:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1573
#1573 trunk rev 6100 breaks scripts with wrong CREATE TABLE ordering
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I need to use the bugfix mentioned in this discussion: https
To this end, I checked out the SVN repo to rev 6100 and built a new JAR from trunk.
However, opening a database, created by v2.5.0, with HSQLDB rev 6100 corrupts the DB script on shutdown.
Somehow the table creation order is wrong. It has certainly changed since v2.5.0!
I've attached a tiny demo database script.
Open this using sqltool & hsqldb rev 6100 then type "SHUTDOWN" and quit sqltool.
Attempting to re-open using sqltool & hsqldb rev 6100 gives me this error:
As you can see from the newly mangled script, hsqldb is trying to create table GROUPADMINS, which has a foreign key constraint on GROUPS.group_id, before creating the GROUPS table.
Sadly I can't seem to get Eclipse IDE to play nicely with gradle and so help debug exactly why this happens.
Good luck!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1574
#1574 Non deterministic INSERT statements generated by ScriptWriterText
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm using the ScriptWriterText class to export databases between computers with differents JREs, and sometimes there is a mismatch between the order of the columns exported in the VALUES part of the INSERT statements, and the order of the column of the database where the script is imported.
I suspect the iteration order of the columns is a not deterministic and may vary depending on the JRE used, maybe due to the use of non-ordered hashmaps?
I've only tested with HSQLDB 1.8, but the 2.x code looks quite similar.
To work around this issue I'd suggest adding the name of the columns in the INSERT statements generated:
INSERT INTO foo (A, B, C) VALUES (value1, value2, value3);
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1575
#1575 Parser error when system_time refers to a variable
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm trying out the temporal tables but have run into a problem trying to use them from within a stored procedure (or function too). In particualr, I want to have the time specification be a variable.
For example, given the following table:
I would like to do this, but it does not compile: "user lacks privilege or object not found: SINCE / Error Code: -5501 / State: 42501"
(Obviously the given procedure doesn't do anything useful).
It seems that the period specification doesn't know about variables. I'm not sure how this is possible since using variables/parameters in the WHERE part of a SELECT works just fine.
Having traced through the code with the debugger a little bit, I think the problem is around ParserDML.java:978 in trunk (r6102). That is, the call to XreadSubqueryTableBody().
This is because, later on, the references to variables are resolved, but the rangeVars (which the input variable belongs to) aren't saved anywhere (except for targets by XreadTargetSpecification() , but since the input variable is not a target, that doesn't get saved).
I am not sure if my hypothesis is correct, or if so how to fix it. I will try to work on a fix and post it here if I come up with one.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1576
#1576 DatabaseManagerSwing doesn't show returned result from PROCEDURE call
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If you have any procedure with a DYNAMIC RESULT SET then this will not be shown in the GUI (even though the procedure will correctly evaluate).
The following patch will show the first result in the table:
I'm not sure if this is written correctly so that it works in all cases but I thought I would put it here in case someone else finds it helpful.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1578
#1578 using clob as a column type will result in a out of mem (or ever growing file)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Sample code below.
Problem is if you are using "clob" as a column then if you insert a clob then that will be at once 32kb of memory or disk usage (depending on mem: or file:)
by itself not a big problem (thats why we do use clobs right? but it would be nice if they only took the amount of memory that they really are, with a few bytes of meta data like length)
But the problem is if you delete the row with the clob, or even drop the whole table, then nothing is clean up, the clob manager (File or Mem based) just keeps growing
If you are using for example mem based and you use hsqldb really as a caching database (so you create temp tables, use that in your app, then drop the table complete) then the memory will grow and grow and you will get an out of mem in the end...
This can be "worked" around by using then the file: so that at least the file will grow and grow.
The test below is based on file because then it is easier to be seen. But mem works the same way (only file is even persistent after restart so you see the file grow and grow even over restarts when there is really no table at all in memory at first start)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1579
#1579 General error when using DISTINCT predicate for row value expressions
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Running the following query produces a "General error"
Caused by: java.sql.SQLException: General error
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1580
#1580 General error with LATERAL and transitive join column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
(Might be related to #1579)
General case, using v2.5.0:
Results in a "General Error" due to col1 inside the LATERAL().
col1 shouldn't be ambiguous because it's the same colum in tableA and tableB, even though it's not qualified with a table name.
Requires tableC.col2 to be primary key:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1581
#1581 General error when table valued function returns values constructor
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Run this script:
This version of the function works as expected:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1582
#1582 ALTER DOMAIN does not update DEFAULT on existing usages
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Consider this script:
I would expect the result to be 1, as it is also e.g. in PostgreSQL and Firebird, but it is null. The default value is not updated on existing columns as it should be.
Likewise, when dropping a default from a domain:
The result should be null, but it is 1. The existing column's default is not affected by the change of domain.
This is not the case when adding / removing CHECK constraints, in case of which the behaviour is as expected. The following script fails with a constraint violation, as expected:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1583
#1583 General Error when trying to use MICROSECOND with DATEDIFF()
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Try this query:
It produces a "general error":
I'm not sure if this is because MICROSECOND isn't supported, or if there's a bug? Trying an arbitrary date part yields a more specific message, though:
Yields:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1585
#1585 Timestamp loses precision when stored and retrieved
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This issue occurs when using JDBC with Java 9+ and HSQLDB 2.5.1.
This does not occur on HSQLDB 2.5.0.
In Java 9+, the Instant precision increased from milliseconds (2020-06-30T13:51:30.875Z) to microseconds (2020-06-30T13:51:30.875297Z).
When storing and retrieving a timestamp with microsecond precision, it is returned with millisecond precision, making it less precise.
The following example illustrates this, when run on Java 9 or greater.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1586
#1586 Performance issues in queries with joins
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I've noticed an unexpected performance issue on a few queries in my system, when they contain join clause.s For instance:
-- tableA has around 3k lines, tableB around 50k lines
This takes around 20s to complete in my test environment (the original query takes over a minute). These tables are not really huge, so 20s seemed a little bit excessive to me.
I think HSQL is using an inefficient join strategy in this case, but I wanted to see what you guys think about this.
On a side note, if you create indexes for both tables, the response time is as expected (even though the amount of data processed in both cases should be roughly the same):
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1587
#1587 Regression when reading timestamp bind value around epoch
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This seems to be a regression introduced in 2.5.1. I haven't seen this in 2.5.0:
This works:
But this doesn't:
The exception is:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1588
#1588 At '@/'  path dereferencing doesn't work in certain situations on Windows
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When running on Windows, @ dereferencing doesn't work when using paths with backslashes in sqltool command-lines, nor with the SqlFile constructions that accept File arguments.
For instance, for a script check-db-setup.sql that includes other scripts via \i @/other-script.sql, this constructor works,
while this does not
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1590
#1590 Bug with selection of multiple boolean constants
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Thank you for great database. We use it a lot for automatic testing of our DB layer. We have found strange issue related to selecting constants together with the actual data from the table. If we select two boolean constants with different names, the result set contains duplicated name. Query:
While this might be bad design for most cases, we use this functionality in a very specific case.
Attaching the image that shows that the result set contains two "WRITE_FLAG" columns. Please note that this is not the IDE bug, since the issue happends during automatic tests built with mvn.
Also, the following query works fine:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1591
#1591 Documentation: Database Limitations unclear
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I can't find any reliable statements about the technical limitations of HSQL.
The current size limit of an HSQLDB database is 8 TB for all CACHED tables and 256GB for each TEXT table. In addition, maximum totall lob size is 64TB.
It's unclear, if the 8TB limit is per cached table or for all cached tables total. The Features page is equally ambiguous:
Disk tables (CACHED TABLE) up to 8TB and text tables up to 256GB each
It's clear that the 256GB limit applies to each text table. But it's ambiguous again, if the 8TB limit applies per cached table or for all cached tables total.
In Data Types and Operations there's no mention of max. length, e.g. of the different character types like VARCHAR.
Please clarify on these limitations and it would be great to have an overview like this one for MS Access
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1592
#1592 DB2 dialect issue with FETCH FIRST x ROWS ONLY FOR UPDATE
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It is a specific DB2 dialect statement which fails only on HSQLDB.
When try fetch limited rows for update it fails on delete such row with message attempt to assign to non-updatable column.
But FOR UPDATE works like expected when is without FETCH FIRST 10 ROWS ONLY
Code which fails:
and exception:
Notice that this statement works perfectly on real DB2 database but our unit tests which are based on hsqldb not.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1593
#1593 Duplicative code
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In the source file SchemaManager.java, the removeSchemaObjects method starting at line 2228 contains two identical back-to-back loops. This does not cause buggish behavior, but it is poor code.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1594
#1594 Sensitive information may be leaked in condlPrint of org.hsqldb.cmdline.SqlFile
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In org.hsqldb.cmdline.SqlFile,
The sensitive information (String s) may be leaked.
We may be able to add control on it.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1595
#1595 Sensitive information may be leaked in displaySqlResults of org.hsqldb.cmdline.SqlFile
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In org.hsqldb.cmdline.SqlFile,
The sensitive information (for headerArray and dsvColDelim) may be leaked.
We may be able to add control on them.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1596
#1596 General error when using TRUNC in GROUP BY
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Running the following script produces a "General error":
It seems a regression in 2.5.1, because with 2.5.0 the script works.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1597
#1597 Sensitive information may be leaked in write of org.hsqldb.result.Result from org.hsqldb.ParserDDL
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In org.hsqldb.ParserDDL,
In org.hsqldb.result.Result,
The sensitive information (rowOut) may be leaked.
We may be able to add control on them.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1598
#1598 Sensitive information may be leaked in write of org.hsqldb.result.Result from org.hsqldb.dbinfo.DatabaseInformationFull
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In org.hsqldb.dbinfo.DatabaseInformationFull{
In org.hsqldb.result.Result,
The sensitive information (rowOut) may be leaked.
We may be able to add control on them.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1599
#1599 Hsqldb should not ignore case of the user name
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In org.hsqldb.Database,
synchronized Session connect(String username, String password,
Java is case sensitive language. Strings "SA", "Sa", "sA", and "sa" are different. Thus Hsqldb should only have one super user "SA" or "sa".
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1600
#1600 In displaySqlResults of org.hsqldb.cmdline.SqlFile, logger.warning(...) should be logger.error(...) to record exception
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Two statements logger.warning("Failed to close SQL result set: " + se) and logger.warning("Failed to close SQL statement: " + se) should be logger.error("Failed to close SQL result set: " + se) and logger.error("Failed to close SQL statement: " + se), respectively.
The reason is that they record exception messages.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1601
#1601 Sensitive messages (URL and username) may leak in getConnection of org.hsqldb.lib.RCData when throwing MalformedURLException
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Sensitive messages (URL and username) are outputted directly and may leak when throwing MalformedURLException.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1602
#1602 HSQL files growing since update to 2.5.1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
since I updated my Software to HSQL Version 2.5.1 from Verion 2.5.0, all Database files grow in an unexpected way. Most db sizes grow from 1Gb to over 14Gb, since the update to 2.5.1, without having significant more data stored.
Most records in this databases get regulary updated by the System.
I am using HSQL in embedded mode.
Is this a Bug or do I have to change db setting that we used in 2.5.0 for 2.5.1 to get rid of this behaviour?
Best regards
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1603
#1603 Queries aborted with timeout long before timeout is reached
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm seeing occasional queries aborted with timeout long before timeout is reached.
Symptoms sound very similar to https which was marked as fixed already.
I'm using Hsqldb via Hibernate using c3p0 for connection pooling.
I'm looking at a log file where the query could have started no more than 3 milliseconds prior to the "timeout reached" message, when the query should have a timeout on the order of 10s of seconds.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1604
#1604 Session.java: unused variable sessionTxId
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Looks like Session sessionTxId is totally not used?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1605
#1605 Session sessionStartTimestamp is assigned but never accessed
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Session.java sessionStartTimestamp has value assigned which seems to never be accessed
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1606
#1606 CREATE TABLE IF NOT EXISTS combined with WITH DATA not working
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I tried to create a table with some data, if this table doesn't already exists.
Tested with: v2.5.1
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1607
#1607 General error if GROUP BY using an alias for a function result
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I'm using the HSQLDB version 2.5.1 as a standalone server instance (later I will switch to the file-based instance).
Like the bug #1596 reported by Stefano Ravera, but more generally, I'm unable to group-by a result of a function call, e.g.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1608
#1608 General error thrown while running SELECT query with group by time period
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi
The following query worked for us with hsqldb version 2.3.1 and even 2.5.0. But starting from 2.5.1 version it fails:
In code it fails due to org.hsqldb.HsqlException: General error
In HSQL Database Manager client I see :
I simplified query and figured out that following query stopped to work starting from 2.5.1 version due to the same error:
I'm attaching screenshot of error and table description.
Thank you
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1609
#1609 CREATE PROCEDURE should not allow duplicate parameter names
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I was trying to test something related to procedures and made a copy paste mistake, where I accidentally re-declared the same parameter names:

What I meant to do is this:
HSQLDB didn't complain about the non-sensical creation, but instead told me this:
The reason being that the procedure was created as if I hadn't declared the OUT parameters:
I think the parser should reject duplicate parameter names.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1610
Variables declared in nested blocks in routines don't work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Consider this table:
And call it as such:
I would expect t to contain 10 records from 1-10, but it is empty:
Trying it a bit differently, avoiding the DEFAULT clause:
Now, this inserts 5 NULL values!
This seems to work correctly, when I completely avoid declaring variables in nested scopes:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1611
#1611 Functions returning a table are returned with DatabaseMetaData.functionNoTable instead of functionReturnsTable
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
For a function declared as RETURNS TABLE(...) DatabaseMetaData.getFunctions() returns the value "functionNoTable" in the column FUNCTION_TYPE of the ResultSet. As such a function does return a table I think the value "functionReturnsTable" would be correct
Tested with HyperSQL 2.5.1 and OpenJDK 11
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1612
#1612 TEXT TABLE connection gets lost
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I use a csv.file for importing data. The text table is attached by
The data base is embedded in LibreOffice Base.
When it happens that in a session the csv-file doesn't exist the connection is lost in following sessions, also if a new csv-file with the given name exists.
The only way is to make a new connection.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1613
#1613 General error when using FILTER (WHERE FALSE) on an aggregate function
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following query produces a "General error" in HSQLDB 2.5.1:
This works:
And so does this:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1614
#1614 Regression in regexp_replace
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I'm trying out HSQLDB 2.6.0, the Thu, 04 Mar 2021 22:14:17 GMT from here:
This no longer works:
It causes a general error:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1615
#1615 HSQL 2.6.0 Release Candidate SCRIPT Error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Fred:
While using my HSQLDB Version 2.5.1 and using the 2.6.0 Release Candidate ( RC ) JAR Version 3 ( as well as prior RC candidates ), when starting the database, the following error(s) result:
I ultimately created a Version 2.6.0 database, starting from scratch, issuing commands to CREATE CACHED TABLES, CREATE FUNCTION, etc, and, INSERT data . . . I was able to create HSQL Version 2.6.0.
I mention the able because the issue is NOT my connection to the JAR or JAVA Version, but rather something in the SCRIPT file. I would be happy to email the database ( a working HSQL 2.5.1 backup file ) to you if you want to investigate it, so, hopefully others will not have a similar issue.
Sliderule
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1616
#1616 Wrong line number displayed with parser error messages
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Consider this wrong statement:
It produces this misleading error:
here is no line 3
2) The token WITH is on line 1
I've run into similar bad line counts in more complex queries, making the error message very confusing.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1617
#1617 CTE cannot be used in some scalar subqueries
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
There are some scalar subqueries that cannot use WITH, others can. Here it doesn't work:
Derived tables also work, as a generic workaround:
Given that the distinction seems arbitrary (especially in the 1 IN (WITH ...) vs 1 = (WITH ...) case, this might just be a parser bug? If it's by design, then consider this to be a feature request.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1618
#1618 UNION ALL or UNION With LIMIT Integer Error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Fred:
Just as an FYI, the issue described here, has been present within HSQL for a long time ( prior versions ), but, perhaps never reported. It is present with HSQL 2.6.0 Beta ( RC6 ) as well.
When using UNION ALL or UNION, when a Select ends with LIMIT INTEGER , for example LIMIT 5 , an error is returned ( Error Code: -5581 / State: 42581 ), however, the statement can be surrounded by parentheses and it will work, OR, use of TOP, for example ( TOP 5 ), parentheses are not required.
For example:
From INFORMATION_SCHEMA.SYSTEM_TABLESTATS
Where INFORMATION_SCHEMA.SYSTEM_TABLESTATS.TABLE_SCHEMA = CURRENT_SCHEMA
I am providing the above example ( not realistic ) so you have an example to work with, after the first UNION ALL.
Sliderule
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1619
#1619 General error when fetching two empty arrays
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Try this in a SQL editor like DBeaver:
It fails with
All of these work:
The error seems to appear only when there are two empty arrays
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1620
#1620 Cannot fetch a constant NULL array without explicit cast
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This works:
Producing:
But this doesn't work
Throwing this error:
The reason seems to be that HSQLDB assumes the type of NULL is VARCHAR, which cannot be converted to an array. But given that I haven't explicitly set the type to VARCHAR, I suspect that it might be possible to delay such a typing decision? I shouldn't have to cast this array, I think...
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1621
#1621 jdbcDriver 2.6.0  is unusable in JRE8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
java.lang.UnsupportedClassVersionError: org/hsqldb/jdbcDriver has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0
I see no artifact with classifier jdk8 or new artifactId with jdk8 suffix.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1622
#1622 JDBC meta on READ_ONLY role returns empty
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I am getting some failing tests since upgrading to 2.6.0
The test is to test a READ_ONLY role in HSQLDB.
as normal user
In 2.5.1 result is true, indicating that the "PUBLIC" schema exist.
In 2.6.0 result is false, which in turn causes the code to try create the schema that then fails as the role is READ_ONLY. However the 'PUBLIC' schema does exist.
Not really sure if the bug is in the JDBC driver or not. Using Intellij's database tool I can browse the 'PUBLIC' schema using the 'sqlgReadOnly' user. Trying to make any change fails as its READ_ONLY.
Thanks
Pieter
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1623
#1623 Odd behavior for nvl2 in 2.5.2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello, I just tried upgrading from 2.4.0 to 2.5.2 and noticed a change in behavior for the nvl2 function that was quite unexpected:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1624
#1624 Insert statement for BLOBs broken.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Insert statement for BLOBs broken.
I have upgraded from 2.5 to 2.6 and now insert statements are broken.
The insert will now end up in this:
Error 'incompatible data type in conversion' while executing statement 'INSERT INTO IWA_TaskData VALUES (1, '00' )'
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1625
#1625 Validating a Hibernate Schema causes a org.hsqldb.HsqlException: General error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
A Spring Boot application that sets the property spring.jpa.hibernate.ddl-auto=validate will run the following SQL on startup (note - I have created tables within the schema "WEBSITE"):
Versions 2.6.0 and 2.5.2 throw an exception on the last line of the above SQL.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1626
#1626 2.6.0 often get Timeouts during selects
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
With HSQDB 2.6.0 we often get
java.sql.SQLTransactionRollbackException: statement execution aborted: timeout reached
Exceptions for normal select statements. The Timeout seems to come after only 2 Seconds.
Possible the 2.6.0 driver does not use the set timeout value?
When we rollback to 2.5.1 this Timeouts do not happen.
We are using the HSQLDb in Embedded Mode.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1627
#1627 Constraint violation exception somtimes in mvcc mode
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I have some transactional problem when set hsqldb.tx to mvcc. I can not reproduce it all the time but sometimes it occurs.
Scenario
I have two tables A and B.
B has a foreign key to A with column X.
This all happens really fast ("under high pressure). Sometimes it works. Sometimes not.
Does anyone has any clue about this?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1628
#1628 numeric value out of range error on basic arithmetic
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I get a data exception: numeric value out of range error (SQL Error: -3403, SQLState: 22003) with a very simple SQL queries:
This fails:
Version used: 2.6.0
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1629
#1629 like expression does not allow addition expression as its right expression
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
This query does not parse:
addition expression should have a higher precedence than like, so Hsql should evaluate '%' + 'PUBLIC' + '%' first then apply it to like expression.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1630
#1630 Release 2.6.0 is missing OSGi metadata
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The release 2.6.0 is missing the OSGi metadata in the manifest. Both 2.5.1 and 2.5.2 are still properly including it (see META-INF/MANIFEST.MF).
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1631
#1631 null value causes NPE in JDBCResultSet.getObject
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
JDBCResultSet.getObject(int columnIndex, Class<t> type) causes a NPE if the value in DB is NULL.
Worked on 2.4.1
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1633
#1633 timeout reached HsqlException thrown when using setQueryTimeout
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi
Starting from version 2.5.2 we are trying to use method setQueryTimeout for PreparedStatement, timeout value provided in seconds, I tried to pass 15, 40 or even 60. But anyway I get following exception:
If query timeout not defined then any exception is not thrown. When I measure how much time such queries take I get maximum 100-110 msec, so always less than 1 second. I tried also to provide large number of seconds as a timeout, so in the case of 500 seconds exceptions were not thrown. Stack trace of exceptions and queries are attached.
It seems me that seconds are considering as a msec.
Thank you,
Evgeny
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1634
#1634 org.hsqldb.HsqlException: statement is invalid thrown while deleting table rows using prepared statement
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following exception happens after upgrading from 2.3.2 to 2.5.2 ( full stack trace is attached ) while deleting data from tables, probably happens when there are thousands of rows that must be deleted:
We are reusing PreparedStatement instances, but there is no access to the same instance at the same time, regression is after upgrading to 2.5.2.
Thank you
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1635
NoSuchMethodException (invokeCleaner) on Android
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
We're using HSQL on embedded Android tablets running Android 4.2 (API level 17) and Android 8.0 (API level 26). After upgrading to 2.6.x I noticed the following error in our logs when we perform a CHECKPOINT DEFRAG command:
This error occurs because the Java version detection in JavaSystem.java doesn't work correctly on Android platforms:
returns "0.9" on our tablets and some random phones I tried it on. This causes the javaVersion variable to default to 11.
To fix this, I slightly modified the existing code in the "unmap" method. When the javaVersion is > 8, then the "sun.misc.Unsafe" code branch is tried first, but it falls back to the existing code branch should a reflection error occur:
I'm perfectly happy with maintaining my own branch if this is too obscure, but I figured you might be interested in this.
Cheers!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1637
#1637 Unique Key constraint cannot be dropped
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1638
#1638 getObject return value influenced by getting previous column
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When getting a primitive type column that was NULL, the wasNullValue of the JDBCResultSet is set to true, which is correct. However, wasNullValue then takes effect for future calls to getObject(columnNumber, type) before actually checking the data in the column in question. This gives the surprise result that a NOT NULL column could return a null value. This effectively breaks the invariant on the column.
This is confirmed in 2.6.0 and 2.5.2, and does not affect 2.5.1. I have not tested trunk as we use maven dependencies for hsqldb.
This bug appears to be related to #1631 "null value causes NPE in JDBCResultSet.getObject", and I suspect that the fix for that issue will also resolve this bug. However in my view this is top priority; it's not possible to rely on returned data in any situation where getObject is used after any other nullable column, with an NPE being the more preferable outcome, but in many cases the symptoms may be far more insidious with nullable columns ignoring actual values. Not all users may realise they are experiencing this bug, as a result. Therfore 2.5.2+ has a very high priority bug and a fix should be pushed out to 2.5.x and 2.6.x to maven central to minimise the impact on users, IMHO.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1639
#1639 Condition does not work for CTE WITH RECURSIVE .. WHERE ... IN (SELECT * FROM unnest(ARRAY[UUID('...')])...
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi guys!
I have built the graph query with a loop prevent condition.
The problem is 'in array' condition doesn't work during CTE WITH RECURSIVE.
But works fine outside CTE or on stand-alone tests;
Also query works fine on PostgreSQL with correct output.
Arrays contains uuid type values.
The problem is located on that line:
I hope it will help to perpoduce and fix the problem
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1640
#1640 Changing table from CACHED to MEMORY does not work
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If using a file based database it seems it is not possible to change a table from cached to memory.
Creating the table as memory and then changing it to cached works fine.
A "General error" exception is thrown with SQLState = S1000 and vendorCode = -458.
The problem exists both in 2.6.0 and 2.6.1 but not in earlier versions.
The problem does not occur is using memory database.
The problem is reproduced in a simple Spring Boot project using a junit test here: https://github.com/nytro77/hsqldb-cached-to-memory-bug
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1641
#1641 Issues with timestamps in JPA named queries
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi all,
We're seeing strange issues when executing queries via JPA using HSQLDB 2.5.2.
These queries perform null checks on incoming placeholder values. This causes data type conversion issues when the incoming placeholder is a SQL TIMESTAMP data type.
These queries worked when using HSQLDB 2.5.1.
Broken query example:
Calling code:
This query results in an HsqlException: "incompatible data type in conversion"
This query worked as expected using HSQLDB 2.5.1, but this query fails under 2.5.2, 2.6.0, and 2.6.1.
The query works when removing the :date IS NULL condition from the WHERE-clause.
I've attached an example Spring Boot project which demonstrates this issue.
This can be run using mvnw spring-boot:run using JDK 11.
Full stack trace of the error when running the query:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1642
#1642 java.sql.SQLSyntaxErrorException: user lacks privilege or object not found
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1643
#1643 HSQLDB fails to compile query with complex conditions
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The following schema:
along with the following query:
fails with this exception:
Interestingly, if the last where condition is omitted, the query works. It also works if the "is not distinct from" predicate is replaced with a "=" predicate. I also tried to use "decode" or an emulation that involves an intersect exists subquery, but all approaches to execute such a query failed so far.
The stack trace is from executing this with 2.3.6 as newer versions report just a "general error" without any context.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1644
#1644 Push JDK8 compatible jar to Maven Central
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It would be nice to have the Java 8 compatible jar pushed to Central. The current coordinate org.hsqldb:hsqldb will not run on Java 8. Specifically, the error follows:
Error: A JNI error has occurred, please check your installation and try again
I'm currently leveraging a workaround using a file:// repo in my project. My coordinate for this solution is org.hsqldb:hsqldb-jdk8:2.6.1. Your free to borrow that. :)
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1645
#1645 CLOB value saved truncated when encryption is used
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
After upgrading to 2.16 we faced the issue that CLOB fields are saved corrupted (truncated) when flushed/persisted to the file.
Environment:
Steps to reproduce:
1. Configure JDBC connection string with blowfish encryption and lob crypt:
2. Configure @Entity with @Lob String filed and explicit size more than 512 KiB (otherwise default 255 symbols length would be applied):
Create Spring CRUD repository for this entity.
Save value with large string (about 1 MiB) to the repository
Force writing to disk (you better know how to do this, i waited 1 second, also stop application persists to disk)
Read persisted value and compare saved String with the original one
Actual Result:
String is truncated to 512 KiB
Expected Result:
String is not changed
Note:
pls follow the PersistenceConfiguration, test the behavior in CorruptedDataSavedExample (Spring Boot Application) and provided test classes.
Pls contact me in case of any question - happy to collaborate with you.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1646
#1646 Can't use SHEMA other than PUBLIC in 2.61
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
With hsqldb 2.61:
If I create a schema, a simple user can't see table and view.
If I create user with ADMIN option: user can see table and view but can't use the set default schema Alternative: user see all table and view.
If I use 2.51, all work fine....
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1647
#1647 NoSuchMethodError when using org.hsqldb.jdbc.JDBCClobClient.getAsciiStream() from alternative jar (Java 8)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Due to incompatible changes in java.nio.ByteBuffer between Java 8 and 11 (methods clear() and flip() of class java.nio.ByteBuffer) calling org.hsqldb.jdbc.JDBCClobClient.getAsciiStream() from alternative jar using JDK8 throws a NoSuchMethodError. The issue as well as resolutions are described at
In order to reproduce the issue compile and run the following piece of code using JDK8:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1648
#1648 org.hsqldb.jdbc.JDBCClobClient.getAsciiStream() returns an input stream with trailing ASCII control character 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It looks like there is an issue in org.hsqldb.jdbc.JDBCClobClient.getAsciiStream() since it returns an input stream with lots of trailing ASCII control characters 0. I.e., an input stream created by calling java.sql.ResultSet.getAsciiStream(int) has different content than an input stream created by calling org.hsqldb.jdbc.JDBCClobClient.getAsciiStream() on the result of java.sql.ResultSet.getClob(int).
The issue can be reproduced by running the following Unit test:
I have tested versions 2.5.2 as well as 2.6.1.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1650
#1650 Column altered with set default  nextval() causes org.hsqldb.HsqlException: General error
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
versions:
description:
create liquibase script kind of:
When test is finishing, liquibase cannot execute remove script
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

