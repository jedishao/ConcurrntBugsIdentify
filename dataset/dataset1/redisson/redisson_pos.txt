Locking on Jersey.I'm trying to use Lock and Unlock on Jersey Resource.User makes a POST request.System LOCK "A".System does some stuff.System UNLOCK "A".User makes another POST request.System LOCK "A".System does some stuff.System UNLOCK "A".The system crashes at point 6 (view attachment).If i try to make some LOCK-UNLOCK in a while loop it works, but when i make these LOCK-UNLOCK from different Threads it does not works.
Lock TTL.Any plan to add TTL to a Lock operation? Don't confuse with tryLock with TIME.I refer to a situation where a thread is dead and leave a resource locked (deadlock).
Deadlock while obtaining lock.Thread gets stuck while obtaining lock. Running 'keys L*' in redis-cli returns an empty list.The same lock name was likely concurrently obtained and held by another thread possibly on another jvm, and then released.Redisson version used: 1.1.5.
Lock reentrancy race.At line A the lock key is set only if it doesn't exist (NX), but on line B it's set assuming it still exists. If the lock timeouts between A and B, another process may obtain the lock, which is then overwritten at B.Here's a test case:This will reliably fail if:run in debug mode with a breakpoint placed in RedissonLock.tryLockInner(long, TimeUnit) on line B (line 306 for 508b903).the breakpoint is released after waiting at least half a second.The same problem is also present in the no-args method RedissonLock.tryLockInner(). (though a race condition is possible only if upgrading an expiring lock)I wonder if reentrancy support shouldn't be done purely in java.
RedissonLock locks up.Under heavy use on production, Redisson's locks get all locked up, and the application stalls. I'm using Redisson 1.1.5.I have 1 thread locked trying to release a lock:Also of note, I have about 30 other threads locked awaiting for a lock (a different one from the one used by the previous thread).I checked the threads with jstack, here is the relevant output:I checked Redis's state:this key is the one corresponding to the thread blocked trying to release a lock. The other threads, that are waiting for a separate lock are locked even though there is no-one taking up such lock....
when thread is interrupt...all Reference variables is locked!!the redisson is locked!all Reference variables is locked!!
java.lang.ClassCastException  java.lang.Integer cannot be cast to org.redisson.RedissonLock$LockValue.when use lock,but throw some class cast exception,sample code :error stack:
Use issues ----lock(long leaseTime, TimeUnit unit)when i run the method.com.lambdaworks.redis.RedisException: ERR wrong number of arguments for 'set' command.How to solve this problem?
lock.lock(2, TimeUnit.SECONDS)RLock lock = redisson.getLock("anyLock");lock.lock(2, TimeUnit.SECONDS);lock.unlock();run it ,report error:
lock.lock(2, TimeUnit.SECONDS) report error ERR wrong number of arguments for 'set' command.RLock lock = redisson.getLock("anyLock");lock.lock(2, TimeUnit.SECONDS);lock.unlock();run it ,report error:
[BUG]Thread dead lock when using distributed lock on 1.2.1.I run into a thread dead lock problem when using distributed lock on 1.2.1:After a certain period, some threads will be dead lock, and we can tell this by VisualVM's thread dump:
Thread dead lock bug when using distributed lock on 1.2.1.I run into a thread dead lock problem when using distributed lock on 1.2.1:There are ten threads call this function like this in a thread pool:After a certain period, some threads will be dead lock, and we can tell this by VisualVM's thread dump:I don't know why, is it a bug?
ConnectionManager call hangs forever if exception is thrown during Command processing.Bug found that can cause MasterSlaveConnectionManager to hang forever on get() call if exception is thrown anywhere in CommandHandler.To replicate the bug, you can use RedissonMap with JsonJacksonCodec to put instance of class that doesn't have default constructor. When you try to fetch that object by using RedissonMap.get() call, deserialization of object will fail in MapOutput because of missing appropriate constructor and thread calling RedissonMap.get() will block forever.In more details, this is happening because get() method awaits forever on Future object, which is released when Command.complete() is called. This complete() call is executed in decode() method of CommandHandler after RedisStateMachine processes Redis response. If, for example, RedisStateMachine throws an exception, complete() won't be called and result/exception will never be set to the Future object. This is causing calling thread to block forever in MasterSlaveConnectionManager.get() method.Pull request with test case that is proving this bug and bug fix proposition will be published shortly.
Issue in locking on key in concurrency.In my project, I have a servlet, that call a utility class.Utility class calls a wrapper class that  I have implemented over redisson lock.Wrapper class holds RLock object for a thread and provides lock and unlock methods that call RLock's lock and unlock method.But while running a apache ab-test tool for concurrency, Following exception occurs.Can you tell me if I am missing something over here?
Opening new connections in org.redisson.CommandExecutorService#async.Rmap.addAndGetAsync (and any *Async methonds) - as i understand it shouldn't lock the calling thread, and return value (Future) as soon as possible.But if for any reason connection is not established yet the calling thread will be blocked.And if redis server is not available at all it will be blocked for very long period of time.Maybe i'm wrong but it doesn't look like async nor lock-free.
RLock.tryLock() thows Exception.org.redisson.client.RedisException: ERR Error running script (call to f_2d027cdc209d32fe7dade9ba284110f88d497180): @user_script:1: WRONGTYPE Operation against a key holding the wrong kind of value . channel: [id: 0x24baaaa8, /192.168.99.1:64875 => /192.168.99.100:6379].ENV: redis 3.0.3/2.8.3.REDISSON: 2.1.1/1.3.1.
EOFException when I use RLock with SerializationCodec.I configure Redisson to use SerializationCodec instead of default JsonJacksonCodec. Then I run my code in environment with concurrent threads and use Lock object to sync thread. After that I get exception and unlock only after expiration in 30 sec. Previous major version of Redisson does't contain this issues. Similar problem I have when I use CountDown.
RLock.isLocked() get hung when I disable/enable my local network.I was crashed into a blocking issue when I was doing some configuration on my windows laptop. RLock.isLocked()  got hung forever with callstack as following:Callstack:The issue is repreduceable. Steps to reproduce the issue:My os is windows, and I haven't tested it on linux.Steps: disable network.  Then enable network. The program will be blocked with the callstack I pasted.
RLock did not work.It's very wired. I just run a very simple test case to try redisson lock. But the result is not good.Can anyone show me why RLock didn't work?The results are as below. It seems the lock failed completely.
ElasticacheCluster not working correctly with DistributedLocks.I am trying to use redisson 2.2.7 for distributed locks. I have a 3 node Elasticache cluster with one of them as master.When I try to get lock using the following code, i get an error saying that i cant write to a Slave. My assumption is that i can feed a list of nodes to the redisson client as Elasticache nodes and it will figure out who the master is...is that not true?The error that i get is shown below..Here is how i am setting up a redisson client.
Read Write lock cannot be correctly unlocked.in ReadLock and WriteLock,  the unlock method use thread id to determine whether it is owned by this thread. it works fine for a single machine.However, when it is used in a distributed processing framework such as MapReduce, the Read and Write lock cannot be correctly unlocked.I browse the source codes and find that the UUID is generated in the lock, I think it is better to use UUID as lock id  istead of thread id or provides a way to let users to set a unque id.
Deadlock using RedissonMultiLock.I am using simple app to run on 2 clients to test locks.While using RedissonMultiLock with 3 locks both clients blocks.
Error if first node is down in 3 node Master-Master cluster.I have a Master Master redis cluster of 3 (node1,node2,node3). Locking mechanism works fine if we block the network for node2 or node 3. But if we block node1's network then it fails with the below error.Below is how I'm creating the Redission client.
Rlock Exception in cluster mode.Hi.I am using redis 3.0.6 (4 node cluster  ) and redisson version 2.2.5.Sometimes when I try lock a key I get the following exceptions:my code :
Rlock performance issue.Hi.I am using reddison Rlock with a cluster setup , and sometimes I see latency( up to 1000ms)  when trying to acquire the lock or unlock.I saw this issue opened by zhxjouc (#455) with a similar problem  and I am working with 2.2.13 but I am still getting latency when a thread is trying  lock a key.My code is running with Java thread pool for accessing redis, I notice that if I work with pool of size 1-2.almost no latency when getting the lock, but working with 30-50 threads cause the lock delay.It could be thread overhead issue but I think that 1000ms is too long for that.Any help on how can I get better performance when locking an unlocking?.Thanks.
Dead Locks Happen in lock() Process.when heavy concurrency happens in my application, a few lock requests will "sink" without any responses, even after the lock lease time has passed. All of these requests wait at RedissonLock.lockInterruptibly().The exact position is RedissonLock.get() after RedissonLock.subscribe().In my opinion, this may be due to a thread removes the netty listener which is used by another thread. It can happen in this way:Thread A is in the loop of getting the lock after subscription.Thread B has also applied subscription and waits for result.Thread A gets the lock very soon and enters  RedissonLock.unsubscribe(). In this step, it possibly removes all the listeners on the same channel, which includes the listener used by Thread B. It causes Thread B can never get subscription response and hang on forever.The similar issue is at http. But I think it is not solved completely.Also I suggest  to apply the ttl algorithm to RedissonLock.get() because this step can cost some time. And if it has a timeout, dead lock can be prevented in a work-around way.This is the thread dump when dead lock happens:
Asynchronous lock release.Hi,I'm trying to create an infrastructure where different machines acquire shared locks through Redisson. Once the lock is acquired, some async tasks gets done, finally, when I finish the job, I'm releasing the Redisson lock through the thread currently running - but i receive the following error.java.lang.IllegalMonitorStateException: attempt to unlock lock, not locked by current thread by node id: xxxxx thread-id: 57.So, I understand the meaning of that, but since I want to perform asynchronous work, I cannot use the acquiring thread to perform the release.Is there a solution for asynchronous programming? Should I not use Redisson Lock?I also references the issue on SO.Thanks.
Deadlock on lock() and not only.Hi,I want to raise the issue with deadlocks again.This bug is still exist and making big headache. As before, It present itself only on very heavy loaded tasks, but in this case I is happens only when client talks to the claster which is located remotely.With a single-local or claster-local servers I was unable to reproduce it, but with remote server it happens with rate 1 / 20 (means from 20 runs of "heavy-load" JUnit test it happens only once)I can see where thread is locked down, it always stuck in CommandAsyncService.get(Future), on l.await() line and never exits from it. As I understand something wrong with mainPromise object, it is staying in incomplete state... and nobody change it. I tried to understand the logics in CommandAsyncService.async(...) function, which is actually deals with connection, retry, redirections and at end should release (or fail) the mainPromise object, but it is nightmare. All these spagetty with promises and futures made the code difficult to read and impossible to analyse. For sure BUG is there, but I am near to give-up. Any thoughts?
Exception in using RedissonMultiLock.I have 5 independent masters and I use single server config to create Redission instance.The RedissonMultiLock works fine if all the redis nodes are alive.If I shutdown one of the redis nodes, it will throw RedisConnectionException.Exception in thread "main" org.redisson.client.RedisConnectionException: Can't init enough connections amount! Only 0 from 5 were initialized. Server: /192.168.223.128:8000.According to The Redlock algorithm, it should try to lock another node but not throw exception. Do I use RedissonMultiLock correctly?It tries to acquire the lock in all the N instances sequentially, using the same key name and random value in all the instances. During step 2, when setting the lock in each instance, the client uses a timeout which is small compared to the total lock auto-release time in order to acquire it. For example if the auto-release time is 10 seconds, the timeout could be in the ~ 5-50 milliseconds range. This prevents the client from remaining blocked for a long time trying to talk with a Redis node which is down: if an instance is not available, we should try to talk with the next instance ASAP.
RLock trylock blocks forever.Found this in 2.2.16, seems like this was not around in 2.2.10. But still verifying.When tryLock is called with 0 wait time, the thread blocks forever. I took a thread dump and it looks like the stack pasted below.A note on the environment: We are running this against an Elasticache cluster in AWS and accessing from 4 EC2 instances. We have seen a lot of command timeouts. I am not sure if that is some way leading to this.
RedissonLock.isHeldByCurrentThread() doesn't check properly.I am using RedissonLock in my system.The lock realted logic in service is like this:But in the log I found:I can see the the thread name is same, so the thread got the lock and the thread try to unlock is same thread.Is there any problem in my code? Or in the logic of lock.isHeldByCurrentThread()?
loop lock blocked when master-slave failover.description: after master-slave failover , the test code blocked forever .redis cluster :test code:exception:
attempt to unlock lock, not locked by current thread by node id.redisson version:2.2.21.implement code:
tryLock method throw Exception:attempt to unlock lock, not locked by current thread by node id: 157ddcf5-0cc2-445d-bdea-98160a459bd5 thread-id: 42.@mrniko.hi, I stress test tryLock(long waitTime, long leaseTime, TimeUnit unit) method, throw Exception:attempt to unlock lock, not locked by current thread by node id: 157ddcf5-0cc2-445d-bdea-98160a459bd5 thread-id: 42.unnormal code:1:but use lock() or lock(long leaseTime, TimeUnit unit) method is normal, why?2:tryLock() can not be block? lock() is blocked?3:tryLock(long waitTime, long leaseTime, TimeUnit unit) method, waitTime must be more than leaseTime value, otherwise occasionally have "attempt to unlock lock, not locked by current thread by node id" ecxeption, in stress test condition, why?
redisson lock failure problem.redisson version:2.2.24.@mrniko.hi, i user jmeter pressure measurement  redisson lock.scene :Pressure test scenarios:Using two tomcat, simulating the two service requests, each request the server queue to obtain a lock, concurrency of 5000, continued request for 20 minutes,After acquiring the lock counter, increments by 1. It appears six times double counting problem, log is as follows::A server.lock and unlock code.
rlock error report.
RedissonRedLock & RedissonMultiLock lock method stuck.If more than 3 locks are supplied to RedissonRedLock or RedissonMultiLock instance then lock method could stuck.
RedissonRedLock trylock success while another thread already hold the lock in specific conditions.it's reproducible, my code is below.redlock lock result will be success, but when I check redis, I found the multi locks belong to different thread.like:I did more test and found:if the single lock is the first of redlock, it's ok.when I move multilock2 to first position ,it will return false correctly.the code below.I am wondering if it is a bug or not?
CommandAsyncService blocks indefinitely.Hi,I have a thread stuck in the CommandAsyncService#get() indefinitely waiting for the CountdownLatch. I don't have a particular repro case but this happens every once in a while on our servers (under load). Redis itself is still delivering events and the instance receives objects on other threads as well.Would it make sense instead of waiting indefinitely on the latch to only wait as long as the timeout is configured (as a safe belt) and abort the action if there hasn't been any success/failure by then?Cheers,Philipp.
DistributedLock - Lock is getting acquired by multiple threads.We are using distributed lock to ensure that only one thread can take a particular action, among a group of threads spread across multiple EC2 instances.Here is a snippet that represents what we are doing.The issue is that we are seeing multiple threads from being able to acquire and do the job.What might be the issue? Do you see an error in the way we are using distributed locks...is there a better way to solve this problem?Our goal is to ensure that for every job run...only one thread gets to do the job.
Documentation on locks.I was wondering if the documentation could be clarified on what happens to each of the lock types should one redisson node request a lock, then die before releasing it while another node is waiting on it.i.e.If the documentation could also note suggested ways to prevent deadlocking in such cases (should deadlocking occur) that would be good too. It's not clear if the distributed locking takes into account the loss of a node while holding a lock.
Unlock does not really unlock.If you unlock a FairLock from some thread, there is a delay until all threads actually see that lock as available.Example test clase.
RReadWriteLock is not reentrant.I was expecting the RReadWriteLock to be reentrant, as the wiki page describes it as (emphasis mine):Redisson distributed reentrant ReadWriteLock object for Java.However, in my simple test:The "reentrancy" clause of the ReentrantReadWriteLock Javadoc states:Additionally, a writer can acquire the read lock, but not vice-versa.My use of redisson relies on these locks being reentrant (at least within the same Thread), however this does not seem to be the case - is this a bug, or a configuration issue on my end?
Problem with Rlock - unlock not releasing lock to waiting threads.I am seeing an intermittent issue with Rlocks. I am implementing what is basically a distributed cyclic barrier using Redisson.I am running multiple instances of a service on different virtual machines. Each instance receives multiple HTTP REST api requests, handled by multiple threads. As each request is received it waits at a redisson count down latch until the threshold for # of requests has been reached.At this point, I need one thread to calculate the results and store it in redis so it can be available to all the other threads across all instances. So each thread tries to acquire an Rlock (with TTL of 5 seconds). One thread gets the lock, does the calculation, and stores the result back to redis. It then releases the Rlock.Now all the other threads that are waiting on the Rlock can (one at a time) acquire the lock, see that the data is already in redis, and simply release the lock and send the result back via HTTP, without repeating the calculation.We are using a master/slave redis cluster with 1 master and 2 or more slaves. As well as using redisson for the countdown latch and rlock, we use jedis for standard redis reads and writes, but never using the same keys as we use for the latch and locks.This is all working about 99% of the time. However periodically, I can see that after the Rlock is released, none of the threads that are waiting for it are able to acquire the lock. I was originally using lock(), and finding that HTTP threads were hanging forever. I switched to using trylock, and this returns, but it returns false, indicating a timeout waiting for the lock.I have added logging to be 100% sure that unlock is being called on the lock in the failure case. Any suggestions on how I can debug this issue, or any known problems with our configuration that might be causing it?We are running Redisson 3.2.3 and redis 3.2.5.Thanks for the help,Sue.
Indefinite lock lost during master failover.I've found my indefinitely held locks will sometimes disappear after a master/slave failover.  The block here will not reschedule a renewal if the attempt of the update fails.The update can fail during a failover, in which case the lock is gone for good.The exception that gets thrown when the update fails:
CommandAsyncService blocks indefinitely [without OutOfMemoryError].all application threads are blocked with stack trace.
RReadWriteLock is incompatible with reentry r&w op.While using RReadWriteLock, a bug (or not?) confuse me for a long time.I described a wrong ops yesterday, and fix it now. Sorry about this.There is a sure logic problem if ops like this:So if there is no holding check and running in a concurrent environment, op4 may throw IllegalMonitorException.But it's ok as follows:I found that "read unlock" will delete the whole lock without checking if there is any other write lock.Is it a bug, or it just shouldn't do and need to do in another way?
Cache ops taking too long.Currently i'm using Redisson 2.7.4  as JCache Provider, and some cache operations are taking a long time to execute, as can be seen in the log below:in some cases it takes ~30 minutes.Taking a thread dump I noticed that all EJB async threads (and some http-executor threads too) were in the same state, generating a huge queue in the async thread pool:Thread dump for some threads (Complete thread dump can be downloaded here):it looks like that is a lock issue, but i don't know how to solve it.
Concurrency Issues.Today I tried some tests to experiment with Redisson's performance using a build from /master and discovered significant concurrency issues. I'm not sure what the state of this code is, so perhaps these issues are known ones, but wanted to make sure you guys are aware of them.I have a very simple app that a) Creates a Redisson instance with a connection pool of 50 to masters and 50 to slaves, then b) creates a quick thread pool using Executors.newFixedThreadPool() and then c) feeds it Runnables, each of which generates a random string and adds it to the end of a Deque.Whenever I do any of these three things: 1) Add >1 millions of queued Runnables or 2) Increase the number of threads >20 or 3) add a .contains() call on the Deque (more on that in a sec), I get a ton of exceptions that don't include any of my code in the stack trace. Here's one such snippet:
RedisConnectionException in High Concurrence.I use redis as required data store, but in high concurrence, there are almost 50% connection failed. I want to know whether to optimize my redis server(use cluster) or my client? Is there any great suggestion? infinity retry until success? And My test code are list after:I use 1000 threads, and find that if i set the connection pool size to 500, the successful proportion achieves maximum.
RedissonSortedSet's order seems broken.Hi, we have been using RedissonSortedSet and just found that on certain cases the order can be broken and I assume RedissonSortedSet is supposedly thread safe.Assuming RedissonSortedSet of integers are added with the following integers : 103, 101, 102. RedissonSortedSet state (list in the redis server) should be: 101, 102, 103. However, there are cases when it is not. It may become 102, 101, 103 instead. And looking at RedissonSortedSet's source code, it uses binary search that requires the list/state to be always sorted. Therefore it breaks many other functionality.The use case that breaks (in my case) is when we are adding multiple items and there is another thread that delete an item in between. Looking the source code implementation briefly, it may be because when removing an item it does not consider getCurrentVersion() (just a quick guess).
Redisson map get makes the thread waiting forever.The map is a regular <Integer, String> map, and by calling a get on some key we see the following thread trace:
Incorrect RedissonRedLock.tryLock behaviour.Lock could acquired by several threads at once when leaseTime parameter less than waitTime parameter passed into RedissonRedLock.tryLock.
performance slow, when multiple thread.I use 1000 mutliple thread to test the list operation. the costTime is average 1000~2000ms. But when single thread it costs 1ms.
StackOverflowException in URLBuilder.URIBuilder is not thread-safe. Following scenario causes infinite recursion and StackOverflowException when someone tries to create new URL:URLBuilder.replaceURLFactory() - URLBuilder.currentFactory set to null, URL.factory set to custom.URLBuilder.replaceURLFactory() - URLBuilder.currentFactory set to custom, URL.factory set to custom again.new URL fails on StackOverflowException. Following code is an infinite loop because currentFactory points to "this":Affected version: 3.3.0.
RedisNodeNotFoundException : Never catch, so never release the countdownlatch.Hello,I have a cluster 3 master and 3 slaves, freshly created.You can see the conf in config dir and the creation script is init_and_launch_cluster.sh.I launch a Java junit test which use Redisson.The test initialise redisson (with a scan interval set to 100 ms in order to produce the bug quicker),and insert key into the redis cluster in 15 thread.I launch the kill-random-redis-master_local.sh script. This script kill a random master, wait 15s (nodetimeout is 10s),relaunch the killed master, wait 10s, and start over.In generally 10 minutes, I have this exception :I can't understand why there is this exception, but it is not catch anywhere.So the thread which handle this command is stuck in await of the countdownlatch (CountDownLatch:148)After 15 RedisNodeNotFoundException, all my insert thread are stuck, i can't insert any more.I made a little project on github, to help people reproduce this:Thanks for you're help.Ps :  In pj you found the log of the last reproction i made (redis 3.2.6 and redission 3.3.0)logs.tar.gz.
URIBuilder seems not to be thread safe.We test this in case of a multi cluster setup and sometimes went the java.net.URL.factory to null instead of the original factory.Best,ebersb.
RedissonList's Iterator Race Conditions.RedissonList iterator as it tries to keep "up to date" with data has a race condition in which if between the .hasNext() and the .next() call the set is emptied the list will throw NoSuchElementException.Unit Test.Additional Information.This also effects RedissonSet, but much less likely to hit the race condition. It causes RedissonSet line 96 to throw a null pointer exception.This is akin to issue #104.
Report possible race issues.Hi, Developers of mrniko/redisson,I am writing to report two race issues on use of ConcurrentHashMap. The issues are reported by our tool in an automatic way. Although manually confirmed, they would be false positives, given we do not know the specification of the program. We would very appreciate if you could check below for details and confirm with us whether they are real problems. For more information, please refer to our website:Line 120 is in the synchronized block on "ENTRIES". If the intention is to guarantee exclusive access for the !containsKey checking on "ENTRIES" and to ensure the atomicity of the checking and unsubscription (line 120,121), then the write operations on "ENTRIES" in line 66, 115, 117, 136 may break this. Relying on the ConcurrentHashMap to ensure exclusive access is dangerous since ConcurrentHashMap has no guarantee of exclusive access.
Redisson shutdown hangs if redis server was down.Start redis server.Create redisson bean.Stop on a breakpoint inside test.Shutdown redis server.call redisson.shutdown()What I see is that ConnectionWatchdog is scheduling tryReconnect indefinitely incrementing attempts and connection is not closing. But if I put break point in ConnectionWatchdog#channelInactive (line 64) and stop there, connections are closing properly. Perhaps there is race condition somewhere?
Possible race-condition in RemoteService.Race-condition during ack checking is possible. For example, ack timeout = 1000 ms:worker receives Request in 999 ms sends ack, but it comes in 1010ms or something like that.client checks ack in 1000 ms and throws RemoteServiceAckTimeoutException.worker invokes method.To solve this problem ackObject was introduced. Worker or client set it to 1 via SETNX command. Worker set it during ack sending. Client during ack receiving.Client check ackObject only if ack timeout has occurred. If client can't set it means that worker have done it already. So client should poll an element from queue again.If worker can't set ackObject then it means that ack timeout already occurred on client side and invocation should skipped.
Possible PublishSubscribe race-condition.There is a rare possibility of race-condition during channel re-connection and RTopic subscription invoked first time.
RScoredSortedSet race condition with Redis cluster.We are running two redis servers in AWS, one master, and a replica. The read mode is the default (slave).In our application code we have something like this..The issue is that since reads go to the slave, addasync returns from master and before the data replicates to the other server it tries to pull the new revrank and returns null. imo the following api would make more sense and avoid that race condition..In the meantime I will either add retry logic or change the read mode to master but that's not going to scale in the long run so it would be nice to have this addressed.Cheers!
Redisson hang on RBatch.execute()Hi, we are using redisson client for our multi-threaded application.We use RBatch to batch two commands as below:During the test, we noticed the hang several time.We first show this error first before the hang:Then our thread calling the batch hung. There is the stacktrack:I tried to set the lower lockWatchdogTimeout and set the batch timeout but this did not help.
RFairLock dead lock issue.Hi,I would like to report a locking issue we are seeing using redisson versions 3.3.2 to 3.5.4, using RFairLock.We have 3 instances of an application competing for a fair lock. After some time, the timeout of the fair lock queue gets set to a time many hours in the future, and no thread ever gets the lock.Here is what redis is reporting during one of the occurences - both commands ran at about 9:30AM PST on 10/17/2017 (1508257800000):You can see that the timeout (zscore of the timeout keys) is set about 6 hours in the future.So far, we haven't been able to reproduce the issue locally, but it is happening daily in our production environment. We are suspecting the problem is happening randomly when one of the application gets killed by the operating system (kill -9), something that can happen quite often due to the way we manage the apps.We were using redisson version 3.2.3 before, and never experienced the issue.Earlier this year, we tried updating to version 3.3.2 and started seeing the problem within a day or so. We reverted to 3.2.3, and waited for an updated version.Two days ago, we decided to upgrade to 3.5.4 assuming the problem was fixed, but it happened again within a few hours.I will keep on trying to reproduce the issue, but any help is appreciated.Thanks,Michael.
JVM crashes when using 1000 concurrent redisson threads.I am scheduling tasks on a Redisson executor. On every schedule a massive chunk of tasks are submitted on two Redisson executors. I have equal number of workers assigned to each of the two executors 500 and a maximum redisson thread count of 1000.I took periodic heap dumps of the JVM and the following was the last dump I captured before the container (my code is running in a docker container) went OOM. It appears to me that the netty's PoolThreadCache continues to increase.Here is a screenshot of the leak suspect report of the last dump I took:I am unable to get the crash dump report of the JVM. Will attach that as soon as I am able to figure out the issue with crash dumps not getting generated.I am using Redisson 3.5.5. The bundled Netty version is 4.1.16.
ReadLock not working correctly.Hi,Is this the expected behaviour of the ReadLock from ReadWriteLock?Clarifying because I did not encounter this in release 3.5.4.There are multiple threads that are trying to lock and unlock the readlock concurrently but I am encountering the "attempt to unlock lock, not locked by current thread by node id" exception.Here is a rough code sample to replicate the issue.The unlock part throws the following exception even though they're all read locks.
org.redisson.RedissonTopic.removeAllListeners got blocked on CountDownLatch.await.Hi there,We hit an issue in one of our server using Redission as Redis Client. We do have a single thread executor handling subscribe/unsubscribe for multiple channels. But last week, we found this got blocked on one server.The org.redisson.RedissonTopic.removeAllListeners is unable to respond request, and blocked in the acquireUninterruptibly FOREVER. I tried to dig into logs, there was one exception thrown before this in CommandAsyncService.syncSubscription line 125 "Subscribe timeout 9500ms". This is possibly related since this was the only "Subscribe timeout" message I saw in last 30 days logs and then this issue happened. But I still don't find prove on this. Looks like this is a rare case since we just hit once. I tried to reproduce locally but with no luck.I think the removeAllListeners should give an option to let caller pass in a timeout. But this is also not going to resolve the root cause. I am wondering if other people have observed this issue before. Please advise if you have any ideas on this, thanks.
RedissonRedLock.tryLock(long waitTime, long leaseTime, TimeUnit unit) still have something wrong.reference from the issue #1175 after it is closed.I change the test case in RedissonRedLockTest.testLockSuccess() like this:The test case above will fail in case of that lock1/lock2 is locked first in thread t1. However, it will pass in case of lock3 is locked first in thread t1.The  is as follows:No matter which lock is locked first in thread t1, the Red Lock will always can be locked or cannot.In this case, the Red Lock should always be locked for most of the lock is available.
[2.11.1] Unable to re-establish MasterConnectionPool after restarting redis.We are Trying to add a mechanism to re-subscribe to topics after restarting redis.Setting UP:Steps to recreate:See Following Error, Redis MasterConnectionPool is not re-established.We were initially at Redisson version 2.9.1, but when we following the previous steps, in step 5, we encountered the same issue as in #1268, which is removeListener thread blocks forever, so make the switch to 2.11.1.We also simulate the scenario with 2.9.1 version,  without doing removeListener (we found out there is pubsub Listener leak issue with 2.9.1, that is why we remove the old listener first), the MasterConnectionPool was able to re-establish.
ReadWriteLock timeout does not work as expected.I wrote a simple test to check the ReadWriteLock.The behaviour.First case: Read thread finishes executing properly, write thread hangs forever.Second case: Behaves as expected. Read thread locks, sleeps and unlocks. Write thread hangs until the read thread finishes and then locks and finishes.What I see in redis in both cases at the beginning:What I see in redis in both cases after some time:The rwlock_timeout entries expire in both cases, it seems like no one is updating their timeout in redis.In the first case, where the readThread finished running but the lock1 entry never expires and is stuck forever like this:What I expected.In the first case when the read thread finished running I'd expect the read locks to expire after some time and unlock the lock1 entry.In the second case I'd expect the rwlock_timeout entries not to expire since the read thread is alive, it sleeps but it is alive.
CommandAsyncService gets blocked at high concurrency [without OutOfMemoryError].Hi,While running a load test on jmeter of 200 users with ramp up time of 10 seconds and number of puts per user equal to 10, threads are getting  stuck. In this test we are using RTransaction and doing load testing on a single key by putting random values for a single key.here is the stack trace:Steps to reproduce or test case.
tryLock waitTime no valid.hello, I have test the tryLock with waitTime, but find it has bug. My test is as follows:As you can see, the second thread faild to acquire lock but the lock is released during 2 seconds...please tell me ,thanks.
severe bug !! when sending read operation to new slave loading previous data after failover.Steps to reproduce or test case.on each servlet request, i send read command like below to redis, in this case, it is problem.result = redissonClient.getScript().eval(Mode.READ_ONLY, script, RScript.ReturnType.VALUE), keyList, argvList.toArray());on each servlet request, i send read or write command like below to redis,  in this case, it is ok.result = redissonClient.getScript().eval(Mode.READ_WRITE, script, RScript.ReturnType.VALUE), keyList, argvList.toArray());send read operation each reqeuest on servlet server continusly.slave is loading data for syncing with master. it takes 10 minutes to load all data completely.during 10 minutes, servlet server is going strange behavior and throuput is very lower and don't return any exception. i think some threads is hanging around Redisson connection with new slave loading data.actually i expected returning error with timeout. why it is not returning any error until slave is completed loading data.after 10 minutes which completed load data from slave. it is going back to normal and throuput become higer.however, read operation with Mode.ReadOnly is going both master and slave not going to slave only.when loading data at slave, redisson is sending read operation to slave. and read opeartion thread is hanging aroud somewhere in redisson.read operation with Mode.ReadOnly is going well to slave only. however after failover, read operation is going both master and slave not going to slave only.
Lock watchdog does not monitor read locks.Hi, when we acquire multiple (concurrent) read locks and process a task for longer than the Config.lockWatchdogTimeout, we get errors when unlocking.Code to reproduce (wait the watchdog timeout):int threadCount = 100;Looking into Redis for the ttl of different keys, I noticed:The key 'mytestlock' gets an extended ttl.The key '{mytestlock}:UUID:THREADID:rwlock_timeout:1 is not set with new ttl and the key disappears before the unlock is called.We tested with Redisson 3.5.4 and 3.7.5.
unlock fail and expirationRenewal still execute.unlock success.unlock  fail.Steps to reproduce or test case.unlock ()client -> redis service network problem.unlock throw Exception.network recovery.expirationRenewal still execute.
Deadlock with RedissonLock used by JCache.We've ran into a number of issues with using the Redisson implementation of JCache via bucket4j in production for rate limiting.JCache.put(K key, V value) calls RedissonLock.lock() with no lease time, resulting in RedissonLock's creating a 'watchdog' thread that continuously renews the lease on the lock in Redis. We are using the default 'lockWatchdogTimeout' of 30 seconds so this thread runs every 10 seconds. What we've seen is in rare cases this watchdog thread never gets canceled, meaning it continues renewing the lease indefinitely until the instance containing the thread is restarted. This causes deadlock of all other threads trying to grab the lock and can ultimately bring down an application as threads build up.Threads using RedissonLock will wait forever for a lock to be released. There is no timeout when waiting for the lock.The 'unlock' method in RedissonLock can fail for a number of reasons -- e.g., timing out trying to unlock with Redis after 3 seconds (this happened in our case). When the unlock fails the watchdog thread is not canceled. This means the instance would have to be restarted to release the lock.A 'watchdog' thread should not exist. This is prone to issues where it never gets destroyed and a lock is held forever. A lease time should be used to ensure the lock is never held forever and somehow it should be verified that the lock is still held by the caller when making updates with it (an atomic check at update time) in case the first thread doesn't execute its update within the lease time and another thread grabs the lock.Threads should not wait forever for the lock. There should be some configurable timeout.A timeout on 'unlock' should be retried, but the lease time in 1 should handle any failure to unlock. If for some reason the 'watchdog' thread is kept, the unlock needs to ensure it is canceled if it fails.
Deadlock while obtaining lock.Thread gets stuck while obtaining lock.The same lock name was likely concurrently obtained and held by another thread possibly on another jvm, and then released.Observed intermittently.
two thread get the same lock by trylock?Steps to reproduce or test case.
RedisOutOfMemoryException.No errors should be thrown after setting maxmemory-policy :volatile-lru, maxmemory:4MB.When I am using two threads to put values through redisson to different database with limited memory(maxmemory-policy :volatile-lru, maxmemory:4MB),  error like org.redisson.client.RedisOutOfMemoryException: command not allowed when used memory > 'maxmemory' sprang up, and the database was flushed by redisson.But I replaced to use Jedis(2.8.0) to test again, no such exceptions came out, and the database was not flushed by Jedis. The items which exceed 4MB  can be deleted by redis automatically because of volatile-lru policy.Steps to reproduce or test case.Run commands on redis-cli, config set maxmemory-policy volatile-lru  config set maxmemory 4mb.Run the code twice(Two threads) using different database with type to 0.singleServer, refer to the codes for details.Codes I run.
Deadlock after Redis timeout.After a Redis response timeout, Redisson needs to be resilient and back to normal operation after Redis timeout and allow to lock and unlock critical sections again.After a Redis response timeout between attempts, Redisson adds the lock to scheduled renew task and never remove them. So, the application enters on a deadlock due to a lock renewed even after unlock.The task can run on the same thread due to reentrant lock feature, but when the task runs on another thread a deadlock occurs.Steps to reproduce or test case.Start Redis.Run the test application (https) with a simple locked task.Force a timeout on Redis runnning a client pause command: "CLIENT PAUSE 5000".
Cannot shutdown RedissonNode if there infinite lock is locked.To reproduce the problem:1 Create a Runnable job which will last ~ 1 minutes (greater than "internalLockLeaseTime" used by RedissonLock.java), get a lock and lock it by lock.tryLock(), release the lock after the job is done.2 Schedule and run this job using Redission framework.This is because RedissonLock.scheduleExpirationRenewal still running after we shutdown the node.Can Redisson stop the scheduleExpirationRenewal when user call node.shutdown()?
Unexpected exception while processing command.I got this problem when I trying to locking an object. Here is the problem:Date: Thu, 19 Oct 2017 10:15:31 GMT.what I'm missing?
The deadlock of RedissonMultiLock.When I used the RedissonMultiLock.lock(), I found the code of Redisson.This is my code.The program execution sequence is as follows:Thread A locked the 'lockSoundbox' of lockA. It's OK?Thread B locked the 'lock2000' of lockB. It's OK?Thread A locked the 'lock2000' of lockA. It's OK?Thread B locked the 'lockSoundbox' of lockB. It's OK?I think there's a deadlock between 3 and 4, but I'm not going to try it out.
Redisson 3.5.7 memory leak with locks.I'm using RLock and seeing memory leak via org.redisson.client.handler.CommandPubSubDecoder.Retained objects delta between heap dumps taken apart.Around 10MB leak per day.
RedissonRateLimiter acquire() method blocks forever.The acquire method should block until a permit is available, but it blocks indefinitely instead of the remaining time until a new permit becomes available. From my testing I have found that it doesn't acquire a permit when one should be available based on the rate and interval I tested. If I set the rate to 1 every 5 seconds, it should take a loop of 10 single permit acquires approximately 50 seconds to complete. Instead it acquires the first permit, then blocks forever on the next acquires call.
Multiple ReadLocks are not sharing locks properly after WriteLock releases.Hello!I've found something that I believe is a bug in Redisson's RReadWriteLock implementation where multiple ReadLocks seemed to become or at least behaved like WriteLock when it tried to lock on a lockpoint that another WriteLock has already acquired a lock, then released. Because of that, ReadLocks are not sharing the lockpoint like it should and are taking turns in locking the lockpoint. Theoretically, this will have a performance impact on applications that expected Redisson to have quicker locking mechanism based on inclusive locking mechanism but under the hood, it is not inclusive and those applications will spend some time to wait for locks to complete.I have tested this with Java's ReentrantReadWriteLock and it has worked exactly what I'd expected it to be.Note: I have only tested this on a single Redis server. I did not test this on clustered Redis servers.Writer Thread locks.Reader Thread 1 fails to lock and waits.Reader Thread 2 fails to lock and waits.Reader Thread 3 fails to lock and waits.Writer Thread unlocks.Read Thread 1 locks.Read Thread 2 locks.Read Thread 3 locks.Read Thread 1 unlocks.Read Thread 2 unlocks.Read Thread 3 unlocks.This behavior matches Java's ReentrantReadWriteLock behavior exactly.Writer Thread locks.Reader Thread 1 fails to lock and waits.Reader Thread 2 fails to lock and waits.Reader Thread 3 fails to lock and waits.Writer Thread unlocks.Read Thread 1 locks.Read Thread 2 fails to lock and waits.Read Thread 3 fails to lock and waits.Read Thread 1 unlocks.Read Thread 2 locks.Read Thread 3 fails to lock and waits.Read Thread 2 unlocks.Read Thread 3 locks.Read Thread 3 unlocks.This behavior does not match Java's ReentrantReadWriteLock behavior exactly.Steps to reproduce or test case.I have written a test code for you to review, download, and test.The link to my code is here:The test code includes two test suites that runs a control test using Java's ReentrantReadWriteLock and an experiment test with Redisson's RReadWriteLock.Each test suites has two test cases where one of the test does multiple ReadLocks locks on the lockpoint before WriteLock locks on it, and another one tests multiple ReadLocks locking on the lockpoint after WriteLock has locked on it. The failure on the latter test on Redisson's RReadWriteLock is what has prompted me to open this issue ticket. Java's ReentrantReadWriteLock passed that test.Overview:To run the test, run mvn test with Maven to test the code.Single server with default configuration created by Redisson's Config class.
RTransaction Unusual Locking Problem.Hi mrniko,We have two methods:As per our test we invoke method 1 and then immediately invoke method 2. The second transaction waits until the first one finishes and seems like the first transaction takes a pessimistic lock on one of the keys of the map being used here and we are totally fine with that.Now we have another method in a different test scenario:As per our previous test we expected a deadlock situation in this scenario, since transaction1 has the lock over the key and transaction2 is trying to alter it. But in our test both transactions commit successfully and maintain order.Please explain the ambigious locking in case of these to scenarios. Thank you.
Redisson continuously send lock request to redis after redis cluster master-slave switch.Evn : jdk 7, redisson version: 2.11.4,redis version:4.0.1,enable redis cluster with 3 masters and 3 slaves.After my redis cluster occurs an master-slave switch and then I try to get a lock, but redisson starts getting out of control, it continuously send Lua script command to get lock, while redis cluster return a MOVED error because of master-slave switch early, it seems that the redisson didn't know the master-slave switch and continuously send Lua script command to the same server to get lock. Approximately redisson sends 150000 lock request to redis in 5 seconds with merely more than a dozen application lock request. The network traffic between the redisson server and redis server rise up to 150Mbps, then it becomes normal after restart the redisson server.The getting lock code general show below,(I just receive 10+ lock request):The network packages general show as below:
redisson keep sending lock request to redis due to redis cluster master-slave failover.i use redisson redLock in my project, i think redLock should be work in any case, below is my code,but when redis cluster code master-slave failover, redisson failed to acquire lock and keep sending acquire lock request, below is request:then send same request again and again... cluster response again...
RedissonBlockingQueue.take() returns null in case of internal error.The take() method in RedissonBlockingQueue uses Future.getNow(), which returns null if an exception occurs. In our case, we have tons of NPEs when the application is stopped.
RedissonRedLock issue.
ReadWriteLock not working.Hello.I'm interested in this project. I am creating a sample project(https) using this,And I do not think readswriteLock is working properly.The error pattern is a bit different, but the example org.redission.example.ReadWriteLockExamples.java is not working properly. Version 2.10.5 does not work either.I do not know much about redis.I hope this project can be maintained.
Caused by: org.redisson.client.RedisException: ERR max number of clients reached.When I use Reentrant Lock in my application, this exception  ERR max number of clients reached occurs.Redisson version is 2.10.7, and redis version is 3.2.
Warn:"Invocation of destroy method 'close' failed on bean with name 'jCacheCacheManager'".I imported Redission in spring boot using distributedLock,but the log always show this WARN.
RedLock slow performance when one redis instance is down.According to RedLock Algorithm, Step 2 RedissonRedLock#tryLock should fail fast if redis instance is unavailable:During step 2, when setting the lock in each instance, the client uses a timeout which is small compared to the total lock auto-release time in order to acquire it. For example if the auto-release time is 10 seconds, the timeout could be in the ~ 5-50 milliseconds range. This prevents the client from remaining blocked for a long time trying to talk with a Redis node which is down: if an instance is not available, we should try to talk with the next instance ASAP.RedLock#tryLock blocks for duration equal to config.retryInterval * config.retryAttempts on each tryLock attempt.Such behavior makes business code not able to consume messages at requried rate, which in turn leads to other negative consequences.Setting config.retryAttempts=1 and config.retryInterval=50 does not work, since initial commands are not fast enough and connection establishment fails.Other timeouts doesn't seem to have relation to this issue.
CommandAsyncService blocks indefinitely [without OutOfMemoryError].Similarly to #889.throw exception.Thread indefinitely park.Using RMapCache and configure maploader to DB(oracle).Shutdown DB,3.RMapCache get operation will be blocked.
RRateLimiter.tryAcquire with timeout blocks forever.If it's not possible to acquire a permission before timeout elapses, tryAcquire should return a boolean false.Even if the timeout elapses, the method tryAcquire keeps blocked.Steps to reproduce or test case.
Unexpected exception while processing command while taking a lock on a key.Note:Same scenario works as expected when there is no exception or error inside the try block.
RedisResponseTimeoutException at getLockedLock for read operation in "replicatedServersConfig".Hi,I'm using jcache  implementation and using AWS Redis servers in "replicatedServersConfig", i'm getting response time out exception at "getLockedLock" method. raised another issue  related to jcache.Please do the needful.Thanks,
Race condition in RedissonLock subscribing to pubsub channel.We are currently using redisson for, among other things, distributed locking. In our use case, we can have hundreds of unique fair locks being acquired many times, usually for short periods of time (~1-2 seconds).During testing, we noticed that sustained load eventually, but consistently, results in threads failing to acquire the lock. To try to pinpoint the problem, we added extra logging in various places in code path used by RedissonLock. What we found was that it sometimes took a long time for the lock to subscribe to its pubsub channel, by which point the unlock message was already published. This results in the thread waiting on entry.getLatch().tryAcquire(ttl, TimeUnit.MILLISECONDS); until failure.In other words, the sequence of events seems to be:thread 1 acquires a lock.thread 2 tries and fails to acquire the lock, initiating pubsub subscription.thread 1 releases lock (semaphore is not released since nothing is waiting on it yet!)thread 2 completes subscription connection and waits on semaphore, eventually timing out.There seem to be two places where this delay happens:Acquiring an AsyncSemaphore in  the PublishSubscribe::subscribe method.The 'await' call in RedissonLock::tryLock.Are we missing something here? It seems strange that it can take upwards of several seconds for the lock to subscribe to its pubsub channel.
redlock unlock fail.
tryLockAsync and lockAsync methods of RedissonMultiLock object get hang.No description provided.
RedissonAtomicLong.get blocks indefinitely.Stack info:Steps to reproduce or test case.Small probability and we can't reproduce, maybe related to bad network.Redisson configuration.cluster default config.
PriorityBlockingQueue doesn't release lock after reconnect on network.reconnect to network.there is a lock  name redisson_sortedset_lock:{testqueue} which dosen't release,it seem's like it can't release lock cause network boke,however it get a lock before network boke.Steps to reproduce or test case.1.run code blow.2.disconnect your network.3.wait a few minute,then reconnect your network.
spring boot started incorrectly, but unlocking can work properly.
Concurrent calls to RemoteService should not result in an exception.Concurrent calls to RemoteService should not result in an exception.Concurrent calls to RemoteService should result in an exception.Steps to reproduce or test case.Redis version.4.0.9.Redisson version.3.6.5.Redisson configuration.
Mitigating Concurrency Issues.Hi,I was wondering what the standard approach is to Mitigate possible Concurrency issues when using Redisson as a spring data source.I am working on an application that gets information from a source application and then layers updated over the top before delivering the updated information to the client.The updates in this case are built for several sources, each one has its own queue.When a new message is placed on any given queue my app reads the message from the queue, and attempts to create or update an updateObject in my redis cache.My problem stems form the fact that messages are constantly being placed on these queues and updates to the updateObject are likely to come in quick succession.Im concerned that this will lead to concurrent read write issues:I have attempted to solve this using @transactional and the RedissonTransactionManager but it doesn't seem to be halting the second messages update while the first message is still updating.Any advice would be really appreciated,Thank you.
[CRITICAL] Redisson cluster sync failed.In cluster mode when master goes down and slave becomes a master, Redisson picks up new master but failes to sync slaves. Cluster nodes synchronisation crashes (never scheduled again) with IllegalArgumentException while processing slave nodes.Redis cluster setup:Relevant stacktrace:Code path which fails with IllegalArgumentException (please note it's 3.0.0 branch):scheduleClusterChangeCheck(cfg, null); is never executed thus Redisson stops sync cluster state. Any subsequent cluster change leads to a service outage.
redission AtomicObject in debug + breakpoint mode cause a different result.version:3.5.4.At first,I delete the key,confirm the key not exist.
Possible race-condition during write operation cancellation.No description provided.
RLock.unlock method returns true if lock doesn't exist.No description provided.
RedissonLock.renewExpiration will refresh forever.If the lock key no longer exist the refresh should cease.The renewExpiration() method ignores the return value of renewExpirationAsync() and reschedules itself forever.Steps to reproduce or test case.Should be as simple as manually removing a lock entry key from redis.On a side note, another thing we noticed while looking into this is that renewExpirationAsync() does a scripted call that returns 0 or 1 depending on HEXISTS. It seems unnecessary as according to the redis docs about PEXPIRE, it does the same thing out of the box in a single command call.
RedissonSemaphore doesn't take linux machine's down into account, like RedissonLock do.when machine is down, RedissonSemaphore should release it never released.Steps to reproduce or test case.
Exception when use RedissonRedLock + RedissonMultiLock.I'm trying to introduce RedLock algorithm in my project, I have 3 Redis nodes, and I need to lock several keys in each thread. Here is my implementation:no exception.Got exception with message: ""org.redisson.RedissonMultiLock cannot be cast to org.redisson.RedissonLock"".Steps to reproduce or test case.the root cause is this code in method tryLock(long waitTime, long leaseTime, TimeUnit unit) of class RedissonMultiLock.java:It trys to cast rLock to RedissonLock, but in my case the rLock is RedissonMultiLock. By this implementation, RedissonMultiLock actually can not group any RLock types but RedissonLock.
RedissonFairLock timeout drift.We've discovered additional conditions that produce timeout drift as originally described in #1104. To summarize the issue, we see that the timeouts in the redisson_lock_timeout sorted set created by RedissonFairLock can gradually increase over time to be hours or days in the future despite the queue only containing less than 10 threads; this condition continues until the fair wait queue empties, or one of the threads dies and the rest of the threads are forced to wait for the far in the future timeout on the dead thread to lapse (this creates a dead lock-like situation).I'll provide a PR to follow shortly with test cases and proposed changes to fix the issue. We greatly appreciate feed back on any misunderstanding of the issue described here and will do the same for feedback on the PR.The expected behavior is that which is documented in the second paragraph of 8.2 Fair Lock:All waiting threads are queued and if some thread has died then Redisson waits its return for 5 seconds. For example, if 5 threads are died for some reason then delay will be 25 seconds.As an example that replicates this documentation, given we have 7 threads trying to take one lock, we expect the redis data structures to contain roughly the following after a 7 threads try to acquire the lock once using RLock.tryLock(threadId) in order with short delays between:1) lock name of the 1st thread.2) "1".3) lock name of the 2nd thread.4) lock name of the 3rd thread.5) lock name of the 4th thread.6) lock name of the 5th thread.7) lock name of the 6th thread.8) lock name of the 7th thread.9) lock name of the 2nd thread.10) lock timeout + 5s.11) lock name of the 3rd thread.12) lock timeout + 10s.13) lock name of the 4th thread.14) lock timeout + 15s.15) lock name of the 5th thread.16) lock timeout + 20s.17) lock name of the 6th thread.18) lock timeout + 25s.19) lock name of the 7th thread.20) lock timeout + 30s.In the above we see that the 7th thread will need to wait 25s after the lock expiration (based on the lease time) until it can acquire the lock if the 1st to 6th threads die.Additionally, we expect that when a thread leaves the queue voluntarily either by wait timeout or by acquiring the lock, that timeouts adjust regardless of where the thread was in the queue. For example, if the 3rd thread in the above example left the queue via a RedissonFairLock.acquireFailedAsync call due to wait timeout, the sorted set would then be expected to be:1) lock name of the 2nd thread.2) lock timeout + 5s.3) lock name of the 4th thread.4) lock timeout + 10s.5) lock name of the 5th thread.6) lock timeout + 15s.7) lock name of the 6th thread.8)  lock timeout + 20s.9)  lock name of the 7th thread.10) lock timeout + 25s.And the 7th thread would then only need to wait 20s after the lock expiration to acquire the lock if the other threads died.In one of our usages of redisson we use the fair lock using 3-6 servers using each a single instance of Redisson running roughly the following code in one thread per server:We observe that since the work done within the locked portion of the code can take only ~100ms that the 6 server threads running this code quickly churn through the lock queue without issue, though a queue is always present since there is little time between when each thread unlocks and locks again. If we take periodic snapshots of the redisson_lock_timeout sorted set, we tend to see that the timeouts increase in increments of 5s over time until the timeouts are hours or days in the future. If one of the servers is killed, then we observe that the other servers stop doing work and are timing out trying to get the lock due to the dead server's lock holding the first position in redisson_lock_queue with a timeout in redisson_lock_timeout that may be hours or days in the future. We expect that under the case that we lose one server of 6, that the timeout values will be in the range of 10 to 35s in the future at any one time (5s lease time + 5s thread wait time * position in queue). It may be said that the above lock usage itself may be foolish, but its mostly working except for the chance of deadlock.Steps to reproduce or test case.I'll provide a PR for this issue to follow with additional test cases added to RedissonFairLockTest. However, what we started with was a modification of the testTimeoutDrift where we changed the wait time from 500ms to 3s and changed the lock holding time from 30s to 100ms (see Thread.sleep(30000)); with this test case, instead of the tryLock failing due to wait timeout, the threads are able to lock and unlock the lock quickly. The new version of the test, the test fails with a timeout drift into the futre, in a similar way that the test failed in #1104.The PR will contain other test cases with the hope that we cover all code changes and produce the expected behavior described above.We run a single redis server for use exclusively by redisson.
`redissonClient.getRedLock` does not work well with Read-Write block in a multi-JVM environment.Then call lock.tryLock() first, return true.Then call lock.tryLock() after the previous call has been made, now it should return false because there is a readLock under the same name.Then call lock.tryLock() first return true as expected.Then call lock.tryLock() after the previous one, now it return true i.e. have both write lock and read lock on the same ReadWriteLock.Steps to reproduce or test case.As described above.In case if this is useful information, if replace getRedLock with getMultiLock it actually works. Also if having only one JVM it also works.
Why do locks fail occasionally?Here's how I use it:But there are still high concurrency issues.Two of the same information is generated.Request to solve.thank.
if waitTime == -1 and leaseTime != -1,multiple threads get the lock at the same time.if waitTime == -1  and leaseTime != -1,multiple threads get the lock at the same time.Thank you.
@Transactional and RLock.unlock()As can be seen from the above, transaction committing occurs after the unlocking operation. Is there a possibility that other threads will have concurrent problems because the thread has been unlocked before the transaction has been submitted?I tested it with JMeter and found no concurrency problems between unlocking and committing transactions.If you know, please let me know. Thank you.
RLock unlock with a different thread that the one that locked gives: attempt to unlock lock, not locked by current thread by node id.I'm using RxJava and as a part of the sequence I use a RLock, at some point (in another process) I unlock it and if the thread to unlock is not the same as the one that blocked I get an exception (see below).I see that the lock has been done using the pool of redisson-netty, and the unlock is done by the same pool, but there are several threads in that pool, so any thread can potentially perform the unlock.That should be as simple as tryLock and unlock without having to control anything else right? Do we have to do something specific to ensure the proper behaviour?The lock gets unlocked regardless the thread that did the lock.An exception is thrown and the lock is not unlocked.
redission ScheduledFuture can not be execute after definite time.complete the ScheduledFuture after ten seconds.the schedule task above can not be executed after 10 seconds, the current thread beign blocked.Steps to reproduce or test case.using spring boot org.redisson.spring.starter.RedissonAutoConfiguration, just configure redis host and port in properties file.
when I unlock the lock, accept an exception.somebody can help me ?
RedissonFairLock deadlock.Seems related to the change on #2099.We have tried to upgrade to 3.11.2 from 3.10.7 and this behavior started occurring.The FairLock acquired on high load system without lease time defined.Redisson does not handle the case of Fair lock without lease time (on high load).when many different threads are asking for the same lock key the client is stuck and no thread acquires the lock.Steps to reproduce or test case.
LockWatchdogTimeout will cause the IllegalMonitorStateException.redis version: 3.0.504.redisson version:3.11.1.setLockWatchdogTimeout will cause the IllegalMonitorStateException.
threads blocked waiting on CountDownLatch.Hello,We are facing an an issue where some threads get blocked indefinitely waiting on the CountDownLatch in CommandAsyncService.get(). Relevant section of the thread dump:The reason for why the requests aren't able to complete isn't relevant here (we are deploying redis in kubernetes, and are working through various upgrade/failure scenarios that currently can result in severed connections). However, it's strange to me that the CommandAsyncService.get() method doesn't respect timeout configuration, and simply calls l.await().Is there any way around this? Please let me know if I am misunderstanding the situation.CommandAsyncService.get() should throw an exception if the future is unable to complete within a timeout.Many threads are hanging indefinitely in the CommandAsyncService.get() method.
thread blocked at closeNodeConnections.Hi,The call to closeNodeConnection waits indefinitely on call to MasterSlaveConnectionManager.closeNodeConnections.Section from thread dump:MasterSlaveConnectionManager.closeNodeConnections should throw an timeout exception if the same is not able to complete within a configured timeout.Thread hangs indefinitely at the request.
when aquire redisson lock , that throw redis response timeout sometimes.redisson.2.2.13.when occouring this problem,the redis has not much qps.
RBlockingQueue.take doesn't throw InterruptedException.No description provided.
RedissonLock.unlock() hangs infinitely waiting for RedissonPromise.Calls to RedissonLock.unlock() are expected to finish after the lock has been deleted from Redis.We have experienced several threads locked forever waiting inside calls to RedissonLock.unlock() in our production system within a few hours after upgrading to Redisson 3.11.4 from Redisson 3.8.0.Steps to reproduce or test case.Unfortunately I am unable to reproduce the problem on my own computer. We have however experienced it on several of our production servers within just a few hours after starting our newer version of the application with updated Redisson version. We upgraded from 3.8.0 in order to receive fixes for bug #1966 - Deadlock after Redis timeout, which we also experienced a couple of times. However right after start we have found some (29) exceptions in our logs coming from internal redisson threads:After that we were investigating issues with blocked threads by analyzing thread dumps. All stuck threads had exactly the same stack trace as below:After examining the source code of RedissonLock.java we have found that the NullPointerException occurs probably due to missing timeout object inside task:I'm not sure if it would solve the root cause why task.getTimeout() returns null, but calls to cancelExpirationRenewal method should not fail with an exception, otherwise, e.g. in RedissonLock.unlockAsync method the future will never receive a result (or a failure):Please add null check for timeout inside the ExpirationEntry object.We had to return to 3.8.0 version because of this problem. I think it is quire severe.Default configuration with sentinel servers.
Interrupted blocking methods aren't canceled.No description provided.
Reactive/Rxjava2 transaction doesn't unlock lock on rollback or commit.No description provided.
Lock expiration renewal stops working after connection error/timeout.After a connection problem during expiration renewal, if a new lock is created (with the same key), the expiration renewal should be working.If an error ocurred in the expiration renewal (ex: connection to redis timeout), it stops working for new locks.Steps to reproduce or test case.Create a Redisson lock.Force a expiration renewal timeout (ex: restarting redis-server)unlock (it will throw a exception, as expected)Create a new lock with the same key.wait for the expiration renewal time.unlock (it will throw a exception, because the expiration renewal is not working)Default, SingleServer, (LockWatchdogTimeout reduced just for testing)I believe the problem is in RedissonLock.unlockAsync method, when opStatus is null, the cancelExpirationRenewal method should be called.
RLock.unlock() results in ERR hash value is not an integer.The following code snippet will successfully create and lock RLock and will throw RedisException unlocking the lock.
Unable to remove lock automatically.The code is as follows, the lock cannot be automatically deleted, the lock will continue to live.
Unable to unlock  when thread isInterrupted.Unable to unlock when thread isInterrupted,Please,RUN it First!!!
RedissonLock fails to unsubscribe from channel when a lock is acquired.Expected behavior.The pub/sub channels that are created while trying to acquire a lock with RedissonLock should be unsubscribed when no longer needed.Actual behavior.Some pub/sub channels are still present (meaning that the client did not unsubscribe) even after some locks are no longer used (either they expired or were unlocked). These channels are never cleaned up (still present after 24h since created). The only workaround to get rid of these is to restart the client.This was observed because it caused a connection leak. When calling RedissonLock to obtain new locks this returned the error : Subscribe timeout: (7500ms). Increase 'subscriptionsPerConnection' and/or 'subscriptionConnectionPoolSize' parameters. We have increased the limits in order to have a workaround for this bug.Steps to reproduce or test case.use RedissonLock.java implementation with method lockInterruptibly(long leaseTime, TimeUnit unit) to obtain multiple locks simultaneously.use lower limits for  subscription-connection-pool-size and  subscriptions-per-connection to increase the probability to see the bug.wait until this error shows up Subscribe timeout: (7500ms). Increase 'subscriptionsPerConnection' and/or 'subscriptionConnectionPoolSize' parameters.connect to Redis server and fetch all lock keys and all pub-sub channels.if number of pub-sub channels > active locks , then the bug was reproduced.if number of pub-sub channels does not decrease at all long after locks are being created, this means that subscriptions have leaked, active long after these are no longer used in RedissonLock implementation.
tryLock(),Machines are not evenly distributed, and one server never gets the lock.Machines are not evenly distributed, and one server never gets the lock.It's going to last forever.
In a frequent job , sometime lock.tryLock() can not get lock.In a spring boot 1.5.6 application , We have a new job which is running every 30 seconds and try to acquire lock to do something . In recent days the job has tried to acquire one specific lock cause of business , We found from 00:13 on Feb 24  the job can not get the lock but after 15:03 the job can get the lock again . The operation on this lock only in this job .the code on the lock as below :Thanks.
Distlock Mutual exclusion problem?1assume a Redis cluster has a 5 master node; A , B, C ,D ,E representative node name.2Client A  lock to A,B,C three node. lock success.3This time B,C Node happen Network partition.4Client B lock to D,E tow node, lock success,Satisfy N/2 + 1 ,because have A,D,E three node.5Violation of mutual exclusion.
RedissonBoundedBlockingQueue drainTo method got infinite hung.Expected behavior.RedissonBoundedBlockingQueue drainTo method will return result.Actual behavior.RedissonBoundedBlockingQueue drainTo method got infinite hung.
RLock throws java.lang.IllegalMonitorStateException in the thread where got the lock twice.In a spring boot 1.5.6 application, We have below operations on merchant account ,In PayServiceImpl class , using spring proxy (IOC) to call payHandlerService.balanceFreeze.It seems (B) throws the exception as below.Thanks.
RedissonLock.tryLock() interrupted and keep renew lock.Expected behavior.when tryLock is interrupted, watchdog renew listener is cancelled too.Actual behavior.when tryLock is interrupted, watchdog keeps renewing lock, this makes ifinite lock.Steps to reproduce or test case.
RedissonReactiveClient getLock and getLexSortedSet is executing on different thread.So here lockk is printed as true, but redissonReactiveClient.getLexSortedSet(redisKey) cannot execute and directly goes to onErrorResume as it is executing in different thread than lock.tryLock(). I'm new to this reactive style of programming, I don't know what I'm doing wrong maybe there is a better way to lock and get the sortedset? please help.
`tryLock` in redis cluster working not properly.Expected behavior.tryLock of RLock can not return true when the lock key still exist in cluster redis.Actual behavior.Sometimes tryLock can acquire the lock key although it still exists. Moreover, the TTL of the lock key also is reset.Steps to reproduce or test case.
Thread that called tryLock stops in standby state.Hello,Running the redisServer again does not resolve the issue.In particular, Scheduler threads cause serious problems.Is there a solution?Below is the test code.This result is Thread dump.
Is tryLock() method expected to return false when redis is not available.Expected behavior.tryLock() method return false after certain time when redis is not available.Actual behavior.tryLock() method is hanging forever.Steps to reproduce or test case.when application starts, the redis server is there, then bring down the redis.local redis docker without authentication.
spring-integration unlock error when use redisson.use redisson and spring-integration.Steps to reproduce or test case.this keyCommands() retuen this OBJECT occur recursion.Maybe,we need override method unlink in RedissonConnection.
redisson lock uncontrolled release.Hi all,I am facing strange problems with Redisson lock.I use the lock without expiration - which means is that watchdog mechanism in-charge of updating the lease time of the lock in redis.Threads that are using the lock, lock it for time ranging from couple of seconds to 300 seconds or even more.The problem:sometimes when a thread(that acquired it) tries to release the lock the thread receives IllegalMonitorStateException , which means that this lock is already released.One solution was to increase the watchdog timeout - I increased it to 100 seconds and it decreased the number of errors but they still occur from time to time...I don't want to increase the timeout even more because this doesn't address the root problem.How can I solve this? is anyone else encountered this behaviour?
Using lockAsync with Scala Akka.Hi,Does lockAsync / unlockAsync and bucket async APIs work with Akka actor using Scala/Play?I am getting the exceptin below:java.lang.IllegalMonitorStateException: attempt to unlock lock, not locked by current thread by node id.Is that because of unlockAsync being triggered inside Future of lockAsync? I am using Scala converters to get Scala Futures from these APIs.If I generate my own thread ids using random long then it does work.Thanks.Rakesh.
redisson 3.13.1 Always connected to redis cluster and can not to acquire lock,but no exception.redis version:5.0.8.redisson version: 3.13.1.i have built redis cluster with six instance.and use cluster nodes to view clusters info.
redisson,The current thread does not release the lock, will hava some problems.yesterday,i test the redisson lock,i use  Apache JMeter to test the concurrency.param: thread number = 1000,  loop = 1.i found when i not release the lock .but other thread can get the lock.example:
Don't release the lock actively, there is a problem in high concurrency.yesterday,i test the redisson lock,i use  Apache JMeter to test the concurrency.param: thread number = 1000,  loop = 1.i found when i not release the lock .but other thread can get the lock.example:
RedissonFairLock deadlock.RedissonFairLock result.Only more than 100 "endLok" were printed.if use RedissonLock,it can be print all.
possible concurrent issue in RedissonLock?When running multiple threads to acquire the lock with the same name, the below error happened from time to time.Not sure if we need any concurrency control in the ExpirationEntry of RedissonLock? e.g. by simply converting the threadIds to a synchronized map, the issue got fixed in our particular case.
Thread gets stuck when trying to acquire lock via tryLock()Expected behavior.The thread should get release i.e tryLock should return false.Actual behavior.Thread gets stuck in Waiting state.Steps to reproduce or test case.This is how we are calling the tryLock and multiple threads are scheduled to call this.Some threads gets stuck at the Waiting state at tryLock.This is observed when the shards are being added (Sharding is In Progress state).  After sharding is done then Restarting the application releases the threads.
FairLock is freezed after run more than 9000 times.this is my code.the log will not print after a few minute.
Non thread-safe initialization in RedissonObjectBuilder.Spring bean created successfully.Steps to reproduce or test case.We have several spring beans that initialise in prallel.
Redisson 3.10.7 & FST 2.57 - Data is not cleared from ThraedLocal while using Serialization of FST library.When we are saving some data in redis using FST codec then data is not cleared from ThreadLocal after data saved to the redis. This is causing issue of OutOfMemory as tomcat thread's ThreadLocalMap is filled with huge data which was used while FST serialization.Detail:Main code requested for putting some data in Redis.Redisson uses FST serialization to serialize data and then stores into Redis.FSTObjectOutput which was used while serialization has stored data in ThreadLocal object and it has not cleared data from ThreadLocal after it was saved in the Redis.I can see ThreadLocalMap contains all the data which was used while FST serialization. Please see below snapshot of heap dump:Ideally it should clear the data from ThreadLocal, once the data stored in Redis.
On Reading Locks.Multithread concurrency can be unlocked successfully.Multithread concurrency with only one thread successfully unlocked.Are there any problems with the script here?When a thread finds the corresponding key, it deletes the whole key. Is it impossible for other threads to find it?Steps to reproduce or test case.
RLock unlock always throw an exception what like 'not lock by current thread'.trylock and unlock success.RLock unlock always throw an exception what like 'not lock by current thread'.Steps to reproduce or test case.According to the stack information, the underlying asynchronous unlocking may cause the unlocking to throw an exception.This phenomenon can be reproduced 100%.
lock.isHeldByCurrentThread() verification  before unlocking Exception.RLock.lock.isHeldByCurrentThread() verification  before unlocking.but sometimes exception.
A lot of threads waiting on "org.redisson.spring.data.connection.RedissonConnection.sync".When I was doing the stress test, there were more than 180 threads (like:org.redisson.spring.data.connection.RedissonConnection.write() ,OR org.redisson.spring.data.connection.RedissonConnection.read(),and org.redisson.spring.data.connection.RedissonConnection.del())waiting for "org.redisson.spring.data.connection.RedissonConnection.sync()". What is the reason for this? And how to solve it?Thank you very much!
redisson-netty thread lock.Hi,We have setup Redis cluster and populated key/values. Some Keys contain more than 2M fields. When we call getAll of RMap using redission client, it takes more than 30 mins and goes to idle state. We analyzed the thread dump and noticed the below errors. We are using redission client (3.13.1). Is this known issue with Redisson client?
Spring Data Redis connection in multi mode may cause thread hang.No description provided.
InternalThreadLocalMap memory leak.Expected behavior.Actual behavior.after long running, application gets oom.Steps to reproduce or test case.our app runs about 1 month.Redis version.cloud service, not sure.very big array(1048576 size) in each threadlocal, new array was created in old-gen and always fail.
RedissonAtomicLong#getAndSet throws NullPointerException when value does not exist yet.Expected behavior.Either no Exception (just create the value, getAndSet shoudl be able to create the value as well) or documented behaviour.Actual behavior.Null Pointer Exception, but only during first execution. Reproducible if I manually delete the value in Redis again.Steps to reproduce or test case.When the value does not exist yet, counterInDb is not null but calling getAndSet on it without checking isExists as well in the if gives me a NullPointerException at RedissonAtomicLong.java:122. This is quite confusing as.The Null Pointer Exception is from the implicit cast from Long to long, which makes it really hard to spot.There is no JavaDoc warning that this might happen.
Redisson throws RedisTimeoutException before timeout has exceeded.Redisson throws RedisTimeoutException after timeout has exceeded.There is a race condition where Redisson may throw RedisTimeoutException after retryInterval has exceeded but before timeout has exceeded (in a nutshell when there are bursts of commands, commands will timeout even if timeout hasn't passed if retry interval is set to 0).line 765 in CommandAsyncService seems suspect as it uses retry interval for the initial timeout value..Steps to reproduce or test case.
RedissonConnection#del() doesnt participate in pipeline.Weve been experiencing intermittent NullPointerExceptions (stacktrace at end) that appeared to be race condition. The root seems to be that del(byte[] keys...) doesnt properly participate when a connection is pipelined.I believe the simplest reproduction is:
RedissonSessionRepository topic listeners initialization has race condition.Weve successfully migrated to using RedissonSessionRepository for our Spring sessions, but we get intermittent NPEs right after startup. It looks like the issue is that the first listener is added before all of the topics are initialized, and the conditional matching in onMessage doesnt go in the same order as initialization.
RedissonSession#changeSessionId expiredKey race condition.We have a number of instances of our app in production, and weve encountered a race condition where the same session might have changeSessionId invoked on the same session separate servers. This means one server might receive -2 from PTTL, which isnt currently handled.
RedissonFairLock blocks indefinitely after threadWaitTimeExpiry. Expected behavior. Threads that are alive and waiting for a RedissionFairLock will always get ownership of the lock it tried to acquire.Actual behavior.When a RedissonFairLock is locked by long-running Thread T1, and other Threads (T2, ..., Tn) attempts to access the lock and waits for longer than 5 minutes (the default threadWaitTime), even when T1 releases the lock all waiting Threads T2 to Tn hang indefinitely.This is due to future threads removing "expired" threads from the Queue, when in reality these threads are still alive and waiting for the lock. These "expired" threads never receive the PUBLISH message on unlock.Steps to reproduce or test case.
Redis is disconnected for a long time, and using redisson's trylock() will cause the thread to wait all the time.Expected behavior.notify thread and return false.or throw redis disconnected exception.Actual behavior.Keep waiting.Steps to reproduce or test case.1.Run application.2.Disconnect redis.3.Execute trylock()(1)Create a thread pool.CorePoolSize:8.QueueCapacity:200.ThreadNamePrefix:cmdpAsyncExecutor-.(2)Use trylock() in a thread.for loop 10 times.4.Wait a few minutes.5.Reonnect redis.6.View thread status.View thread stack information.We found that the thread had been waiting.
TimeSeries.iterator() doesn't respect the ordering.Expected behavior.Since timeseries is a sorted collection I'd expect iterator() to respect the ordering. There's a range method which in fact returns the values in order, however it returns Mono<Collection> instead of Flux which means entire set is loaded into the memory, while I'd like to read it reactively in chunks.Actual behavior.redisson.getTimeSeries("").iterator().next().block() != redisson.first.block()Steps to reproduce or test case.add multiple elements into the timeseries collection.open a redis desktop manager or use the cli to check the elements are indeed ordered by score.call iterator() method and check the ordering.I've checked also the non-reactive version and seems to work in the same way.
Concurrency issues with RQueue::removeIf / RQueue's iterator.Expected behavior.When having one producer and at least two consumer threads (or instances of an application) I expect RQueue's removeIf(lambda) method to remove elements without negative side effects.I expect the same when using an iterator to iterate over the queue and calling remove() on the iterator.Actual behavior.There are multiple types of errors that I observed:I've seen NullPointerExceptions inside the removeIf lambda (e.g. q.removeIf(element -> element.getId().equals("id"))), where element is null (we don't add null elements to the queue).Other times removeIf does not actually remove the elements from the queue, probably because it did not find the elements in the first place.As the default implementation of removeIf uses the iterator my guess is that basically the iterator causes this issue and may not work properly when another consumer of a queue modifies the queue while it is used.Steps to reproduce or test case.I have the following test case which can be used to reproduce the issue. It uses Lombok, Awaitility and Apache's RandomStringUtils for convenience (in case you want to run it without modification). Below I will explain steps to modify the test to get different results.The test has one producer thread pushing 200 random Strings (wrapped with QueueEntry objects) and one or more consumer threads "processing" these objects (parking them in an "in-progress-queue" - which is the problematic queue).If you run the test as it is here it will fail because the "in-progress-queue" is not empty.Modifications to change the test results:Change the number of consumer threads to 1 in the line marked with (4). Now the test should pass. Change the number back to something larger than 1.Comment in the RLock and its usages marked with (1). Now the test should pass. Comment the lines out again.Comment out the removeIf(...) statement (marked with (2)) and comment in the remove(...) statement (marked with (3)) instead. Now the test should pass.Instead of using removeIf() you could also modify the test to use an iterator and remove elements with it. The results will be similar.Default single server config with JsonJacksonCodec.
Rlock not able to handle 200 concurrent requests.Hi,I was trying to use distributed lock to do some operations and here I am facing an issue - it wasn't able to handle more than 200 concurrent requests.I am using redis as single server instance for this testing and here is my code.Please advise on what to look for to handle higher concurrency and handle the required requests per second. Our target was to handle around 1k concurrent requests.
Is there any method can tell multiLock is held by current thread?In RedissonMultiLock method isHeldByCurrentThread direct throw unsupportedOperationException.
RedissonFairLock.tryLock returns with false after timeout even when lock has been released.We use RedissonFairLock in our application to synchronize across multiple threads. Whenever there are several simultaneous lock attempts on the same lock, one or more of the waiting threads always errors out with tryLock returning false after the waitTime configured, which is almost 5 mins, even though I can clearly see that the other threads that were holding the lock have long since released it.Is there a known issues with RedissonFairLock? Also, would switching to a non-fair lock help (we don't really need fairness guarantees)?Code used to obtain lock -.Redisson version being used is v3.13.6.
the lock expiration  failed when the network breaked.I have a problem ,this is code:at imeUnit.MINUTES.sleep(5), break the network,the program throw a network exception,restore the network,find the lock expiration  failed,other thread can lock the key ,is a problem?
can i use redisson with spring cloud getway and webflux.when i use redisson in spring cloud getway filter,but when i unlock this rLock report error log not locked by current thread , this rLock is not the same thread when i locked, how can i fix this problem.
RMapCacheReactive.getLock returns org.redisson.RedissonLock instead of org.redisson.api.RLockReactive.Expected behavior.RMapCacheReactive.getLock should return an instance of org.redisson.api.RLockReactive.Actual behavior.RMapCacheReactive.getLock returns an instance of org.redisson.RedissonLock and throws exception:Steps to reproduce or test case.
Redisson 3.12.5 unlock hang forever.redisson version:3.12.5.redis version:5.0.7.redis cluster mode: 3 master 3 slave deployed in vm.question descript:In my production environment, a thread(dubheTaskExecutor-pool-2-thread-836) hangs up forever  whe invokes RedissonLock.unlock() . And I never found any error  in my application log. I am desired to find out if is a bug in redisson 3.12.5.The thread dump file is below(core part) :
internal RedissonBaseLock.evalWriteAsync() method isn't executed again if cluster slaves amount > 0.Expected behavior.Actual behavior.Steps to reproduce or test case.Redis version.Redisson version.Redisson configuration.
RedissonBaseLock can't be unlocked if exception got thrown from the same thread.RedissonBaseLock.java:312.opStatus is always null:RedissonPromise.java:183  (status of netty's Future is canceled due to Exception thrown from the same thread)I'm experience it permanently.At a first glance (if i'm not mistaken) this is a Blocker  level issue.RedissonPromise.java:187:Same behavior if Exception got thrown too early, such that f.getNow() results null.
Failed to cancel scheduled tasks and blocked threads.Expected behavior.Normal mission cancellation.Actual behavior.Failed to cancel the task and blocked the thread.Steps to reproduce or test case.When canceling a timed task, if the task is running, there is a chance that the task will not be canceled. If we use synchronous method to cancel the task, it will cause the current thread to block permanently, and the actual analysis shows that it will be in the scheduleCheck method.
There may be concurrency problems in org.redisson.RedissonLock#tryLockInnerAsync.if two thread get the same lock subject and run the method at same time, one's leaseTime is -1, and another's leaseTime is bigger than 30, the "-1" thread first cam in and set internalLockLeaseTime, then another thread came in set the internalLockLeaseTime bigger than 30, then if the bigger leaseTime one get the real redis lock! It while cause watchDog invaild and the lock be expired ahead of time.
Thread lock starvation with RSemaphore on high concurrency with hundreds of locks.We (@cstamas and me) are currently trying to integrate Redisson into Apache Maven Resolver. The purpose is to synchronize access to the Maven local repository across multiple processes. In our tests we experience a constant thread stravation where a write lock cannot be obtained in time, but tens of other threads are given access with RSemaphore. I assumed that we are having the same problem as in #1763, but the fiddling with config options did not work out. The lock is a hot path in dependency resolution.The amount of locks is around 500 accessed by around 100 Maven modules from 48 threads (T4C on a 12-core Xeon processor).Here is a sample condensed overview of semaphore permit acquisition and release:I can provide full debug output, actual source code (see PR), pre-compiled Maven distro and the sample project which almost always fails for me. Help would be very appreciated because I am at my wit's end.Note: The following lock types work flawlessly with the same project: rwlock-redisson (RReadWriteLock), semaphore-local (Semaphore) and rwlock-local (ReentrantReadWriteLock).
Async lock thread affinity.Hi,I'm working on a kotlin micronaut web app which is attempting to be fully async and we're using redisson for distributed locks.I'm somewhat unclear on how to exactly use it correctly.The official documentation says RLock object behaves according to the Java Lock specification. It means only lock owner thread can unlock it otherwise IllegalMonitorStateException would be thrown. Otherwise consider to use RSemaphore object. which sounds weird as in an async context, executing on a specific threads is never guaranteed.The code I'm using for now, in a kotlin suspendable func, is like this:The unlock line is not guaranteed to execute on the same thread as the lockAsync line.Thanks!
RLockReactive fail to unlock when subscribe to tryLock(Thread ID)RLockReactive fail to unlock with RedissonShutdownException when un lock it at the end of the reactive sequence with doFinally() or with then() operators. When testing it with StepVerifier the unlock is complete sucessfully but fail with subscribe().Example:
Trylock causes a deadlock,System restart can be unlocked.Expected behavior.Actual behavior.Steps to reproduce or test case.
RExecutorService does not work,block at future.get()redis_version:6.0.2.code.
Redisson renewExpiration not working after redis lock expires.Expected behavior.After a lock is expired or removed on Redis-Server a new lock should be created with renewalExpiration renewing TTL of a lock.Actual behavior.A new lock is created , but for some reason this new lock do not call renewExpiration but add this ThreadId to a map and  I dom't figure it out how it uses this map to renewal.On this specific case I was trying to use the same thread to reaquire this log,  I tried with a new thread but the result was the same.If I restart the applications, as this map is empty  the ttl renewal works as spected.Steps to reproduce or test case.Set a HIgh watchdog (30s) aquire a lock expire it manually on server. Aquire it on the same application.
Redis renewExpiration  ttl problem.Expected behavior.Allways renew the ttl key .Actual behavior.after looses a lock and try to aquire it again, sometimes the renewal feature doesn't work as it supposed.Steps to reproduce or test case.To reproduce it I had to use debug stop the execution before this call.with redis client remove the key.Then release the flow.if the key does'nt exists the renewal doesn't fire for this specific key, and it's everything ok.So my code aquire this lock again since it's not the owner, and everything seems to work except that it doesn't renew the ttl key lock because of this.In my point of view if you add.cancelExpirationRenewal(threadId)when the renewal fails , it solves the problem.
Thread lock starvation with RSemaphore on high concurrency with hundreds of locks (2)This is a followup to #3573. We have now received the first user feedback from @jebeaudet which exhibits the same behavior as in the previous issue.We have tried with Maven 3.8.x from the maven-3.8.x and Resolver 1.7.1 with Redisson's RSemaphore. Several runs observe the same behavior. One thread tries to acquire a lock which is not held by anyone else and after the time is up (30 s) no lock is obtained. @jebeaudet privately provided me log files of those builds and I have distilled the lock workflows from it. Here is a sample:The write lock artifact:ch.qos.logback:logback-classic:1.2.3 is not held by someone, yet it fails. I can provide several SQLite databases which contain raw imported data, prepared data as well as joined by lock workflows/transitions. The thread dump of a hanging build looks always the same:Nothing, but localhost....PS: There is a similar issue with RReadWriteLock, but I will report this seperately.
FairLock deadlock again.Expected behavior.redisson_lock_queue and redisson_lock_timeout keys disappear once the lock that was taken by the first thread is timed out automatically after lock lease time, and then the second thread successfully takes the lock.Actual behavior.Sometimes, even though the lock from the first thread is timed out and released automatically, the redisson_lock_queue and redisson_lock_timeout keys are stuck and the second thread can't acquire the lock, even though nobody is holding it. The second thread is stuck for full acquisition timeout. Then, it fails to get the lock eventually, and keys redisson_lock_queue and redisson_lock_timeout disappear, making the lock available again. Feels like lock expiry notification gets lost and Redisson waits for nothing. P.S. In this regard, would be great to have FairSpinLock not to rely on pubsub.Related? #2883.Steps to reproduce or test case.1 redis, 1 sentinel.
redis lock not working as expected.I have the following test scenario:My expectation is the following: if the lock couldn't be acquired in 1s  then it should show Failed to acquire lock. If the lock is acquired then, in 30s, it should be automatically released by Redis (this 30s are very important as I only need to allow another run after this time). Only one should should be acquired in 30s. The problem is that most of the times I get multiple Lock acquired between 30s intervals.The code launches 1000 parallel execution tasks. Since Redis is (mostly) single threaded then I wasn't expecting any concurrency problems (and I'm pretty sure they aren't on the Redis side). I am also using a single node Redis.I have also tried another solution using Zookeeper Leader Election and that one works as expected using the same caller function. It acquired a single lock at a time.Am I doing something wrong here are we having a Redisson issue?
Different effects when Mix using of multiLock and Lock.version:redisson-3.14.1.When try to using both multiLock and Lock with same key.There are different effects according to the lock order.When lock lock1 first ,then lock multiLock1.multiLock1 stun for a few seconds and then, lock success(lock1 are not released)When lock multiLock1 first ,then lock lock1.lock1 are stoped and waitting for the multiLock to release.it's confused.
RedissonKeys.Delete() method pass empty array will block the thread in redis cluster.Expected behavior.Exception throws like single redis server.Actual behavior.In cluster mode, this method will block the thread and no exception throws.Steps to reproduce or test case.
RedissonLock Perhaps leak.The tryLock(long waitTime, long leaseTime, TimeUnit unit) method, if the subscribe timeout, and then subscribeFuture.cancel success,when to call unsubscribe , the subscribeFuture.onComplete should move out ,before the if block.
RedissonFairLock does not automatically clear non-running LockName and non-reference lock methods after a restart and cannot be automatically renewed.Expected behavior.Actual behavior.Production environment with a large number of requests queued.At around 19:23 on the night of September 9, due to the existence of a large Key in the store, after submitting a work order to delete the large Key, there was still an imbalance of data in the cluster slice, and the DBA tried to perform a memory analysis of the cluster, resulting in a Redis slice switch and the loss of some of the Keys.Redisson's unlock mechanism will go to determine the LockKey Delete state, if Delete succeeds then publish a message to notify other clients subscribed to this lock to compete for the lock, if Delete fails then there is no message notification (Delete is a failure state when Key does not exist), thus causing the clients subscribed to the lock to wait until the thread wait timeout time (default is 5 minutes) to compete for the lock again.Steps to reproduce or test case.Creating a large number of concurrent requests for locks.The business processing request time to obtain the lock is set longer than 30s.If thread A acquires the lock and manually deletes the key of the lock, i.e. RawName.Observing other threads acquiring locks, you will find that you need to wait a long time to get the lock, and after restarting the main method of Test, you cannot empty the last waiting queue, but can only wait for the default 5 minutes to clear the non-running state LockName(id:ThreadId)1. RedissonFairLock.lock will not automatically renew.
RLockReactive tryLock waits longer than waitTime.Expected behavior.long id = Thread.currentThread().getId();... lock.tryLock(0, 10, TimeUnit.SECONDS, id) ...Expect this to exit immediately - reporting "true" if it can get the lock now ; and reporting "false" if it can not.Actual behavior.It seems to block and wait until the lock becomes available.The waitTime (set as 0 above) does not seem to influence the behaviour.Steps to reproduce or test case.Invoke "lock.tryLock(0, 10, TimeUnit.SECONDS, id)", when the lock is already taken.Expect it to instantly return with "false". It does not.((Note: I am working inside Quarkus, and using "reactor.core.publisher.Mono" to convert the Redisson Reactive Core (Mono) into Mutiny (Uni) - this does not seem related ; but it makes it harder for me to frame a pure-Redission test case))
Redisson getLock operation does not respect timeout.Expected behavior.Redisson getLock timeouts after timeout.Actual behavior.Redisson getLock timeouts after timeout + 1 second.
org.redisson.client.RedisResponseTimeoutException: Redis server response timeout when use RLock.My Redis don't use cluster just only one node address.I used RLock for locking but was not successful because of Redis timeout. I restarted the service, it worked but it's not the best way in the production stage.I referred to the same issue about Redis timeout such as an increased nettyThreads or timeout config but it's not successful.
Concurrent attempt to RReadWriteLock writeLock leaves thread in blocked state after lock is released.Two Threads, thread-1 and thread-2 attempt to acquire a RReadWriteLock.writeLock() using lock().  thread-1 gets the lock, thread-2 waits for lock to be released.  thread-1 releases lock, thread-2 acquires lock.When thread-1 releases lock (using unlock()), thread-2 does not acquire lock.  Wait for lock expires after 35 seconds.2 threads call lock() on the same RReadWriteLock at or near the same time.thread that gets the lock release the lock (after a second or two).Note that thread-2 doesn't automatically get the released lock..Redisson version 3.16.3.
Stuck on joining HashedWheelTimer worker thread when shutting down.Sometimes Redisson seems to be stuck when calling Redisson.shutdown(). Some Services are stuck for mutliple days. But it seems to be a very rare event,This is the Thread netty tries to join. Stack trace does not seem to change over time (took multiple thread dumps)Expected behavior.Redisson.shutdown() should be blocking but not for ever.Actual behavior.Redisson.shutdown() is stuck for days (in some rare cases) and the Timer thread is taking huge amounts of processing power (this is not a typical profiler bug with parking threads but acutally showing in the process list)Steps to reproduce or test case.Not tried but maybe if you could point us in a direction.
Got a NullPointerException in WriteReadLock.the redis model is cluster.
Error while aquiring ReadWriteLock and Lock at the same time.When I call method runWithRwLock and method runWithMultiLock at same time, it works fine. When I call method runWithLock and method runWithRwLock/runWithMultiLock, the following error is frequently reported.This is my test code:
Error while acquire Lock and ReadLock.Steps to reproduce or test case.Let me simplify the reproduce process:Thread x get Lock "A" and hold the Lock.Thread y get ReadLock "A" and also hold the ReadLock.Thread x and thread y all acquired lock.Thread y call method unlock to release ReadLock y.After a few seconds, thread x also call method unlock to release Lock x,and then it causes an exception.Here is stacktrace.
multilock fail.Expected behavior.lock successful.Actual behavior.lock fail when get first lock in multi lock case.Steps to reproduce or test case.when I set waitTime=0in multi lock , it fail, but I set waitTime=0 in RLock, it success, why?then , debug RedissonMultiLock,  until Line 413.I think it is a bug in RedissonMultiLock.
multi lock fail.Expected behavior.lock successful.Actual behavior.lock fail when get first lock in multi lock case.Steps to reproduce or test case.when I set waitTime=0in multi lock , it fail, but I set waitTime=0 in RLock, it success, why?then , debug RedissonMultiLock,  until Line 413.I think it is a bug in RedissonMultiLock.
MultiLock causes deadlocks in high concurrency cases.Expected behavior.No DeadLock.Actual behavior.DeadLock.Steps to reproduce or test case.The complete code is as follows.Deadlocks don't happen every time. After running the program for a while, I found that the process stopped and the keys in redis were fixed, not added or reduced. What is even more strange is that the expiration time I added the lock is 800ms, and if I use the ttl command to get the expiration time, the result should be 0 or 1, or a negative number. But, I found a number greater than 1! And it keeps changing, like this: 4s -> 3s -> 2s -> 4s.
Thread safe problem when acquiring locks resulting in a pubsub channel not being unsubscribed  64 .This seems to be related to:  #2575.Expected behavior.The pub/sub channels that are created while trying to acquire a lock with RedissonLock should be unsubscribed when no longer needed.Actual behavior.In some cases when 2 pub sub channels are created around the same time for (2 different lock keys) one of the two throws an error:Subscribe timeout: (7500ms). Increase 'subscriptionsPerConnection' and/or 'subscriptionConnectionPoolSize' parameters.After this error the failed lock key will lock fine when not locked. If it is however locked by another thread it will fail instantly.see the logging from the test case below, notice how thread [pool-1-thread-12] successfully locks key 0. When thread [pool-1-thread-19] tries to lock lockkey-0  it instantly throws the subscribe timeout exception (notice it does not wait 7500ms).If there is no locking going on and the app is still running it kept a channel active for lockkey-0.Some logging from the test case when it fails:
ERR WAIT cannot be used with slave instances.This occurs when trying to acquire a lock.Currently, we are using redisson library 3.16.3 with readmode: SLAVE.
Redisson : Issue while unlock lock for a key ,  attempt to unlock lock, not locked by current thread.Hi, I am getting below errors in my logs.I am using below dependency for redisson:And I've implemented a wrapper that will acquire and release the lock:But the log "Released lock for key" is logged rarely. The task after acquiring the lock gets finished within 1 or 2 seconds and the wait and lease time are 5 and 15 secs respectively.RedisConfig is :We have redis server in Kubernetes cluster with 2 pod instances both as master.Would appreciate any suggestions?
response timeout in a job lead to deadlock.redisson version: 3.9.0.action:creating a thread makes a job every 5 seconds.job acquire fairlock  trylock(10, 10, timeunit.seconds)redis busy or bad network, response timeout(default 3s)5s, this instance will reentrance  lock.other intance will not acquire lock.suggestion:when redis response timeout , should redisson release the lock?
Unable to Acquire Fair Lock.In my code, I am trying to acquire a fair lock with a fixed waitTime and leaseTime as so.fairLock.tryLockAsync(waitTime, leaseTime, timeUnit, ID)Sometimes, all the requests will get queued up for apparently no reason, while no request has acquired the lock.I can see a wait queue being formed for the lock, but the actual lock is not held by anyone.Eventually, all of these waiting requests will time out and fail, and new requests will work fine.This seems to happen randomly when there are a lot of simultaneous requests.I am not able to reproduce this locally.
when i set waitTime=0 in multi lock, it fail.about issue#4033,I don't need to wait,so i set waittime=0,but the result is failure,why Rlock set waittime=0 result is successful?i need get the lock fail,end business at once.see source code,when waitime=0,leaseTime assignment zero.Expected behavior.Actual behavior.Steps to reproduce or test case.
Only one thread can get lock after unlock timeout.I have an annotation RLock which works with an aspect, it work well in some programs.Here is the codes:Someday a program throws Exception at every tryLock in two of three containers. I checked ELK and find RedisTimeoutException before the first exception.By the way, every container works with only one thread to consume kafka messages.It seems taht  the first RLock unlock timeout and then redisson treats next RLocks as reentrant cause they have the same thread Id.How to ensure that the lock could be released correctly after unlock timeout.
isHeldByCurrentThread() goes wrong.
RScheduledExecutorService's  WorkerOptions.workers() doesn't work.and i found it will consume 16 thread once instead of 5.
RateLimiter error during unpack on redisson 3.15.0 - bad argument #2 to 'unpack' (string expected, got nil)Expected behavior.No error when trying to acquire a RRLimiter lock.  Error seen when multiple threads from one application try to acquire a lock from a 3 member redis cluster.Actual behavior.Intermittent error seen,I believe this internal script's function is the source of the problem:I am not so sure why v would be nil.Steps to reproduce or test case.Not easily reproducible as of yet.3 member redis cluster with sentinel for HA, running in kubernetes 1.20.2.
RLock lease expiry not being extended in spite of RLock being acquired.Our Microservice is running in Azure. It is deployed with 2 pods. Each pod attempts to lock the same RLock, the idea being that only one pod at a time holds the lock.RLock is obtained using org.redisson.api.RLockAsync.lockAsync() and onComplete() lambda is used to get notified about lock acquisition.We observed following: pod1 had the lock but then pod2 got notified of lock acquisition.So we lock again and within 1 msec get notified about success: heldByCurrentThread=true, ttl=29998.This cycle continues from then on:we get the lock without issues, very quickly.within 30 seconds we lose the lock and so go back to 1)I think 30 seconds matches lockWatchdogTimeout setting. I see no warn/error log related to Redis connectivity.I have logs for around 3 hours when this issue has persisted but it might have well lasted for much longer.Expected behavior.If a client acquires RLock then it should also be able to extend its (lease) time.Actual behavior.A client acquires RLock but then seems unable to extend its (lease) time.
Spring Data Connection in multi mode causes thread stuck.No description provided.
ClassLoader issues when using Redisson Transactions with asynchronously acquired RedissonClient.Expected behavior.When using an asynchronously acquired RedissonClient (via. CompletableFuture.supplyAsync) with transactions I expect the class loader used to be correct.Some constraints to narrow the scope of the problem:I only encountered this when using Redisson Transactions.If the client was acquired in a synchronous way, I did not see this behavior.Actual behavior.Similar to #2984 I am seeing that on the first request to my WAR on a web server, the correct class loader is used.But then on subsequent requests, it is using the URLClassPath loader what I assume to be the default for Jetty.After this breaks, I get consistent ClassNotFound errors.Steps to reproduce or test case.I created a minimal jetty project here in an effort to reproduce the code we are running.In words, my steps were as follows:Create a Jetty Server running 9.4.30.v20200611, and deploy a WAR on it. Place objects into Redis, using transactions, and try to retrieve them. After the first request succeeds, observe ClassNotFound issues when trying to retrieve other objects. The stack trace can be found in a gist.
RedissonList concurrency problem.How is it ever safe to rely on RedissonList.size() when each invocation can technically return a different value? This would mean that calling subList(0, size()), or even two consecutive calls to hasNext() on the iterator (which is normally what happens) can potentially cause an exception.
Subscribe Timeout Exceptions - After master Re-election.None, or fewer Exceptions (just during failover period)After a master re-election (and not otherwise) a constant percentage (about 1%) of the requests fail with this exception until the Redisson client is re-initialised manually with a pod bounce.Steps to reproduce or test case.I'm running 3 redis (master, slave, slave) alongside 3 sentinels.I'm running a constant load of requests through a redissonclient which can run indefinitely without any issue..When I introduce a chaos script to trigger a failure of the master and thus a master re-election I start to see the errors, yet only for a very small percentage of the requests (but that percentage remains constant indefinitely until a manual restart of the redisson client happens)Each request acquires and releases an RLock with a chance of collision with another request being 1 in 100.If I lower the collision probability to be 1 in 10000 this problem almost completely disappears to the point where I lose only perhaps a single request, during the failover period (which is the expected behaviour for the higher collision case also)
io.netty.resolver.dns.DnsNameResolver#doResolveCached race condition java.lang.IndexOutOfBoundsException.It's an known bug in io.netty 4.1.19.Final and fixed in 4.1.20.Final.netty/netty#7583.Sometimes my AWS Lambdas fail by creation of the redisson client.
Ack object used to prevent race-condition during ack receiving should be created per request.Because of this issue RemoteServiceAckTimeoutException could be thrown prior the end of ackTimeout.
Long latency issue while call Redis.I use Redisson 2.2.10 to implement distributed lock in my project. And then I run load test for this distributed lock using SINGLE redis mode. But I found there are many long latency APIs and it's wired that almost all the long latency APIs are a little greater than 1000ms. The average latency and 99 percent latecy is about 15ms and 30ms respectively. And then I used jstack to get the call stack and found most of the threads are WAITING at the same place just as the following.Meanwhile, during the test, I dumped the network packages using tcpdump and found that it took about 1000ms before the first package was sent to Redis. And in Redis server, I didn't find any slow query whose latency is more than 10ms. So I think, for the long latency API, most of the time might be cost in the project not the network. So can you help me to have a check or give me some suggestion about this issue?  Many thanks.
SAdd in RedisAsyncConnection uses wrong AddValues.line 629 uses addMapValues for sadd rather than addValues.This causes JsonJacksonCodec to fail on Longs stored with the mapObjectMapper to fail to be deparsed by objectMapper;Change line 629.Test code follows :
RedissonList concurrency problem.How is it ever safe to rely on RedissonList.size() when each invocation can technically return a different value? This would mean that calling subList(0, size()), or even two consecutive calls to hasNext() on the iterator (which is normally what happens) can potentially cause an exception.
"Connection Leak" in RedissonBucket.Hi guys!When I get some error, eg: redis servers down, in getAsync and setAsync the connection is not released, once the release is done in a Promise listener, that never executes, can you check this please?Regards,Marcio.