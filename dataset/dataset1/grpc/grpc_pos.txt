605
BufferingHttp2ConnectionEncoder does not shutdown properly on channelInactive
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
@nmittler
There is a nasty race condition during the handling of channelInactive in NettyClientHandler which goes a bit like this....
This reproduces for NettyClientTransportTest.bufferedStreamsShouldBeClosedWhenTransportTerminates with 5.0beta5.
Having streams being created as a side-effect of channel inactivation is undesirable. Potential fixes include
Reorder teardown in Http2ConnectionHandler.BaseDecoder.channelInactive so encoders are closed() before streams are closed.
Make BufferedHttp2ConnectionEncoder check channel.isActive() when trying to create streams.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

789
Simplify Netty pipeline using Buffering handler · Issue #789
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The current pipeline configurations using one of the buffering handlers (tls, plaintext, etc.) are rather complicated and it's not always clear which handlers will handle exceptions in various cases.
We currently add the buffering handler and the HTTP/2 handler at startup. The buffering handler holds any writes until the startup handshake (e.g. SSL/TLS) completes, at which point it directs all buffered writes to the HTTP/2 handler. While those writes are occuring, the buffering handler stays in the pipeline (this is due to threading behavior of Netty WRT writes occuring outside of the event loop). If any problems occur while those writes are taking place, exception handling could occur in either the buffering handler or the HTTP/2 handler.  It would be desirable to guarantee that exception handling can occur in only a single place at any point in time.
Proposed change:
Part 1): Add a ChannelHandlerAdapter as the last handler in the pipeline. Netty has a race condition when writes occur from outside of the event loop.  The last ChannelHandlerContext is extracted in this thread and then the write is called. If however, the pipeline is changed between when the context is obtained and the write occurs ... badness ensues. As a workaround, there is some hacky code in the buffering handler to account for this race. A better solution to this problem would be to simply enforce the existence of a handler at the tail of the pipeline which never changes.  This will just be a pass-through, but must implement the write method (this is to avoid another Netty gotcha, where it will skip handlers if it has determined that they are uninterested in the event).
Part 2): With the handler from Part 1 in place, the installation of the buffering and HTTP/2 handlers can be modified to a replace.  Initially, only the buffering handler is installed (not the HTTP/2 handler). When the startup handshake completes successfully, the buffering handler will replace itself with the HTTP/2 handler, and then empty it's queued writes to the HTTP/2 handler.
In this way we guarantee that only one of these handlers exists in the pipeline at a time.  Failures due to the initial handshake will be handled by the buffering handler. Failures due to writes will always be handled by the HTTP/2 handler.
@ejona86 FYI
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

999
Possible race condition ServerImpl between start() and shutdown() · Issue #999 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
I believe it may be possible if start and stop are called concurrently that the shared executor may not get released.  I'm not sure if this is an actual problem, but it does go against the @ ThreadSafe annotation.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

2865
Rare race condition in Client · Issue #2865 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
While more prominent when using compression, this race occurs without it as well.  The typical race looks something like:
Client starts and RPC
The transport to the server is not yet available, so a DelayedClientTransport is used.
The server handles the RPC and sends back headers and a compressed message.
The client sees there are headers, and begins executing the queued stream callbacks, on the channel executor threads instead of the transport thread
The client sees the Data frame, and tries to decompress it on the network thread.  *This fails since the headers from 4 have not yet been processed.
The stream has already failed, but the queued callback for onHeaders() is finally executed on the app thread.
This is the root cause of #2157.  As mentioned, this isn't just for compression.  ClientInterceptors will see headers after data has been received.   The solution (temporary?) seems to be to move OkHttp to used AbstractClientStream2 in #2821, and then move decompression from ClientCallImpl to the stream.  That will fix the decompression, but not interceptors.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

3421
First received frame was not SETTINGS. Hex dump for first 5 bytes · Issue #3421 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Please answer these questions before submitting your issue.
What version of gRPC are you using?
What JVM are you using (java -version)?
java version "1.8.0_131"
Java(TM) SE Runtime Environment (build 1.8.0_131-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)
What did you do?
Inside a test I ran a grpc server and 5 clients that accessed the server concurrently (all from the same IP; everyone with its own channel). This is only happening sometimes, looks to me like some kind of race condition within grpc.
What did you expect to see?
Successful call
What did you see instead?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

120
Remove blocking parts from NettyClientTransport
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
NettyClientTransport#newStream is currently a blocking operation. It blocks until the HEADERS frame has been written on the wire. This is behavior is not what people who use our asynchronous API would come to expect.
The blocking also is the cause for severe performance issues in the QPS Client as it results in more or less in as many threads being created as there are concurrent calls going on (We have seen ~850 Threads for 1000 concurrent calls, resulting in OOM).
The blocking may also lead to deadlocking the EventLoop in cases where a DirectExecutor is used. One scenario where a deadlock might happen is when the EventLoop is not able to completely flush the HEADERS frame on the wire because then Netty would internally create a task to flush the remaining bytes and put this task in its task queue. This task can never be completed though as the EventLoop Thread is blocked by our very own newStream method waiting for the task to be completed ...
This issue depends on #116 and #118 to be resolved first.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

696
In-process transport deadlock during shutdown
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Simultaneously shutting down both server and client sharing the same in-process transport can lead to a deadlock. During server shutdown, the transport lock is held while calling transportShutdown on the channel listener, which attempts to lock the channel. At the same time, channel.shutdownNow() holds the channel lock while also trying to lock the transport which leads to a deadlock:
Found one Java-level deadlock:
Found 1 deadlock.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1175
Connection window should auto-refill · Issue #1175 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Consider an application with 2 streams, A and B. A receives a stream of messages and the application pops off one message at a time and makes a request on stream B. However, if receiving of data on A has caused the connection window to collapse, B will not be able to receive any data and the application will deadlock. The only way (currently) to get around this is to use multiple connections for the streams, which would needlessly complicate the application code.
The C implementation already solves the problem by auto-refilling the connection window, so Java and the other languages should just follow suit here.
Created PR netty/netty#4423 in Netty to support configuring the local flow controller to auto-refill.  Until that's in, we should probably just set the connection window to MAX_INT for now.
@louiscryan @ejona86 @ctiller @a11r
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1408
Potential risk of deadlock from calling listeners under locks · Issue #1408 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Methods of ClientTransport.Listener and ServerTransportListener are usually called under a lock. The biggest reason for locking is to guarantee the ordering of multiple methods on the same listener.
However, these listeners usually call into channel layer code, and may in turn acquire locks from there, which forms a transport lock -> channel lock lock order. On the other hand, when channel layer calls into transport layer, it's possible to form a channel lock -> transport lock lock order, which makes deadlock possible.
It's unlikely an issue today because there is an implicit rule today that channel layer will not hold any lock while calling into transport. However, as the code base grows, it will become harder to keep track of such requirement.
A possible solution is to always schedule listener calls on a serialized executor, with the cost of a thread, so that listener order can be guaranteed without the need of locking. There may be better options.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1510
DelayedClientTransport and InProcessTransport means deadlock · Issue #1510 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
There is a chance of deadlock when DelayedClientTransport is linked with an InProcessTransport. See /pull/1503.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1549
Simplify implementation of back-pressure in StreamObserver-based stub · Issue #1549 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Pending API changes can allow reactive/async pattern for interacting with flow control and applying back pressure: https
In many cases, automatic back-pressure in generated stubs could be very useful -- e.g. having calls to StreamObserve#onNext(T) block instead of queueing.
It's been pointed out that this could cause deadlock for bidi-streaming operations, so perhaps we can just not expose this functionality for bidi-streaming calls?
It may also be worth pointing out that most other runtimes (wrapped languages and Go) already expose streams via blocking operations and already require that apps be aware of and work-around possible deadlock issues resulting therefrom. So maybe providing similar mechanisms in Java is fine, with said caveats.
Another possible alternative could possibly be done in an extension/add-on instead of in GRPC. For example, wrapping streaming requests and responses with RxJava Observables may further simplify the async case enough to make the synchronous (and possibly-deadlock-prone) case unnecessary.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

2152
Deadlock found in TransportSet · Issue #2152 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When running benchmarks where the client started up faster than the server, The first few calls failed as unavailable.  Our internal deadlock detector seems to think there is a deadlock around here:
Deadlock(s) found:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

2246
Deadlock with TransportSet · Issue #2246 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hello,
I was testing Grpc with RoundRobinLB and a custom NameResolver when this deadlock happened:
Found one Java-level deadlock:
I don't know if it may relate to my own code or if the issue is on grpc side.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

2388
New deadlock in TransportSet and GrpcTimer · Issue #2388 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi,
I have encountered a new deadlock in TransportSet. I'm running under v1.0 with #2258 cherry-picked.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

2515
Deadlock in grpc due to recursive grpc call · Issue #2515 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Please answer these questions before submitting your issue.
What version of gRPC are you using?
1.0.1
What JVM are you using (java -version)?
openjdk version "1.8.0_102"
OpenJDK Runtime Environment (build 1.8.0_102)
OpenJDK 64-Bit Server VM (build 25.102-b01, mixed mode)
What did you do?
If possible, provide a recipe for reproducing the error.
Turned on FINE logging
Used the Logging client in google-cloud-java
What did you expect to see?
Not what I saw below...
What did you see instead?
Deadlock.
Found one Java-level deadlock:
Java stack information for the threads listed above:
Background: I have been trying various strategies to resolve googleapis/google-cloud-java#1386 , where using the Logging service at level FINE results in grpc logging to the Logging service in a recursive way. I tried using a ThreadLocal to prevent this, but this doesn't work with grpc because the actual call is executed on a worker thread. Essentially I think I need some way to bail out of the LoggingHandler.publish call if I can detect that this is in the scope of a grpc worker thread sending a request to the Logging service.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

3084
Potential deadlock due to calling callbacks while holding a lock · Issue #3084 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
InProcessClientStream and InProcessServerStream are synchronized on their own. InProcessClientStream.serverStreamListener is called under synchronized (InProcessClientStream.this), and vice versa.
If the application tries to call methods on ClientCall or ServerCall from within the callbacks (assuming that it has already taken care of the thread-safety of the method calls on "Call" objects), a deadlock is possible when direct executor is used. For example:
Thread1
Thread2
As locks are acquired in reverse orders from two threads, a deadlock is possible.
The fundamental issue is that we should not call into application code while holding a lock, because we don't know what application code can do thus we can't control the order of subsequent locking.
OkHttp has the same issue, because OkHttpClientStream.transportDataReceived(), which will call into application code, is called under lock.
We could use ChannelExecutor (maybe renamed) to prevent calling into callbacks while holding a lock.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

116
Buffer Messages until TLS Handshake and HTTP2 Negotiation complete
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When grpc uses Netty as the client transport all RPC calls (aka HTTP2 Streams) block until the TLS Handshake and the HTTP2 negotiation is complete.
This blocking implementation (in grpc) is currently required as Netty's SslHandler doesn't buffer messages until the Handshake is complete ("You must make sure not to write a message while the handshake is in progress unless you are renegotiating."), and there is nothing to stop the user from starting to make RPC calls immediately.
This behavior comes with two problems:
With RPC calls blocking until the TLS Handshake is complete, every call launched before the TLS Handshake and HTTP2 Negotiation are done will block its thread from which one would expect async behavior though.
In cases when a DirectExecutor is being used it might lead to the EventLoop blocking forever (deadlock effectively). There is several scenarios how a deadlock could happen. One such scenario is when you are writing a server in Netty and within that server you want to connect to a grpc service to fetch some data. If you now use a DirectExecutor and reuse the EventLoop of the server with the grpc client, the TLS handshake would block the server's EventLoop, which is also the very EventLoop responsible for completing the TLS HandShake. That way neither the server nor the client would ever make progress again.
@nmittler , @ejona86 and I talked about this problem earlier today and we agreed to get rid of the blocking behavior by adding an additional ChannelHandler to the end of the pipeline (tail) that will buffer any data until TLS & HTTP2 are working. After that it will send the buffered messages through the pipeline and remove itself from the pipeline.
@nmittler @ejona86 @louiscryan
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

330
OkHttpClientTransport.onGoAway() races with startPendingStreams()
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
onGoAway has two phases: do things necessary under lock and final cleanup. In the first phase it collects the streams to terminate in the second and sets goAway.
startPendingStreams() does not observe goAway and also creates new streams that should be failed due to the goAway. From an initial look, it seems it would be best to remove failPendingStreams() and simply integrate its two phases into onGoAway()'s two phases; that is, when holding the lock in onGoAway, replace pendingStreams with an empty list, and then when not holding the lock call transportReportStatus
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

583
OkHttp's cancellation is not properly synchronized
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
OkHttpClientStream.sendCancel() calls finishStream() from an application thread. But finishStream() calls transportReportStatus() without any lock held. That is not synchronized correctly, as transportReportStatus() may only be called from the transport thread (i.e., while lock is held).
It seems that all usages of streams is done while lock is held except for within finishStream() and data(). data() can actually race with finishStream() and end up sending DATA frames after the RST_STREAM. It seems it would be best to just have stream protected by lock, because it having its own synchronization isn't providing much benefit and isn't leading to correct code.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1290
Propagate RuntimeException's back to caller · Issue #1290 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When using ManagedChannelImpl, if a method of ClientCall.Listener (possibly also of ClientCall itself) throws a RuntimeException, the exception propagates up into SerializingExecutor. The executor will log it under SEVERE but cannot take corrective action. The thread initiating ClientCalls.blockingUnaryCall or its cousins will block forever waiting for a response.
Ideally, the exception should be thrown back in the thread that called blockingUnaryCall.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1905
Errors with concurrent server push · Issue #1905 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hi, I'm getting errors when making concurrent server pushes to the same client connection.
You can see simplified repro here https:
Just clone and run mvn test to see repro.
It is supposed to send 100 messages, but generally the client does not receive 100, and there are errors logged in the process, e.g.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

2015
Race on NettyClientTransport.start · Issue #2015 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Bootstrap.connect seems to add a listener to a ChannelFuture that is concurrently modified by the NioEventLoop and the client provided executor in ConcurrencyTest
Conflicting accesses:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

17
Race for Netty between cancel and stream creation ·
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
AbstractClientStream.cancel won't cancel the stream on the wire if it appears the stream has not yet been allocated, as is described by the comment:
// Only send a cancellation to remote side if we have actually been allocated
// a stream id and we are not already closed. i.e. the server side is aware of the stream.
However, what happens if this is the case, is that the transport is not notified of the stream destruction, and the stream will still eventually be created by the transport and not be cancelled. This issue does not seem a problem with the OkHttp transport, since it allocates the stream id before returning any newly created stream. However, Netty delays id allocation until just before the stream headers are sent, which 1) is always done asynchronously and 2) may be strongly delayed due to MAX_CONCURRENT_STREAMS.
It appears that the optimization in AbstractClientStream should be removed outright and sendCancel's doc be updated to specify the expectation to handle such cases (as opposed to directly cause RST_STREAM). Both OkHttp and Netty seem to be handling such cases already. More importantly, the optimization seems highly prone for races given that id allocation is occurring in the transport thread whereas AbstractClientStream.cancel is happening on some application thread; using the normal synchronization between application and transport threads seems more than efficient enough and simpler.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

238
Race in Server handler initialization
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
When initializing an incoming client connection, we call startAsync() on the transport, which registers the handler on a separate thread. This is obviously a race, and it would have probably been fixed if I had finished Service removal in #35.
Symptom:
The quickest fix would be to call awaitRunning() from initChannel(). That reduces the rate new connections can connect, but is probably the most expedient solution, until #35 is finished.
@nmittler, thoughts?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

887
OkHttp: race between sendCancel and sendFrame. · Issue #887 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
If sendCancel is called (by timeout for example) before the stream is started, a following sendFrame will cause a NPE:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1253
ClientCallImpl operations race with Context cancellation. · Issue #1253 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
With be60086, we don't create the stream when the Context is cancelled, so the following request(), sendMessage(), halfClose() will encounter an IllegalStateException like:
@louiscryan, FYI, I'll send you a PR to fix it soon.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1343
Deadline can fire before stream started · Issue #1343 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
In ClientCallImpl the deadline is scheduled before stream.start(). However, if the deadline has already elapsed the runnable will be executed immediately and race with the start. I've only looked into how OkHttp may be impacted.
I believe a NullPointerException would be thrown when trying to notify the stream listener due to the cancellation. However, due to #1237 the exception won't be logged. Thus, this will result in a hung stream that never completes with no logging as to what went wrong.
This was discovered due to timeout_on_sleeping_server on android being flaky, because it uses a very small timeout. The test would fail at awaitCompletion.
@carl-mastrangelo, FYI
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1569
okhttp: race between receiving data and closing deframer · Issue #1569 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
As reported on SO:
When receiving a DATA frame, it seems there is a race between getStream() and calling transportDataReceived(). Although I wouldn't expect to trigger that race often.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

1981
Executor usage in `ClientCallImpl` races with channel shutdown and termination. · Issue #1981 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
ManagedChannelImpl clear scheduledExecutor in shutdown(), and releases (which potentially closes) executor in maybeTerminateChannel().
Neither newCall() nor ClientCallImpl checks the shutdown state of the channel. ClientCallImpl relies on FailingClientTransport for the expected behavior. However, ClientCallImpl uses the passed in executors anyway, for scheduling the deadline timer and invoking the call listener.
If ClientCallImpl tries to schedule a deadline timer after the channel is shut down, it will get a NPE. If it runs the call listener after the shared executor has been closed, which is 1 second (SharedResourceHolder.DESTROY_DELAY_SECONDS) after all references are gone, e.g., the application calls Call.start() that late, it will get a RejectedExecutionException. Our current tests are not testing for the two cases.
This doesn't seem to be a serious issue. It only affect people who try to use Calls after the channel has been shutdown. I am yet to figure out a solution.
Anyway, it seems executor should be cleared after being returned to the shared pool, like scheduledExecutor.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

2014
Race on NettyServer shutdown. · Issue #2014 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Even with 2292cbf it seems there is still a race with shutdown.  Initial guess is that the shutdown and close are happening in the Boss Event loop and the normal event loop.  Conflicting reads and writes:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

2377
Census Race · Issue #2377 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Read of size 4 at 0x7f00465dfbcc by thread T123:
cc: @adriancole @zhangkun83
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

2453
Threading of StatsTraceContext · Issue #2453 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
StatsTraceContext assumes non-thread-safety, which is fine as long as the RPC is closed by the application through the ClientCall/ServerCall interface, which are also not thread-safe.
However, if the RPC is not closed by the application, but either cancelled by the other side, or closed by transport due to errors, which will call callEnded() from the transport thread which is not synchronized with  the application thread. As the application may not be notified about the closure in time, it may still trying to send messages, resulting in wireBytesSent() etc being called after callEnded(), which would trigger a check failure. There is also a data race on the counter fields as wireBytesSent() etc write them and callEnded() reads them from different threads without synchronization.
We will remove the preconditions checks from writeBytesSent() etc. For the data race, some kind of synchronization would be required, maybe atomics? @ejona86
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

2562
Race between pick and transport shutdown · Issue #2562 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Right now they are done in two steps:
A transport that is in READY state is selected
newStream() is called on the selected transport.
If transport is shutdown (by LoadBalancer or channel idle mode) between the two steps, Step 2 will fail spuriously. Currently we work around this by adding a delay between stopping selecting a subchannel (which owns the transport) and shutting it down. As long as the delay is longer than the time between Step 1 and Step 2, the race won't happen.
This is not ideal because it relies on timing to work correctly, and will still fail in extreme cases where the time between the two steps are longer than the pre-set delay.
It would be a better solution to differentiate the racy shutdown and the intended shutdown (Channel is shutdown for good). In response to racy shutdown, transport selection will be retried. The clientTransportProvider in ManagedChannelImpl is in the best position to do this, because it knows whether the Channel has shutdown. clientTransportProvider would have to call newStream() and start the stream, and return the started stream to ClientCallImpl instead of a transport.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

3843
Race in io.grpc.internal.MessageFramer.commitToSink · Issue #3843 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
messagesBuffered seems to be updated on error and from the application thread.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

3856
Netty Flow Control tests have data races · Issue #3856 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The window size read from the testing thread should be executed on the event loop.  Since this test is flaky, and fixing this race would make the test more flaky (but less racy), the test is being marked @Ignored.
This issue tracks reenabling the flow control tests and fixing the data race.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

zzzzzz

6601
Deadlock on start gRPC server · Issue #6601 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
What version of gRPC-Java are you using?
What is your environment?
openjdk version "1.8.0_171"
OpenJDK Runtime Environment (IcedTea 3.8.0) (Alpine 8.171.11-r0)
OpenJDK 64-Bit Server VM (build 25.171-b11, mixed mode)
What did you expect to see?
Proper start of gRPC Server
What did you see instead?
Start sometimes hangs with deadlock
Steps to reproduce the bug
I suppose it's a race condition related to synchronization in gRPC (ServerImpl), await in NetServer.start and vertx/nettty event loops (probably single threaded). Probably it could happened at any time if someone start gRPC server and concurrently open new client connection to that server.
In my case I stopped and started the gGPR server but I'm not sure if it is somehow related.
Analysis
What I see in the thread dump is the following 2 threads that stays in that state, seems, forever:
From what I see in these thread dumps and the code I think that this could be the problem (deadlock):
Vertx grpc starts server (ServerImpl.start) in vertx blocking thread
ServerImpl synchronize on lock and then try (keeping lock) to start server (NetServer.start)
NetServer.start opens a channel, binds to it, and since that moment it, I assume, may receive connections from remote clients
It seems, at this time a remote client opens connection to this server (already bound)
Then in channel's event loop (probably single threaded) is received initChannel which try to get ServerImpl.lock in ServerListenerImpl.transportCreated (coudln't because got by ServerImpl.start)
NetServer.start then schedules runnable in channel's event loop and blocks with channelzFuture.await()
Now, channelzFuture.await() waits for a runnable to be executed in channel's event loop (probably single threaded)
At this point channelzFuture.await keeps ServerImpl.lock lock, while the ServerListenerImpl.transportCreated occupies/blocks (this is what I suppose) the single threaded channel's event loop thus making impossible to process further
I'm attaching file with thread dumps of the whole JVM
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

6641
Deadlock in server transport with multiple ports · Issue #6641 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
ServerImpl.start() calls NettyServer.start() while holding ServerImpl.lock. NettyServer.start() awaits a submitted runnable in eventloop. However, this pending runnable may never be executed because the eventloop might be executing some other task, , like ServerListenerImpl.transportCreated(),  that is trying to acquire ServerImpl.lock causing a deadlock.
This is a deadlock for multiple-port server transport usecase with the same deadlock mechanism as #6601.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

8383
RPCs start to fail as commands are no longer written to the wire after receiving 503.  · Issue #8383 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
What version of gRPC-Java are you using?
I am using 1.23.0, however I can reproduce the issue on 1.39.1.
What is your environment?
What did you expect to see?
I would expect new RPCs to function normally and send messages to the wire.
What did you see instead?
We have a service which acts as a gRPC client and makes requests to several gRPC servers. Our client uses a single underlying channel and makes a combination of streaming and unary requests. The unary requests happen on a regular 5 second interval. It appears that once our client makes a certain amount of requests, the server gets unhappy and sends us an HTTP 503 / gRPC UNAVAILABLE (likely due to rate limiting). After we receive this 503 it appears that any following RPCs will fail since commands/messages are no longer being written to the wire.
These are the last messages that we see on the HTTP2 frame logger
When our code proceeds to make the next unary requests on the 5 second interval the HTTP2 frame logger is silent (with the exception of messages written on a totally different channel/target).
Originally we had no deadlines on these requests so these RPCs were hanging indefinitely (if I add deadlines the deadlines do work which is different from #8334). It appeared as if all of our threads executing gRPC requests were in a deadlock so I attached a debugger and took a thread dump. Every single one of the threads using the shared gRPC channel was parked in a WAITING state inside of waitAndDrain.
It took me a while to grok the code but I realized that these threads were essentially waiting for some Netty thread to parse the response from the server and enqueue some callback code for our blocking threads to execute. What was troubling to me is that the client was never even writing any requests onto the wire so surely  it would never receive any response and these threads would sit here indefinitely (unless a deadline was configured).
I continued tracing the code paths and debugging the code. I discovered that the new RPCs were getting legitimate stream implementations, ie. NettyClientStream and not NoopClientStream. I discovered that writes to a stream were enqueued onto a channel's WriteQueue. I verified that the WriteQueue was periodically flushing messages to the Netty channel.
For example, here is a create stream command that is being flushed. And it will not be loged by the HTTP frame logger.
In order to double check that the HTTP frame logger wasn't malfunctioning I decided to take a packet capture and noticed a void of packets (after the initial 503 failures) when there should have at least been packets sent every 5 seconds. Capture ends when I kill the JVM. Note: Packet capture was taken at a different time than above screenshot so port numbers aren't the same because its a different TCP connection.
I haven't gone through debugging the Netty pipeline yet but I figured I should post here before I go even further down the rabbit hole.
Steps to reproduce the bug
See above. Only able to reproduce with this particular target service which sends us 503s at certain request rates. Luckily it is extremely reproducible.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

8914
binder: Deadlock due to unexpected re-entrancy of transactions on process-local Binder · Issue #8914 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
BinderTransport locking was written under the assumption that calls to IBinder#transact() enqueued the Parcel for delivery to the peer and returned immediately. However, Android guarantees the unique object identity of IBinder instances within a process. And so when a client creates a Channel to a Server/Service within its own process, BinderClientTransport.outgoingBinder == BinderServerTransport.outgoingBinder. android.os.Binder#transact() on that object is implemented not as a system call to the binder driver but as a direct call to its own onTransact() method.
This is a problem because BinderTransport#handleTransaction() holds its 'this' lock while calling outgoingBinder.transact() in multiple places. If two peer instances of BinderClientTransport and BinderServerTransport are running handleTransaction() on different threads at the same time, they can each end up holding their own lock while waiting (forever) for the other's.
Steps to reproduce one instance of this bug
Use BinderChannelBuilder to create a Channel to an android.app.Service hosted by the same process
Have both the client and server repeatedly send messages to each other around the same time from different threads
What did you expect to see?
No deadlock
What did you see instead?
Example deadlock, via sendAcknowledgeBytes():
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

6396
ThreadlessExecutor thread dead lock · Issue #6396 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
What version of gRPC are you using?
version: 1.21.0
Possible Reason
invoke execute method, add runnable to ThreadlessExecutor
start execute LockSupport.park(this);
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

5015
Revisit LoadBalancer API's threading model · Issue #5015 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
The LoadBalancer main interface is not thread-safe, and is guaranteed to be called from the SynchronizationContext. This has relieved implementors from worrying about synchronization.
As for the auxiliary interfaces, SubchannelPicker is intentionally thread-safe because it on the critical path. Helper and Subchannel are not on the critical path, we made them thread-safe because they are implemented by GRPC and we thought making them thread-safe would probably provide more convenience to their callers.
However, client-side health checking (#4932) and our (Google-internal) request routing work revealed two use cases where a LoadBalancer may wrap or delegate to another, while adding additional logic. Helper and Subchannel may also be wrapped in the process.
For example, HealthCheckingLoadBalancerFactory wraps Helper.createSubchannel() to initialize health checking on the created Subchannel, and we find it much easier to implement if createSubchannel() were always called from the SynchronizationContext, which is not the case right now since createSubchannel() is thread-safe. In fact, probably all LoadBalancers always call createSubchannel() from the SynchronizationContext, otherwise it may race with handleSubchannelState() and it's non-trivial to handle, and will cancel out the benefits of the threading guarantee on the main LoadBalancer interface.
Because of the apparent issue with createSubchannel(), we are going to suggest always calling it from the SynchronizationContext, and will log a warning if it's not the case.
We'd like to discuss whether it makes sense to make Helper and Subchannel non-thread-safe, and require them to be called from SynchronizationContext.
My argument for non-thread-safety: we made Helper and Subchannel thread-safe based on the mindset that they is only one implementation which is from GRPC. In fact, a 3rd-party developer may want to wrap them and add their own logic, and it now becomes their burden to make their added logic thread-safe too.
Possible argument for thread-safety: Subchannel.requestConnection() may be called from the critical path. However, since it doesn't guarantee any action for the caller, the caller can easily enqueue it to the SynchronizationContext.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

5286
gRPC appengine executor hangs on async call · Issue #5286 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
What version of gRPC are you using?
What did you expect to see?
I am using gRPC client in the appengine standard java8 runtime. I want to supply appengine thread factory for the async calls. But when I specify that gRPC call hangs. I created an appengine servlet which does this and the servlet never returns any response. AppEngine logs does not show this request either.
When I specify, a normal executor the async call works. I can see the request in the appengine logs. It seems like gRPC is doing some operation that is not allowed on the appengine thread which causes it to crash.
But when I specify an AppEngine specific thread factory, the gRPC call hangs.
In #3382 @ejona86 Mentioned that appengine executor can be specified to do appengine specific calls. But that does not work. Are there tests to cover this use case ?
I started this thread https on discussion group, but after spending more time, I believe the issue is caused by this bug.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

5450
Data race in NameResolve.Listener.onError · Issue #5450 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
NameResolve.Listener.onError can be called concurrently in different threads, so the following code in onError() impl may have data race.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

6323
Request got cancelled by RST_STREAM - race condition · Issue #6323 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Revised version of #6166. We actually thought that the elapsed time was a factor but it seems it is a race condition and can be observed instantly.
We implemented a service that receives a request from a gRPC client and then waits for a condition to trigger. When this condition is finally met, the gRPC server sends a message to the client and completes the connection.
In our service it can happen that a trigger is fired by a few threads concurrently since we receive updates via several input sources. This is where we sometimes run into the described problem.
When creating the reproducer it could be observed that the request is always answered and the RST_STREAM comes in later. However, onComplete() is not called.
NB: In our full-blown service (Spring Boot based) we only saw the reply in the tcp dump but not in our application. But this is probably due to the nature of a race condition.
We also see this message in the logs:
This is probably where the RST_STREAM originates from that causes our service to not get the response or have onCompleted() not called.
I read that StreamObservers are not thread-safe but I don't know how StreamObserver and ServerCallImpl are connected.
I was wondering if ServerCallImpl could be made thread safe in this regard. Looking at the ServerCallImpl source code I can see that there are several fields representing the state but only one is volatile.
So after all it seems we are simply using gRPC wrong. Do you think there is a chance that gRPC can behave better in such cases because we wasted quite some time finding the root cause of this issue.
We now prevented this problem by guarding the triggering by an AtomicBoolean that only lets the first thread pass. Another approach was to create a synchronized method in our StreamObserver that calls onNext() and subsequently onCompleted().
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

6929
happens-before in StreamObserver · Issue #6929 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
It's not clear to me what concurrency guarantees are provided in StreamObserver. For example given below where the observer is converted to a future:
Given that onError/onNext/onCompleted may be called from different threads, is there a happens-before relationship (according to the Java Memory Model) between subsequent calls to the observer or not, assuming the observer is not shared between multiple calls? This is not documented anywhere. If there is a happens-before, then the volatile on value is redundant, if not, then it would be required.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

7044
io.grpc.StatusRuntimeException: INTERNAL: Protocol error Rst Stream · Issue #7044 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
What version of gRPC-Java are you using?
What is your environment?
Linux. Java gRPC client talking to a Go gRPC server.
We have a multithreaded client that makes different calls to the gRPC server. We reuse the ManagedChannel, but create a separate blocking stub for each thread. We see the error when the server takes a long time (a few minutes) to process and returns a large error message.
If I change the code to use a ManagedChannel per thread, the problem goes away. This makes it appear that there is a concurrency issue with the ManagedChannel.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

7581
Thread Leak · Issue #7581 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
What version of gRPC-Java are you using?
What is your environment?
OpenJRE 15 on Linux and Windows
What did you expect to see?
I'm trying to receive chat messages via a long running stream.
What did you see instead?
GRPC permanently spawns new threads while reading from the stream without closing them.
My function which i use to receive the messages looks like the following:
All threads have the same stack trace:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

7691
thread hangs and cannot exit · Issue #7691 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
What version of gRPC-Java are you using?
What is your environment?
What did you see instead?
Thread is hanging.
Steps to reproduce the bug
I've read some issues(#7362, #7579, #5286, #7635), but cannot figure out the reason. It's very difficult to reproduce it.
In my env, it seems because of unexpected channel.shutdown:
Thread1 and Thread2 hold the same channel. Thread1 shutdown channel unexpectedly. Is it possobile that Thread2 will hang?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

8190
grpc hang due to the ELG thread placement of NameResolver refresh method · Issue #8190 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
What version of gRPC are you using?
What operating system (Linux, Windows, …) and version?
Both Linux and Windows
What did you do?
Implement a customized NameResolver which extends NameResolver, let's call it "CustomizedNameResolver".  In the override refresh() method, it makes a grpc call to service discovery agent to retrieve a list of service instances and then resolve them.
What did you expect to see?
Expect the customized namer resolver works whenever being called and not hang in the existing grpc call.
What did you see instead?
grpc calls hang in the customized name resolver, particularly on the grpc calling inside overridden refresh() method.
We did a thread dump analysis, the problem is the grpc call inside overridden refresh() method is placed in gRPC ELG thread instead of worker thread, which in turns blocks all gRPC traffic causing grpc call hang indefinitely.
According to comment on refresh() method, the document does not clearly states that you must delegate a grpc call to a worker/background thread to not block other grpc calls.
First, is the placement of grpc call inside overridden refresh() method on the grpc ELG thread an expected behavior? Why cannot we delegate it to worker thread by default?
Second, some guides and explanations could be added to the document on NameResolver to further clarify.
Attach a thread dump on ELG for your reference. Thank you.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

8334
RPC seems to hang indefinitely · Issue #8334 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Hey! I'm hoping you might have some insights into what could be going on here, or any tips for debugging it better. We have a grpc client on version 1.36.2 and it seems like after some amount of time, threads that issued RPCs hang indefinitely. The calls are being issued with a blocking stub, and in this particular instance they are being wrapped in kotlin coroutines. Here's two traces:
Somewhat ironically, they both have the same elapsed time which seems to mean something happened to cause the whole client to stop processing RPCs. The above are colocated in the logs with GOAWAYs like such
Some added background notes as well:
A deadline of 60s is on all of these RPCs which doesn't help in this particular instance.
Datadog APM is attached so they are dynamically injecting some interceptors.
We have a jwt client interceptor which will make a blocking RPC call for authentication. I've always thought this wasn't the best thing to do, but I've tried to inject faults with it and can't reproduce this blocking behavior.
Any thoughts or debugging help would be much appreciated!
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

8174
grpc-default-executor threads stop processing tasks: how can we troubleshoot in such situation? · Issue #8174 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Do you know a good way to troubleshoot "grpc-default-executor" threads' status?
In apache/beam#14768 (comment), when I tried to upgrade Beam's vendored (shaded) gRPC dependency to 1.37.0 (or 1.36) from gRPC 1.26.0, I observed that some tests (GrpcLoggingServiceTest or BeamFnLoggingServiceTest randomly) do not finish. Borrowing Kenn's words, BeamFnLoggingServiceTest does the followings:
start a logging service
set up some stub clients, each with onError wired up to release a countdown latch
send error responses to all three of them (actually it sends the error in the same task it creates the stub)
each task waits on the latch
(GrpcLoggingServiceTest has similar structure)
Unfortunately it occurs only in Beam's CI Jenkins environment (which takes ~1 hour to finish). I cannot reproduce the problem locally.
From the observation of the trace log and the previous thread dump, it seems that grpc-default-executor threads stop processing tasks (the thread dump showed no "grpc-default-executor" threads in the JVM when the test was waiting for the them to count down a CountDownLatch) and one of the latches are not counted down. This results in the test threads waiting forever for the remaining latch. I cannot tell why the "grpc-default-executor" threads stop working (disappear?).
Do you know how to troubleshot such situation?
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

8536
BinderChannel flow control can get stuck under load · Issue #8536 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
What version of gRPC-Java are you using?
head
What is your environment?
Steps to reproduce the bug
We're launching a unary "GetTile" gRPC interaction between two Android processes. Response message can be ~100kb and clients request several tiles one after the other. Telemetry from the field shows that after a while some clients start experiencing DEADLINE_EXCEEDED errors even though other calls to the same server process over different Channels continue to succeed.
By lowering BinderTransport#TRANSACTION_BYTES_WINDOW and requesting tiles in a loop I can reproduce similar symptoms locally. Using the debugger I can see the server's BinderTransport#transmitWindowFull becomes stuck true even though all bytes have been acknowledged by the client. The server is generating response messages but isn't able to put them on the wire. I believe the problem is that BinderTransport#sendTransaction() updates transmitWindowFull based on an unsynchronized read of acknowledgedOutgoingBytes, which may not include concurrent updates by #handleAcknowledgedBytes() on another thread.
What did you expect to see?
Binder transactions should pause when flow control kicks in then resume when enough outstanding bytes are acknowledged.
What did you see instead?
Outstanding bytes are acknowledged but transmitWindowFull remains true in a way that's inconsistent with acknowledgedOutgoingBytes and numOutgoingBytes.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

8565
Data race in RetriableStream.onReady() · Issue #8565 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
As shown in the following stack trace, RetriableStream.onReady() is calling isReady() on transport thread, whereas isReady() is calling frame().isClosed(), but framer.closed is not thread-safe.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

8640
ManagedChannel never terminates with shutdown/shutdownNow · Issue #8640 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
What version of gRPC-Java are you using?
What is your environment?
As a Kubernetes pod running /linux
openjdk version "1.8.0_302"
OpenJDK Runtime Environment (build 1.8.0_302-b08)
OpenJDK 64-Bit Server VM (build 25.302-b08, mixed mode)
What did you expect to see?
Waits for the channel to become terminated, giving up if the timeout is reached.
What did you see instead?
ManagedChannel.isTerminated() always return true after 100 or more tries shutdown()/shutdownNow()
Steps to reproduce the bug
The issue gets reproduced intermediate and when observed the thread hangs. The channel stop is called using the following snippets.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

8642
CsdsService not properly synchronized with XdsClient · Issue #8642 · grpc/grpc-java · GitHub
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Consider this code from CsdsService:
The initial issue is that getSubscribedResourcesMetadata() and getCurrentVersion() have no synchronization:
That is bad. However, the xdsClient API itself is insufficient for CSDS because those two method calls need to be atomic; even if each of those methods were thread-safe the version needs to match the resources returned.
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
