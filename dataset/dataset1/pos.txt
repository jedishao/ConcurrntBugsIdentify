BufferingHttp2ConnectionEncoder does not shutdown properly on channelInactive.@nmittler.There is a nasty race condition during the handling of channelInactive in NettyClientHandler which goes a bit like this....This reproduces for NettyClientTransportTest.bufferedStreamsShouldBeClosedWhenTransportTerminates with 5.0beta5.Having streams being created as a side-effect of channel inactivation is undesirable. Potential fixes include.Reorder teardown in Http2ConnectionHandler.BaseDecoder.channelInactive so encoders are closed() before streams are closed.Make BufferedHttp2ConnectionEncoder check channel.isActive() when trying to create streams.
Simplify Netty pipeline using Buffering handler9.The current pipeline configurations using one of the buffering handlers (tls, plaintext, etc.) are rather complicated and it's not always clear which handlers will handle exceptions in various cases.We currently add the buffering handler and the HTTP/2 handler at startup. The buffering handler holds any writes until the startup handshake (e.g. SSL/TLS) completes, at which point it directs all buffered writes to the HTTP/2 handler. While those writes are occuring, the buffering handler stays in the pipeline (this is due to threading behavior of Netty WRT writes occuring outside of the event loop). If any problems occur while those writes are taking place, exception handling could occur in either the buffering handler or the HTTP/2 handler.  It would be desirable to guarantee that exception handling can occur in only a single place at any point in time.Proposed change:Part 1): Add a ChannelHandlerAdapter as the last handler in the pipeline. Netty has a race condition when writes occur from outside of the event loop.  The last ChannelHandlerContext is extracted in this thread and then the write is called. If however, the pipeline is changed between when the context is obtained and the write occurs ... badness ensues. As a workaround, there is some hacky code in the buffering handler to account for this race. A better solution to this problem would be to simply enforce the existence of a handler at the tail of the pipeline which never changes.  This will just be a pass-through, but must implement the write method (this is to avoid another Netty gotcha, where it will skip handlers if it has determined that they are uninterested in the event).Part 2): With the handler from Part 1 in place, the installation of the buffering and HTTP/2 handlers can be modified to a replace.  Initially, only the buffering handler is installed (not the HTTP/2 handler). When the startup handshake completes successfully, the buffering handler will replace itself with the HTTP/2 handler, and then empty it's queued writes to the HTTP/2 handler.In this way we guarantee that only one of these handlers exists in the pipeline at a time.  Failures due to the initial handshake will be handled by the buffering handler. Failures due to writes will always be handled by the HTTP/2 handler.@ejona86 FYI.
Possible race condition ServerImpl between start() and shutdown()I believe it may be possible if start and stop are called concurrently that the shared executor may not get released.  I'm not sure if this is an actual problem, but it does go against the @ ThreadSafe annotation.
Rare race condition in Client6.While more prominent when using compression, this race occurs without it as well.  The typical race looks something like:Client starts and RPC.The transport to the server is not yet available, so a DelayedClientTransport is used.The server handles the RPC and sends back headers and a compressed message.The client sees there are headers, and begins executing the queued stream callbacks, on the channel executor threads instead of the transport thread.The client sees the Data frame, and tries to decompress it on the network thread.  *This fails since the headers from 4 have not yet been processed.The stream has already failed, but the queued callback for onHeaders() is finally executed on the app thread.This is the root cause of #2157.  As mentioned, this isn't just for compression.  ClientInterceptors will see headers after data has been received.   The solution (temporary?) seems to be to move OkHttp to used AbstractClientStream2 in #2821, and then move decompression from ClientCallImpl to the stream.  That will fix the decompression, but not interceptors.
First received frame was not SETTINGS. Hex dump for first 5 bytes2.Please answer these questions before submitting your issue.What version of gRPC are you using?What JVM are you using (java -version)?java version "1.8.0_131".Java(TM) SE Runtime Environment (build 1.8.0_131-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)What did you do?Inside a test I ran a grpc server and 5 clients that accessed the server concurrently (all from the same IP; everyone with its own channel). This is only happening sometimes, looks to me like some kind of race condition within grpc.What did you expect to see?Successful call.What did you see instead?
Remove blocking parts from NettyClientTransport.NettyClientTransport#newStream is currently a blocking operation. It blocks until the HEADERS frame has been written on the wire. This is behavior is not what people who use our asynchronous API would come to expect.The blocking also is the cause for severe performance issues in the QPS Client as it results in more or less in as many threads being created as there are concurrent calls going on (We have seen ~850 Threads for 1000 concurrent calls, resulting in OOM).The blocking may also lead to deadlocking the EventLoop in cases where a DirectExecutor is used. One scenario where a deadlock might happen is when the EventLoop is not able to completely flush the HEADERS frame on the wire because then Netty would internally create a task to flush the remaining bytes and put this task in its task queue. This task can never be completed though as the EventLoop Thread is blocked by our very own newStream method waiting for the task to be completed ...This issue depends on #116 and #118 to be resolved first.
In-process transport deadlock during shutdown.Simultaneously shutting down both server and client sharing the same in-process transport can lead to a deadlock. During server shutdown, the transport lock is held while calling transportShutdown on the channel listener, which attempts to lock the channel. At the same time, channel.shutdownNow() holds the channel lock while also trying to lock the transport which leads to a deadlock:Found one Java-level deadlock:Found 1 deadlock.
Connection window should auto-refill7.Consider an application with 2 streams, A and B. A receives a stream of messages and the application pops off one message at a time and makes a request on stream B. However, if receiving of data on A has caused the connection window to collapse, B will not be able to receive any data and the application will deadlock. The only way (currently) to get around this is to use multiple connections for the streams, which would needlessly complicate the application code.The C implementation already solves the problem by auto-refilling the connection window, so Java and the other languages should just follow suit here.Created PR netty/netty#4423 in Netty to support configuring the local flow controller to auto-refill.  Until that's in, we should probably just set the connection window to MAX_INT for now.@louiscryan @ejona86 @ctiller @a11r.
Potential risk of deadlock from calling listeners under locks0.Methods of ClientTransport.Listener and ServerTransportListener are usually called under a lock. The biggest reason for locking is to guarantee the ordering of multiple methods on the same listener.However, these listeners usually call into channel layer code, and may in turn acquire locks from there, which forms a transport lock -> channel lock lock order. On the other hand, when channel layer calls into transport layer, it's possible to form a channel lock -> transport lock lock order, which makes deadlock possible.It's unlikely an issue today because there is an implicit rule today that channel layer will not hold any lock while calling into transport. However, as the code base grows, it will become harder to keep track of such requirement.A possible solution is to always schedule listener calls on a serialized executor, with the cost of a thread, so that listener order can be guaranteed without the need of locking. There may be better options.
DelayedClientTransport and InProcessTransport means deadlock1.There is a chance of deadlock when DelayedClientTransport is linked with an InProcessTransport. See /pull/1503.
Simplify implementation of back-pressure in StreamObserver-based stub4.Pending API changes can allow reactive/async pattern for interacting with flow control and applying back pressure: https.In many cases, automatic back-pressure in generated stubs could be very useful -- e.g. having calls to StreamObserve#onNext(T) block instead of queueing.It's been pointed out that this could cause deadlock for bidi-streaming operations, so perhaps we can just not expose this functionality for bidi-streaming calls?It may also be worth pointing out that most other runtimes (wrapped languages and Go) already expose streams via blocking operations and already require that apps be aware of and work-around possible deadlock issues resulting therefrom. So maybe providing similar mechanisms in Java is fine, with said caveats.Another possible alternative could possibly be done in an extension/add-on instead of in GRPC. For example, wrapping streaming requests and responses with RxJava Observables may further simplify the async case enough to make the synchronous (and possibly-deadlock-prone) case unnecessary.
Deadlock found in TransportSet5.When running benchmarks where the client started up faster than the server, The first few calls failed as unavailable.  Our internal deadlock detector seems to think there is a deadlock around here:Deadlock(s) found:
Deadlock with TransportSet4.Hello,I was testing Grpc with RoundRobinLB and a custom NameResolver when this deadlock happened:Found one Java-level deadlock:I don't know if it may relate to my own code or if the issue is on grpc side.
New deadlock in TransportSet and GrpcTimer8.Hi,I have encountered a new deadlock in TransportSet. I'm running under v1.0 with #2258 cherry-picked.
Deadlock in grpc due to recursive grpc call1.Please answer these questions before submitting your issue.What version of gRPC are you using?1.0.1What JVM are you using (java -version)?openjdk version "1.8.0_102".OpenJDK Runtime Environment (build 1.8.0_102)OpenJDK 64-Bit Server VM (build 25.102-b01, mixed mode)What did you do?If possible, provide a recipe for reproducing the error.Turned on FINE logging.Used the Logging client in google-cloud-java.What did you expect to see?Not what I saw below...What did you see instead?Deadlock.Found one Java-level deadlock:Java stack information for the threads listed above:Background: I have been trying various strategies to resolve googleapis/google-cloud-java#1386 , where using the Logging service at level FINE results in grpc logging to the Logging service in a recursive way. I tried using a ThreadLocal to prevent this, but this doesn't work with grpc because the actual call is executed on a worker thread. Essentially I think I need some way to bail out of the LoggingHandler.publish call if I can detect that this is in the scope of a grpc worker thread sending a request to the Logging service.
Potential deadlock due to calling callbacks while holding a lock8.InProcessClientStream and InProcessServerStream are synchronized on their own. InProcessClientStream.serverStreamListener is called under synchronized (InProcessClientStream.this), and vice versa.If the application tries to call methods on ClientCall or ServerCall from within the callbacks (assuming that it has already taken care of the thread-safety of the method calls on "Call" objects), a deadlock is possible when direct executor is used. For example:Thread1.Thread2.As locks are acquired in reverse orders from two threads, a deadlock is possible.The fundamental issue is that we should not call into application code while holding a lock, because we don't know what application code can do thus we can't control the order of subsequent locking.OkHttp has the same issue, because OkHttpClientStream.transportDataReceived(), which will call into application code, is called under lock.We could use ChannelExecutor (maybe renamed) to prevent calling into callbacks while holding a lock.
Buffer Messages until TLS Handshake and HTTP2 Negotiation complete.When grpc uses Netty as the client transport all RPC calls (aka HTTP2 Streams) block until the TLS Handshake and the HTTP2 negotiation is complete.This blocking implementation (in grpc) is currently required as Netty's SslHandler doesn't buffer messages until the Handshake is complete ("You must make sure not to write a message while the handshake is in progress unless you are renegotiating."), and there is nothing to stop the user from starting to make RPC calls immediately.This behavior comes with two problems:With RPC calls blocking until the TLS Handshake is complete, every call launched before the TLS Handshake and HTTP2 Negotiation are done will block its thread from which one would expect async behavior though.In cases when a DirectExecutor is being used it might lead to the EventLoop blocking forever (deadlock effectively). There is several scenarios how a deadlock could happen. One such scenario is when you are writing a server in Netty and within that server you want to connect to a grpc service to fetch some data. If you now use a DirectExecutor and reuse the EventLoop of the server with the grpc client, the TLS handshake would block the server's EventLoop, which is also the very EventLoop responsible for completing the TLS HandShake. That way neither the server nor the client would ever make progress again.@nmittler , @ejona86 and I talked about this problem earlier today and we agreed to get rid of the blocking behavior by adding an additional ChannelHandler to the end of the pipeline (tail) that will buffer any data until TLS & HTTP2 are working. After that it will send the buffered messages through the pipeline and remove itself from the pipeline.@nmittler @ejona86 @louiscryan.
OkHttpClientTransport.onGoAway() races with startPendingStreams()onGoAway has two phases: do things necessary under lock and final cleanup. In the first phase it collects the streams to terminate in the second and sets goAway.startPendingStreams() does not observe goAway and also creates new streams that should be failed due to the goAway. From an initial look, it seems it would be best to remove failPendingStreams() and simply integrate its two phases into onGoAway()'s two phases; that is, when holding the lock in onGoAway, replace pendingStreams with an empty list, and then when not holding the lock call transportReportStatus.
OkHttp's cancellation is not properly synchronized.OkHttpClientStream.sendCancel() calls finishStream() from an application thread. But finishStream() calls transportReportStatus() without any lock held. That is not synchronized correctly, as transportReportStatus() may only be called from the transport thread (i.e., while lock is held).It seems that all usages of streams is done while lock is held except for within finishStream() and data(). data() can actually race with finishStream() and end up sending DATA frames after the RST_STREAM. It seems it would be best to just have stream protected by lock, because it having its own synchronization isn't providing much benefit and isn't leading to correct code.
Propagate RuntimeException's back to caller9.When using ManagedChannelImpl, if a method of ClientCall.Listener (possibly also of ClientCall itself) throws a RuntimeException, the exception propagates up into SerializingExecutor. The executor will log it under SEVERE but cannot take corrective action. The thread initiating ClientCalls.blockingUnaryCall or its cousins will block forever waiting for a response.Ideally, the exception should be thrown back in the thread that called blockingUnaryCall.
Errors with concurrent server push0.Hi, I'm getting errors when making concurrent server pushes to the same client connection.You can see simplified repro here https:Just clone and run mvn test to see repro.It is supposed to send 100 messages, but generally the client does not receive 100, and there are errors logged in the process, e.g.
Race on NettyClientTransport.start1.Bootstrap.connect seems to add a listener to a ChannelFuture that is concurrently modified by the NioEventLoop and the client provided executor in ConcurrencyTest.Conflicting accesses:
Race for Netty between cancel and stream creation ·.AbstractClientStream.cancel won't cancel the stream on the wire if it appears the stream has not yet been allocated, as is described by the comment:// Only send a cancellation to remote side if we have actually been allocated.// a stream id and we are not already closed. i.e. the server side is aware of the stream.However, what happens if this is the case, is that the transport is not notified of the stream destruction, and the stream will still eventually be created by the transport and not be cancelled. This issue does not seem a problem with the OkHttp transport, since it allocates the stream id before returning any newly created stream. However, Netty delays id allocation until just before the stream headers are sent, which 1) is always done asynchronously and 2) may be strongly delayed due to MAX_CONCURRENT_STREAMS.It appears that the optimization in AbstractClientStream should be removed outright and sendCancel's doc be updated to specify the expectation to handle such cases (as opposed to directly cause RST_STREAM). Both OkHttp and Netty seem to be handling such cases already. More importantly, the optimization seems highly prone for races given that id allocation is occurring in the transport thread whereas AbstractClientStream.cancel is happening on some application thread; using the normal synchronization between application and transport threads seems more than efficient enough and simpler.
Race in Server handler initialization.When initializing an incoming client connection, we call startAsync() on the transport, which registers the handler on a separate thread. This is obviously a race, and it would have probably been fixed if I had finished Service removal in #35.Symptom:The quickest fix would be to call awaitRunning() from initChannel(). That reduces the rate new connections can connect, but is probably the most expedient solution, until #35 is finished.@nmittler, thoughts?
OkHttp: race between sendCancel and sendFrame.If sendCancel is called (by timeout for example) before the stream is started, a following sendFrame will cause a NPE:
ClientCallImpl operations race with Context cancellation.5With be60086, we don't create the stream when the Context is cancelled, so the following request(), sendMessage(), halfClose() will encounter an IllegalStateException like:@louiscryan, FYI, I'll send you a PR to fix it soon.
Deadline can fire before stream started4.In ClientCallImpl the deadline is scheduled before stream.start(). However, if the deadline has already elapsed the runnable will be executed immediately and race with the start. I've only looked into how OkHttp may be impacted.I believe a NullPointerException would be thrown when trying to notify the stream listener due to the cancellation. However, due to #1237 the exception won't be logged. Thus, this will result in a hung stream that never completes with no logging as to what went wrong.This was discovered due to timeout_on_sleeping_server on android being flaky, because it uses a very small timeout. The test would fail at awaitCompletion.@carl-mastrangelo, FYI.
okhttp: race between receiving data and closing deframer6.As reported on SO:When receiving a DATA frame, it seems there is a race between getStream() and calling transportDataReceived(). Although I wouldn't expect to trigger that race often.
Executor usage in `ClientCallImpl` races with channel shutdown and termination.8ManagedChannelImpl clear scheduledExecutor in shutdown(), and releases (which potentially closes) executor in maybeTerminateChannel().Neither newCall() nor ClientCallImpl checks the shutdown state of the channel. ClientCallImpl relies on FailingClientTransport for the expected behavior. However, ClientCallImpl uses the passed in executors anyway, for scheduling the deadline timer and invoking the call listener.If ClientCallImpl tries to schedule a deadline timer after the channel is shut down, it will get a NPE. If it runs the call listener after the shared executor has been closed, which is 1 second (SharedResourceHolder.DESTROY_DELAY_SECONDS) after all references are gone, e.g., the application calls Call.start() that late, it will get a RejectedExecutionException. Our current tests are not testing for the two cases.This doesn't seem to be a serious issue. It only affect people who try to use Calls after the channel has been shutdown. I am yet to figure out a solution.Anyway, it seems executor should be cleared after being returned to the shared pool, like scheduledExecutor.
Race on NettyServer shutdown.1Even with 2292cbf it seems there is still a race with shutdown.  Initial guess is that the shutdown and close are happening in the Boss Event loop and the normal event loop.  Conflicting reads and writes:
Census Race7.Read of size 4 at 0x7f00465dfbcc by thread T123:cc: @adriancole @zhangkun83.
Threading of StatsTraceContext5.StatsTraceContext assumes non-thread-safety, which is fine as long as the RPC is closed by the application through the ClientCall/ServerCall interface, which are also not thread-safe.However, if the RPC is not closed by the application, but either cancelled by the other side, or closed by transport due to errors, which will call callEnded() from the transport thread which is not synchronized with  the application thread. As the application may not be notified about the closure in time, it may still trying to send messages, resulting in wireBytesSent() etc being called after callEnded(), which would trigger a check failure. There is also a data race on the counter fields as wireBytesSent() etc write them and callEnded() reads them from different threads without synchronization.We will remove the preconditions checks from writeBytesSent() etc. For the data race, some kind of synchronization would be required, maybe atomics? @ejona86.
Race between pick and transport shutdown6.Right now they are done in two steps:A transport that is in READY state is selected.newStream() is called on the selected transport.If transport is shutdown (by LoadBalancer or channel idle mode) between the two steps, Step 2 will fail spuriously. Currently we work around this by adding a delay between stopping selecting a subchannel (which owns the transport) and shutting it down. As long as the delay is longer than the time between Step 1 and Step 2, the race won't happen.This is not ideal because it relies on timing to work correctly, and will still fail in extreme cases where the time between the two steps are longer than the pre-set delay.It would be a better solution to differentiate the racy shutdown and the intended shutdown (Channel is shutdown for good). In response to racy shutdown, transport selection will be retried. The clientTransportProvider in ManagedChannelImpl is in the best position to do this, because it knows whether the Channel has shutdown. clientTransportProvider would have to call newStream() and start the stream, and return the started stream to ClientCallImpl instead of a transport.
Race in io.grpc.internal.MessageFramer.commitToSink4.messagesBuffered seems to be updated on error and from the application thread.
Netty Flow Control tests have data races5.The window size read from the testing thread should be executed on the event loop.  Since this test is flaky, and fixing this race would make the test more flaky (but less racy), the test is being marked @Ignored.This issue tracks reenabling the flow control tests and fixing the data race.
Deadlock on start gRPC server0.What version of gRPC-Java are you using?What is your environment?openjdk version "1.8.0_171".OpenJDK Runtime Environment (IcedTea 3.8.0) (Alpine 8.171.11-r0)OpenJDK 64-Bit Server VM (build 25.171-b11, mixed mode)What did you expect to see?Proper start of gRPC Server.What did you see instead?Start sometimes hangs with deadlock.Steps to reproduce the bug.I suppose it's a race condition related to synchronization in gRPC (ServerImpl), await in NetServer.start and vertx/nettty event loops (probably single threaded). Probably it could happened at any time if someone start gRPC server and concurrently open new client connection to that server.In my case I stopped and started the gGPR server but I'm not sure if it is somehow related.Analysis.What I see in the thread dump is the following 2 threads that stays in that state, seems, forever:From what I see in these thread dumps and the code I think that this could be the problem (deadlock):Vertx grpc starts server (ServerImpl.start) in vertx blocking thread.ServerImpl synchronize on lock and then try (keeping lock) to start server (NetServer.start)NetServer.start opens a channel, binds to it, and since that moment it, I assume, may receive connections from remote clients.It seems, at this time a remote client opens connection to this server (already bound)Then in channel's event loop (probably single threaded) is received initChannel which try to get ServerImpl.lock in ServerListenerImpl.transportCreated (coudln't because got by ServerImpl.start)NetServer.start then schedules runnable in channel's event loop and blocks with channelzFuture.await()Now, channelzFuture.await() waits for a runnable to be executed in channel's event loop (probably single threaded)At this point channelzFuture.await keeps ServerImpl.lock lock, while the ServerListenerImpl.transportCreated occupies/blocks (this is what I suppose) the single threaded channel's event loop thus making impossible to process further.I'm attaching file with thread dumps of the whole JVM.
Deadlock in server transport with multiple ports4.ServerImpl.start() calls NettyServer.start() while holding ServerImpl.lock. NettyServer.start() awaits a submitted runnable in eventloop. However, this pending runnable may never be executed because the eventloop might be executing some other task, , like ServerListenerImpl.transportCreated(),  that is trying to acquire ServerImpl.lock causing a deadlock.This is a deadlock for multiple-port server transport usecase with the same deadlock mechanism as #6601.
RPCs start to fail as commands are no longer written to the wire after receiving 503. 8.What version of gRPC-Java are you using?I am using 1.23.0, however I can reproduce the issue on 1.39.1.What is your environment?What did you expect to see?I would expect new RPCs to function normally and send messages to the wire.What did you see instead?We have a service which acts as a gRPC client and makes requests to several gRPC servers. Our client uses a single underlying channel and makes a combination of streaming and unary requests. The unary requests happen on a regular 5 second interval. It appears that once our client makes a certain amount of requests, the server gets unhappy and sends us an HTTP 503 / gRPC UNAVAILABLE (likely due to rate limiting). After we receive this 503 it appears that any following RPCs will fail since commands/messages are no longer being written to the wire.These are the last messages that we see on the HTTP2 frame logger.When our code proceeds to make the next unary requests on the 5 second interval the HTTP2 frame logger is silent (with the exception of messages written on a totally different channel/target).Originally we had no deadlines on these requests so these RPCs were hanging indefinitely (if I add deadlines the deadlines do work which is different from #8334). It appeared as if all of our threads executing gRPC requests were in a deadlock so I attached a debugger and took a thread dump. Every single one of the threads using the shared gRPC channel was parked in a WAITING state inside of waitAndDrain.It took me a while to grok the code but I realized that these threads were essentially waiting for some Netty thread to parse the response from the server and enqueue some callback code for our blocking threads to execute. What was troubling to me is that the client was never even writing any requests onto the wire so surely  it would never receive any response and these threads would sit here indefinitely (unless a deadline was configured).I continued tracing the code paths and debugging the code. I discovered that the new RPCs were getting legitimate stream implementations, ie. NettyClientStream and not NoopClientStream. I discovered that writes to a stream were enqueued onto a channel's WriteQueue. I verified that the WriteQueue was periodically flushing messages to the Netty channel.For example, here is a create stream command that is being flushed. And it will not be loged by the HTTP frame logger.In order to double check that the HTTP frame logger wasn't malfunctioning I decided to take a packet capture and noticed a void of packets (after the initial 503 failures) when there should have at least been packets sent every 5 seconds. Capture ends when I kill the JVM. Note: Packet capture was taken at a different time than above screenshot so port numbers aren't the same because its a different TCP connection.I haven't gone through debugging the Netty pipeline yet but I figured I should post here before I go even further down the rabbit hole.Steps to reproduce the bug.See above. Only able to reproduce with this particular target service which sends us 503s at certain request rates. Luckily it is extremely reproducible.
binder: Deadlock due to unexpected re-entrancy of transactions on process-local Binder1.BinderTransport locking was written under the assumption that calls to IBinder#transact() enqueued the Parcel for delivery to the peer and returned immediately. However, Android guarantees the unique object identity of IBinder instances within a process. And so when a client creates a Channel to a Server/Service within its own process, BinderClientTransport.outgoingBinder == BinderServerTransport.outgoingBinder. android.os.Binder#transact() on that object is implemented not as a system call to the binder driver but as a direct call to its own onTransact() method.This is a problem because BinderTransport#handleTransaction() holds its 'this' lock while calling outgoingBinder.transact() in multiple places. If two peer instances of BinderClientTransport and BinderServerTransport are running handleTransaction() on different threads at the same time, they can each end up holding their own lock while waiting (forever) for the other's.Steps to reproduce one instance of this bug.Use BinderChannelBuilder to create a Channel to an android.app.Service hosted by the same process.Have both the client and server repeatedly send messages to each other around the same time from different threads.What did you expect to see?No deadlock.What did you see instead?Example deadlock, via sendAcknowledgeBytes():
ThreadlessExecutor thread dead lock9.What version of gRPC are you using?version: 1.21.0Possible Reason.invoke execute method, add runnable to ThreadlessExecutor.start execute LockSupport.park(this);
Revisit LoadBalancer API's threading model1.The LoadBalancer main interface is not thread-safe, and is guaranteed to be called from the SynchronizationContext. This has relieved implementors from worrying about synchronization.As for the auxiliary interfaces, SubchannelPicker is intentionally thread-safe because it on the critical path. Helper and Subchannel are not on the critical path, we made them thread-safe because they are implemented by GRPC and we thought making them thread-safe would probably provide more convenience to their callers.However, client-side health checking (#4932) and our (Google-internal) request routing work revealed two use cases where a LoadBalancer may wrap or delegate to another, while adding additional logic. Helper and Subchannel may also be wrapped in the process.For example, HealthCheckingLoadBalancerFactory wraps Helper.createSubchannel() to initialize health checking on the created Subchannel, and we find it much easier to implement if createSubchannel() were always called from the SynchronizationContext, which is not the case right now since createSubchannel() is thread-safe. In fact, probably all LoadBalancers always call createSubchannel() from the SynchronizationContext, otherwise it may race with handleSubchannelState() and it's non-trivial to handle, and will cancel out the benefits of the threading guarantee on the main LoadBalancer interface.Because of the apparent issue with createSubchannel(), we are going to suggest always calling it from the SynchronizationContext, and will log a warning if it's not the case.We'd like to discuss whether it makes sense to make Helper and Subchannel non-thread-safe, and require them to be called from SynchronizationContext.My argument for non-thread-safety: we made Helper and Subchannel thread-safe based on the mindset that they is only one implementation which is from GRPC. In fact, a 3rd-party developer may want to wrap them and add their own logic, and it now becomes their burden to make their added logic thread-safe too.Possible argument for thread-safety: Subchannel.requestConnection() may be called from the critical path. However, since it doesn't guarantee any action for the caller, the caller can easily enqueue it to the SynchronizationContext.
gRPC appengine executor hangs on async call8.What version of gRPC are you using?What did you expect to see?I am using gRPC client in the appengine standard java8 runtime. I want to supply appengine thread factory for the async calls. But when I specify that gRPC call hangs. I created an appengine servlet which does this and the servlet never returns any response. AppEngine logs does not show this request either.When I specify, a normal executor the async call works. I can see the request in the appengine logs. It seems like gRPC is doing some operation that is not allowed on the appengine thread which causes it to crash.But when I specify an AppEngine specific thread factory, the gRPC call hangs.In #3382 @ejona86 Mentioned that appengine executor can be specified to do appengine specific calls. But that does not work. Are there tests to cover this use case ?I started this thread https on discussion group, but after spending more time, I believe the issue is caused by this bug.
Data race in NameResolve.Listener.onError5.NameResolve.Listener.onError can be called concurrently in different threads, so the following code in onError() impl may have data race.
Request got cancelled by RST_STREAM - race condition2.Revised version of #6166. We actually thought that the elapsed time was a factor but it seems it is a race condition and can be observed instantly.We implemented a service that receives a request from a gRPC client and then waits for a condition to trigger. When this condition is finally met, the gRPC server sends a message to the client and completes the connection.In our service it can happen that a trigger is fired by a few threads concurrently since we receive updates via several input sources. This is where we sometimes run into the described problem.When creating the reproducer it could be observed that the request is always answered and the RST_STREAM comes in later. However, onComplete() is not called.NB: In our full-blown service (Spring Boot based) we only saw the reply in the tcp dump but not in our application. But this is probably due to the nature of a race condition.We also see this message in the logs:This is probably where the RST_STREAM originates from that causes our service to not get the response or have onCompleted() not called.I read that StreamObservers are not thread-safe but I don't know how StreamObserver and ServerCallImpl are connected.I was wondering if ServerCallImpl could be made thread safe in this regard. Looking at the ServerCallImpl source code I can see that there are several fields representing the state but only one is volatile.So after all it seems we are simply using gRPC wrong. Do you think there is a chance that gRPC can behave better in such cases because we wasted quite some time finding the root cause of this issue.We now prevented this problem by guarding the triggering by an AtomicBoolean that only lets the first thread pass. Another approach was to create a synchronized method in our StreamObserver that calls onNext() and subsequently onCompleted().
happens-before in StreamObserver2.It's not clear to me what concurrency guarantees are provided in StreamObserver. For example given below where the observer is converted to a future:Given that onError/onNext/onCompleted may be called from different threads, is there a happens-before relationship (according to the Java Memory Model) between subsequent calls to the observer or not, assuming the observer is not shared between multiple calls? This is not documented anywhere. If there is a happens-before, then the volatile on value is redundant, if not, then it would be required.
io.grpc.StatusRuntimeException: INTERNAL: Protocol error Rst Stream4.What version of gRPC-Java are you using?What is your environment?Linux. Java gRPC client talking to a Go gRPC server.We have a multithreaded client that makes different calls to the gRPC server. We reuse the ManagedChannel, but create a separate blocking stub for each thread. We see the error when the server takes a long time (a few minutes) to process and returns a large error message.If I change the code to use a ManagedChannel per thread, the problem goes away. This makes it appear that there is a concurrency issue with the ManagedChannel.
Thread Leak8.What version of gRPC-Java are you using?What is your environment?OpenJRE 15 on Linux and Windows.What did you expect to see?I'm trying to receive chat messages via a long running stream.What did you see instead?GRPC permanently spawns new threads while reading from the stream without closing them.My function which i use to receive the messages looks like the following:All threads have the same stack trace:
thread hangs and cannot exit9.What version of gRPC-Java are you using?What is your environment?What did you see instead?Thread is hanging.Steps to reproduce the bug.I've read some issues(#7362, #7579, #5286, #7635), but cannot figure out the reason. It's very difficult to reproduce it.In my env, it seems because of unexpected channel.shutdown:Thread1 and Thread2 hold the same channel. Thread1 shutdown channel unexpectedly. Is it possobile that Thread2 will hang?
grpc hang due to the ELG thread placement of NameResolver refresh method9.What version of gRPC are you using?What operating system (Linux, Windows, …) and version?Both Linux and Windows.What did you do?Implement a customized NameResolver which extends NameResolver, let's call it "CustomizedNameResolver".  In the override refresh() method, it makes a grpc call to service discovery agent to retrieve a list of service instances and then resolve them.What did you expect to see?Expect the customized namer resolver works whenever being called and not hang in the existing grpc call.What did you see instead?grpc calls hang in the customized name resolver, particularly on the grpc calling inside overridden refresh() method.We did a thread dump analysis, the problem is the grpc call inside overridden refresh() method is placed in gRPC ELG thread instead of worker thread, which in turns blocks all gRPC traffic causing grpc call hang indefinitely.According to comment on refresh() method, the document does not clearly states that you must delegate a grpc call to a worker/background thread to not block other grpc calls.First, is the placement of grpc call inside overridden refresh() method on the grpc ELG thread an expected behavior? Why cannot we delegate it to worker thread by default?Second, some guides and explanations could be added to the document on NameResolver to further clarify.Attach a thread dump on ELG for your reference. Thank you.
RPC seems to hang indefinitely3.Hey! I'm hoping you might have some insights into what could be going on here, or any tips for debugging it better. We have a grpc client on version 1.36.2 and it seems like after some amount of time, threads that issued RPCs hang indefinitely. The calls are being issued with a blocking stub, and in this particular instance they are being wrapped in kotlin coroutines. Here's two traces:Somewhat ironically, they both have the same elapsed time which seems to mean something happened to cause the whole client to stop processing RPCs. The above are colocated in the logs with GOAWAYs like such.Some added background notes as well:A deadline of 60s is on all of these RPCs which doesn't help in this particular instance.Datadog APM is attached so they are dynamically injecting some interceptors.We have a jwt client interceptor which will make a blocking RPC call for authentication. I've always thought this wasn't the best thing to do, but I've tried to inject faults with it and can't reproduce this blocking behavior.Any thoughts or debugging help would be much appreciated!
grpc-default-executor threads stop processing tasks: how can we troubleshoot in such situation?7Do you know a good way to troubleshoot "grpc-default-executor" threads' status?In apache/beam#14768 (comment), when I tried to upgrade Beam's vendored (shaded) gRPC dependency to 1.37.0 (or 1.36) from gRPC 1.26.0, I observed that some tests (GrpcLoggingServiceTest or BeamFnLoggingServiceTest randomly) do not finish. Borrowing Kenn's words, BeamFnLoggingServiceTest does the followings:start a logging service.set up some stub clients, each with onError wired up to release a countdown latch.send error responses to all three of them (actually it sends the error in the same task it creates the stub)each task waits on the latch.(GrpcLoggingServiceTest has similar structure)Unfortunately it occurs only in Beam's CI Jenkins environment (which takes ~1 hour to finish). I cannot reproduce the problem locally.From the observation of the trace log and the previous thread dump, it seems that grpc-default-executor threads stop processing tasks (the thread dump showed no "grpc-default-executor" threads in the JVM when the test was waiting for the them to count down a CountDownLatch) and one of the latches are not counted down. This results in the test threads waiting forever for the remaining latch. I cannot tell why the "grpc-default-executor" threads stop working (disappear?).Do you know how to troubleshot such situation?
BinderChannel flow control can get stuck under load3.What version of gRPC-Java are you using?head.What is your environment?Steps to reproduce the bug.We're launching a unary "GetTile" gRPC interaction between two Android processes. Response message can be ~100kb and clients request several tiles one after the other. Telemetry from the field shows that after a while some clients start experiencing DEADLINE_EXCEEDED errors even though other calls to the same server process over different Channels continue to succeed.By lowering BinderTransport#TRANSACTION_BYTES_WINDOW and requesting tiles in a loop I can reproduce similar symptoms locally. Using the debugger I can see the server's BinderTransport#transmitWindowFull becomes stuck true even though all bytes have been acknowledged by the client. The server is generating response messages but isn't able to put them on the wire. I believe the problem is that BinderTransport#sendTransaction() updates transmitWindowFull based on an unsynchronized read of acknowledgedOutgoingBytes, which may not include concurrent updates by #handleAcknowledgedBytes() on another thread.What did you expect to see?Binder transactions should pause when flow control kicks in then resume when enough outstanding bytes are acknowledged.What did you see instead?Outstanding bytes are acknowledged but transmitWindowFull remains true in a way that's inconsistent with acknowledgedOutgoingBytes and numOutgoingBytes.
Data race in RetriableStream.onReady()6As shown in the following stack trace, RetriableStream.onReady() is calling isReady() on transport thread, whereas isReady() is calling frame().isClosed(), but framer.closed is not thread-safe.
ManagedChannel never terminates with shutdown/shutdownNow4.What version of gRPC-Java are you using?What is your environment?As a Kubernetes pod running /linux.openjdk version "1.8.0_302".OpenJDK Runtime Environment (build 1.8.0_302-b08)OpenJDK 64-Bit Server VM (build 25.302-b08, mixed mode)What did you expect to see?Waits for the channel to become terminated, giving up if the timeout is reached.What did you see instead?ManagedChannel.isTerminated() always return true after 100 or more tries shutdown()/shutdownNow()Steps to reproduce the bug.The issue gets reproduced intermediate and when observed the thread hangs. The channel stop is called using the following snippets.
CsdsService not properly synchronized with XdsClient4.Consider this code from CsdsService:The initial issue is that getSubscribedResourcesMetadata() and getCurrentVersion() have no synchronization:That is bad. However, the xdsClient API itself is insufficient for CSDS because those two method calls need to be atomic; even if each of those methods were thread-safe the version needs to match the resources returned.
DefaultEventBus not thread safe.I wrote a prototype for our project, I chose vert.x as a middleware which involves consumes messages from kafka and redirecting it to mongoDB, since consumer api in kafka is a blocking api so I create a work verticle, after consuming about 50k messages, I found my cpu utilization was nearly 100% and consuming verticle stuck , I then found it was cause by a infinitive loop in HashMap.put() which was called by getHandlerCloseHook(context).entries.add(new HandlerEntry(address, handler)) in DefaultEventBus.registerHandler().I think it was caused by concurrently calling the put or remove on the HashMap by a worker verticle, thus causing the HashMap to be broken.After I change the.It seems everything was ok.I wondering if it is proper to change the HashSet to ConcurrentHashSet, if so shall I make a pull request?
ExecuteBlocking timeout2.It should be possible to set specify a timeout in an executeBlocking that happens when no worker thread is available before this timeout.This would be used in JDBCClient for instance, etc...
Deadlock if the connection is still being used while HttpClient is closing4.This happens in Vert.x 3.2. In thread 5717, we hold the lock on HttpClientImpl.close (lock #1) and then we try to acquire a lock on ClientConnection.close (lock #2). In thread 7035, we hold a lock #2 in ClientConnection.handleClosed and try to hold a lock #1 at HttpClientImpl.checkClosed. And boom, we get a deadlock.I don't have a good repro now since it's quite hard to reproduce consistently. But I can demonstrate from the stack trace:
EventLoop thread blocked on Vertx.clusteredVertx()0Vertx 3.2.1.Sometimes EventLoop thread blocked on Vertx.clusteredVertx().Method Vertx.clusteredVertx() called on the main java thread on application startup.
Websocket deadlock during handshake4.
Deadlock between ClientConnection and HttpClientRequestImpl5.Hi,After a week attached to our load balancer which does a simple poll to one of our rest end points (we have no other traffic right now) all 4 machines (4 instances) end up in deadlock and had to be "kill -9"ed , which  in theory brought down our whole DC. Since we are in testing stages this was not a big problem but I imagine to most it would be a bit hair raising.Also looking at the traffic it seems our load-balancer (a cisco content switch with ACE module) sends a request gets the response then resets the connection. I believe this behaviour is causing the deadlock, since this is important for us to go live I will try create a test to replicate this behaviour.This was running 3.2.0 code (Ive seen no fixes in 3.2.1) and using JVM 1.8_74 on RHEL 6.5Here is the jstack information.Debugger attached successfully.Server compiler detected.Deadlock Detection:
Deadlock in ServerConnection / HttpClientRequestImpl6.We use an HttpClient per thread, and this just happened.Found one Java-level deadlock:Java stack information for the threads listed above:There is a chance that it's just a glitch, we had some weird behaviour, two instances launched instead of one or something...Also, this code looks a bit dangerous (io.vertx.core.http.impl.HttpClientRequestImpl#getLock):
Lock is not released in clustered vertx2.When I updated the Vertx version, this code stopped working:It only executes the code at the first time, but the lock is not released. The execution reaches the release point. What can be the problem?
Deadlock with HTTP proxy app3.Hi,I've implemented a reverse HTTP proxy with vert.x-ceylon version 3.2.0. I have now several times found the app in deadlock state, after which it does not respond to new conncetion attempts anymore. Here is an example:Found one Java-level deadlock:Java stack information for the threads listed above:The org.otherone.vhostproxy.vertx.MyPump class in the middle of both traces is a customized version of the standard PumpImpl class which adds just some logging after the write() call. It does not hold any locks when calling vert.x apis.I strongly assume both threads are processing tasks related to the same request, since they lock against the same connection object, and in HTTP/1 only one response can be streaming data at once.To me it seems the deadlock arises because in both HttpServerResponseImpl and HttpClientResponseImpl the methods involved in the deadlock all hold locks while calling both upstream and downstream methods. By downstream I mean the pump calling HttpServerResponseImpl.write() and HttpClientResponseImpl.resume(), and by upstream I mean HttpServerResponseImpl.handleDrained() and HttpClientResponseImpl.handleChunk() calling the pump.  The pump is located at the "top", coordinating traffic between two downstream connections. As can be seen all four vertx http methods hold or attempt to hold locks. Having written numerous proxy, router and gateway software products in the past I have learned that this locking model is not deadlock-safe. One robust solution to this is to decide on a locking model where locks are allowed to be held a) only while calling downstream code or b) only while calling upstream code. Locking-while-calling-downstream has seemed sensible to me so that's what I have used.So if one would decide not to allow locks during upstream calls, the solution (where there is state that actually needs protection) is to introduce state instead. So for example HttpClientResponseImpl.handleChunk() that wants to deliver new stream data upstream, instead of doing (pseudocode):Introducing state variables typically makes the classes more complex as they may need to check whether they are "in the middle of something", but this should avoid deadlocks as long as it is clear what is upstream and what downstream. And in case it is not, don't hold locks when calling other components at all and use state in all cases..To further visualize what I'm trying to say here, here's before & after sequence diagrams - arrows towards the pump are upstream calls and arrows away from te pump are downstream calls:I would be interested in hearing what you think. I am considering looking deeper into the HTTP classes and see if I can get some pull requests going myself..
Blocked thread warning when opening or closing an AsyncFile 7.Users have reported blocked thread warning when opening or closing an AsyncFile.Here are example stack traces:and.One user provided more details and reported he was using a mapped Windows shared drive.I have been able to reproduce the opening issue by a slow network filesystem (with twickle and nbd).
File close is done from the wrong thread8.Got this thread warning from time to time due to AsyncFileImpl.close is called from Vert.x event loop.
Asynchronous read/write done on wrong thread9.The open/close issue has been addressed here #1573 . Async read/write still is blocking.
Context#executeBlocking does not log blocked threads3.No description provided.
Vertx Deadlock:  ServerConnection.handleClosed() v. SockJSSession.write()3This deadlock is happening on v3.2.1, however it appears that the same lock inversion problem exists on the current v3.3.3.   I will be trying to created a tiny, canned example to repro this soon, but because it involves a 'channel inactive' call I'm thinking this is far more of a timing issue instead of a connection/load issue.Also, all locking here is OUTSIDE of our 'acme' code, and inside the vertx codebase.  From my understanding of these stack traces and the threading model, this essentially means that any SockJSSession.write() call (regardless of the thread it is called from) can deadlock with a 'ServerConnection.handleClose()' call.some package names have been changed to 'acme' to protect the innocent:Found 1 deadlock.Heap.The only options we see right now to work around this are either (A) reducing the eventloop threadpool size to one, or (B) somehow acquiring and synchronizing on the appropriate ServerConnection instance BEFORE we make a SockJSSession.write() call.  This option (B) must be done before all calls to 'synchronized' SockJSSession methods, not just .write().The  'bus bridge impl' that we are using is based on the impl at:with minor modifications to support a 'PING' socket data message type.  The ping support just pokes some bean internal state, doesn't do any other socket read/write calls, and it independent of the above stack trace.
File caching implementation not thread safe4.The FileResolver implementation of caching is not thread safe. The problem occurs when the same file (that is about to be cached) is concurrently being read from multiple verticles. This issue can be reproduced with FileSystem#readFileBlocking, however, I believe that async implementation will yield similar results.The problem here is the following. Consider a use case, when a file is read in multiple verticles in parallel. This file does not reside in a local filesystem, and as such Vert.x will try to cache the file by moving it to the .vertx directory.First verticle that hits FileResolver#resolveFile will trigger the copying action of the resource to the designated .vertx cache directory. Only after this copy action is finished, the file will be read - thus this verticle will read complete data.Every other instance that hits FileResolver#resolveFile will find the cached file during resolution - as the file is currently being copied over there - however, it might happen that it will read the cached file before it is copied completely - thus this verticle will read only partial data.The problem can be reproduced by running supplied verticle in multiple instances. The verticle only reads 10 files from attached jar (all are 1MB in size) and outputs the size of read payload.
createHttpServer: io.vertx.core.VertxException: Thread blocked. ver 3.3.39.I followed the example, but an error occurred.An error does not occur in 3.2.1 but an error occurs in 3.3.3.Is this a JDK issue? my jdk version is 1.8.0_73.
httpclient deadlock1.my app locked today,then i print the stack log:Found one Java-level deadlock:Java stack information for the threads listed above:Found 1 deadlock.
Don't call HttpServer request/ws close handler holding locks3.Motivation:currently the ServerWebSocket and HttpServerRequest close handlers are called under synchronized lock of the connection. This can create deadlocks in SockJSSocket because a SockJSSocket may uses different request/websocket (reconnects) and event loops.Change:when processing a Netty close event, call the close handlers in the next tick (i.e scheduled to run on context).
Synchronize require() in Ruby verticles.When starting multiple instances of the same module/verticle type concurrently, and where the verticle does a require(), this can result in failures because require() in the shared ruby runtime is not threadsafe.We can fix this by overriding the require() method with a version that synchronizes access to the original version.
Shared instance of HttpClient is not guaranteed to be executed in the same thread4.This does not behave as I expected where the callback of HttpClient called inside event loop is not guaranteed to be called back in the same event loop due to the connection from another event loop is reused in another event loop.
HttpClientMetrics#createEndpoint may be called multiple times for the same endpoint6.The implementation of io.vertx.core.http.impl.ConnectionManager.QueueManager#getConnQueue can lead to HttpClientMetrics#createEndpoint being called multiple times for the same endpoint if different threads try to use a fresh HTTP client:HttpClientMetrics#createEndpoint is called from the ConnQueue constructor so a single instance of ConnQueue must be created if none is in the map.Otherwise SPI implementations my leak resources acquired in #createEndpoint as #closeEndpoint would never be called for the corresponding queue instance.
EventLoopContext#executeFromIO fails when executed by Netty GlobalEventExecutor5.When Netty uses its GlobalEventExecutor thread and propagates into Vertx, EventLoopContext#executeFromIO does not expect such thread and fails to execute the task:for instance:This can happen if the creation of a Bootstrap fails, in this case the promise is created with the GlobalEventExecutor (see AbstractBootstrap#initAndRegister())
Consult vert.x a multithreade problem3.The program starts with 3 threads, the thread pool is set to 2, but the result is only two threads.Program code is as follows:
Race condition in Future setHandler/completion between threads9.In one of our projects we have encountered a problem with CompositeFuture.all() when running "outside".of a verticle (in junit tests). Here is a sample program:After a while it prints something like.Inside implementation of CompositeFutureImpl.all() the setHandler() function is called upon each future in the composition (in our case two). The problem here is that the completion of futures runs in a different thread than setting the handler. Neither of the functions (setHandler() nor complete()) are atomic and it may result in.a situation when the handler is never called. The problem seems to be more fundamental. The future.implementation (FutureImpl) is not thread safe and using it from different threads may cause difficulties.When running inside a verticle it's ok because asynchronous callbacks are serialized within the event loop.and should not run concurrently.I don't know if this is an expected behaviour and Futures (and their compositions) should be used only.within verticles. Our understanding based on looking into the implementation of CompositeFutureImpl is that.the intention was to make it thread safe.
Race condition in clustering.No description provided.
Race condition in event bus sending.When two or more event loops attempt to send to the new server concurrently and the connection has not yet been setup.
Race condition in deploying modules.If deploy two modules of same name at same time, can result in one not being deployed.
Worker verticle cannot synchronously wait for EventBus.send even when using additional workpool thread8.The general idea is:execute in an worker verticle.instantiate a countdown latch.wait for countdown latch.From my observations the eventbus.send finish execution however nothing happens on the eventbus. The target does not get any message.I tracked a bit the problem inside EventBus.send and it leads me to HazelcastAsyncMultiMap.get implementation which does an executeBlocking which is ordered with the caller.The body of execute blocking in HazelcastAsyncMultiMap.get is never executed.This is strange however because the eventbus.send is executed by another worker thread than the caller.Is this the expected behavior? Seems to me like a problem.Some code:Sorry but the project is not compileable as a standalone but you can just copy this class. Will notify when i make the project work as a standalone project.
ConcurrentModificationException from ConnectionManager.close0.I got this exception from HttpClient.close:I'm guessing the problem is with the synchronization on the methods of the inner ConnectionQueue class that access the allConnections member:
Vert.x ReadStream hangs in paused if WriteStream fails6.I have the following usecase for a vert.x aplication:write a REST handler for a GET request.in this handler copy data from a ReadStream onto the response.This looked straight forward, in the handler I would create the ReadStream and then use a Pump to pipe the stream onto the response (WriteStream).I noticed however that it can happen that the client closes the HTTP connection to my handler while the pump is still active. In this situation I had expected an exception on the WriteStream instance to occurs. However this is not the case, instead the WriteStream#writeQueueFull method returns "true" which sets the ReadStream into paused mode. It now waits for a drain event, but this event is never sent, because the write connection has been closed. The result is that over time the number of open (paused) ReadStreams grows, eventually generating a leak.What is the correct way to handle this situation?The first aspect that looks strange is that there is no exception on the write stream. But even if I were able to figure out the error situation (e.g. by listening on the close event on the response), what am I supposed to do with the ReadStream? It cannot be canceled but it also cannot stay open. The approach I can think of is to pump its content into a nil stream (i.e. consume it but ignore its content).Overall this makes the complete pumping process pretty complicated.The example below shows a simple testcase to reproduce the issue. The main method makes a request against a verticle and closed the connection immediately.On the server (i.e. in the verticle) no exception is triggered, instead the ReadStream is locked in paused state.The output of the sample code is:request.
SockJSSession needs to use the transport thread when using the transport7.Currently the SockJSSession uses the transport for write/close operations from any thread. This can cause deadlocks when used from external threads or because the transport thread has changed (for instance a session reconnection with an scaled http server).It can be fixed by recording the transport context when the transport is associated with the session (register method) and doing a runOnContext when the current context is not the same. This shall also prevent race conditions for instance a write operation is scheduled from a non vertx thread, the register method is called and gives a new transport to the session. In this case the write will be rescheduled.
httpclient allows request to be created in a context other than the one an httpclient was instantiated in resulting in a deadlock on pumping.8Hi there,Here's a reproducer for #2287. Perhaps it would make sense to bind an http client request to the context in which it is created (like AyncFile) instead of the context in which it's first used or throw an exception. In my case this is only an issue in tests since I need to access some of the internal sfs api associated with a verticle instance. Another option could enhance the vertx unit artifact by creating a rule to deploy a verticle and allow a test to be run in the context of a verticle instead of the context of the RunTestOnContext rule so that resources initialized for a test (AsyncFile) could be pumped to an http client request that made an initial connection using a verticle context. The example below is trivial... in a more complex application, it becomes difficult to manage the current behavior during test execution. A non trivial example of an api might look like volumeReplicaGroup.consume(size, newArrayList(MD5, SHA512), asyncFile) and the http clients used internally by the volume replica group may or may not already be bound to the verticle context instead of the test context asycFile is bound to (it would depend on if the verticle had attempted to use the client).
Thread blocked9.
Possible deadlock when trying to get multiple locks.Here is the code, which illustrates the problem.The Consumer verticle acuires lock1 and gets stuck.I have reproduced the same problem using HazelcastClusterManager and IgniteClusterManager.
HTTP event handler should not be called when holding a lock1.We still have event handlers that are called while holding a lock which can lead to deadlocks.We should remove them.
HTTP Client - Deadlock8.Sometimes, it appears that some eventloops are blocked while doing some http client relative stuff (Vert.x 3.7.0)Here is a threaddump:I'm trying to provide a simple reproducer but it seems that the deadlock is pretty hard to reproduce.
Spring Async + Vert.x issue - Vert.x thread / Netty eventloop blocked (may be a bug)9Hi,When an async spring task is getting executed, vertx/netty threads are blocked.To see the issue in action:Result: The https will keep on processing and will not complete with status code 200 until we move forward the breakpoint.
Thread blocked for long time2.What is the issue is related too? And what is the possible way to solve it?
[Worker pool - ExecuteBlocking] Queing delay too high2.Hello,During some load testing on our architecture based on VertX I found some issues concerning the use of ExecuteBlocking. I don't know yet if it comes from our way of doing or if something weird is going on.1. Context.We are calling an endpoint that is in charge of execute some I/O (a rest call to a third party). For that we are executing that I/O in a executeBlocking() method (using vertx.executeBlocking() with ordered = false).Unfortunately, we can't use the Vertx webclient for that purpose.2. Configuration.We are using 4 event loops (fix to 4 as we have only 2 CPU on our hardware) and a worker pool size of 100 using 4 verticles.The I/O blocking process is executed in a average of 200ms.The load test is using JMeter (on the same local machine) using 1000 threads that will call this specific endpoint (using HTTP).For the verticle deployment we are using a specific Verticle that is in charge of deploying the "business verticles" based on configuration via  vertx.deployVerticle. The business verticle to deploy is configured that way :3. Where is the potential problem ?The average response time of the call is around 2 seconds.The delay is caused by the time to send the blocking task to the worker pool.As we can see in the metrics most of the queue_delay is around 2 seconds.As we can see the worker pool is busy but it seems that some threads are waiting sometimes (indicating that he seems not "over charged"). (we've tried adding more threads in the pool but it didn't change anything, more threads were waiting, that's all).The event loops are not blocked (we don't have any warnings, and we've set the warning to 100ms).We've set timers to correlate data for the queuing delay and we've noticed the same numbers (2 seconds delay for a process to be executed).Here is a trace (from Jaeger) displaying the duration for one specific call during load testing :We can see that the process inside the execute blocking is taking around 200ms but before that the process was doing nothing but waiting to be processed.Questions :Do we have missed a configuration ?Is there something we're doing wrong ?How can we explain that delay ?Can we avoid the queuing delay in some way ?This kind of contention is not happening with lower concurrency (like 100 threads)We've tried to have one worker pool per verticle (so 4 worker pool) to avoid potential contention but it didn't change anything at first sight.We can't have more than 2 CPU on our hardware.We are using Vertx 3.7.1Thank you and don't hesitate to ask me anything that can help you investigate this.
Deadlock in ASyncFileImpl/PipeImpl (using vertx-web-client and request.sendStream(asyncFile))2Hi,Using vertx 3.8.1Using something like (kotlin+coroutine) :It happens that the call never return, but it's random (Never when I tried to extract a reproducer, almost always in the real context, of course).To investigate, I've copied PipeImpl and ASyncFileImpl and added some trace. (Modified version in attached files with output)It's seems like at some time, something has kept the monitor of ASyncFileImpl instance, preventing the resume call  to execute the method.
Clustered Lock is not released if more locks are requested than available worker threads2.I am facing this issue while migrating from vertx 3.5.4 to 3.7.x.If I am requesting more locks than available workers, then lock is not released in vertx versions 3.6.x and 3.7.x. Please find the reproducer below:Above same code works if I change vertx version to 3.5.4.Also if I reduce numTestEvents in dataprovider(for above reproducer code) to a number less than 20 then also everything seems to be working.
Deadlock when deploying several instances of a verticle a starting net server4.This is a regression on 3.8.2 due to #3117.
Vertx#close Future will not callback the handler3.The Future returned by vertx.close() never completes because the thread pool has been shut down.The first of these works, the second doesn't:Sync.await is just a blocking method that waits for a future to complete using setHandler.The output from that is:and it never completes.
io.vertx.core.VertxException: Thread blocked5.
FileSystemImpl.existsInternal can block event loop thread3.Version.Invoking vertx.filesystem().exists() can block the event loop thread. This is because resolveFile() runs in the event loop thread instead of in the worker thread:Do you have a reproducer?You can mock calls to resolveFile() to sleep for any amount of time or set a breakpoint on the resolveFile call as a test runs. Wait up to the blocked thread checker interval and unpause execution. BlockedThreadChecker will report that the thread has been blocked.Steps to reproduce.See above.
Thread blocked in io.vertx.core.net.impl.clientconnection.ConnectionManager.getConnection6.While upgrading https from Vert.x 4.0.0 milestone 4 to milestone 5, we noticed a serious issue with HttpClient connections being thread blocked.. It seems to appear when connection can not be made to a remote host. Using same client and issuing another request that also fails results in a thread blocked warning and a halt of the event loop..
HTTP client retry on same host can self deadlock6.The HTTP client retrying the same host on a failure can self deadlock because it will close the pool after it performs the callback. It should close the pool before so when it retries and fail it does not retry indefinitely.
Thread blocked with drainHandler, writeQueueFull 7.POSTing many buffers to an echoing server gives thread blocked failures when we use drainHandler+writeQueueFull approach.The same POST with write and a completion handler does not.Write with handler is a much simpler strategy, but from documentation it's not clear whether it's an alternative for drainHandler. In any case we cannot explain our thread blocked problems with drainHandler.What's the best strategy for writing?Environment.Below is a unit test with test methods drainHandler and  writeHandler. Each test sends 10 times 2 GB to an echoing server (part of unit test).always completes and memory usage is low.
Race condition throws an unhandled exception in CompositeFuture.all5.Version.Which version(s) did you encounter this bug ?3.9.2, but this is likely in newer versions as well.Context.When at least two futures executing on different threads are passed to CompositeFuture.all(), there is a race condition throwing an unhandled and unexpected exception if the futures both fail at virtually the same time.The gap between this synchronized block in all method reached when a future fails.and the synchronized block in doComplete method.can create a situation where a subsequent future/thread sees that composite.isComplete() is false, yet making it to doComplete method, returning false after seeing result is not null.After returning from tryFail method, this then throws the unhandled and unexpected exception in fail method.Steps to reproduce.Here is a simple test with @RunWith(VertxUnitRunner.class) treating unhandled exceptions as unit test failures.
Blocked thread anomaly6.The following message implies the thread has been blocked for several times longer than the JRE has been running.
EventBus cannot handle received message when using Future inside2.Version.Context.I encountered an exception which looks suspicious while executeBlocking when handling received message in EventBus MessageConsumer.Do you have a reproducer?This is a test you can add it in LocalEventBusTest.It is exception when I debug in my project, hope this help.
Deadlock when closing Vertx and WorkerExecutor concurrently6.I met a deadlock issue recently on Vertx version 4.1.1 and after investigated and looked into the VertxImpl, CloseFuture, WorkerExecutorImpl, it looks like the deadlock happened in Vertx, please take a look:In my case, the deadlock issue is hit when the vertx.close and workerExecutor.close happened to be called at same time, one from main thread and another from eventloop thread. (the workerExecutor is created by the vertx.createSharedWorkerExecutor)Backtrace printed by jstack:Found one Java-level deadlock:Found 1 deadlock.Checked vertx 4.1.1 code and find:When closing WorkerExecutor, it will do:Lock self (WorkerExecutorImpl)Remove self from CloseFuture.Close SharedWorkerPool, which require lock Vertx.When closing Vertx, it will do:Lock self (VertxImpl)Close CloseFuture, which synchronized copy the list of callbacks and invoke them outside of the synchronized block, one of the callback is to close the same WorkerExecutorImpl instance as above.So, when the copy of callbacks in CloseFuture happened before removing the WorkerExecutorImpl from CloseFuture, it will still be called and cause deadlock.T1.Start close Vertx, locked VertxImpl instance.T2.Start close WorkerExecutor, locked WorkerExecutorImpl instance.T3.In CloseFuture, the list of callbacks are copied.T4.Remove WorkerExecutorImpl instance from CloseFuture. But since the callback is already copied in T3, this doesn't prevent the callback from being called.T5.The callback of close WorkerExecutorImpl instance is called. This require lock on the WorkerExecutorImpl instance, which was acquired by eventloop thread at T2. So it is waiting for the lock to be released by eventloop thread.T6.Close SharedWorkerPool, and this require lock on the VertxImpl instance, which was acquired by main thread at T1. So it is waiting for the lock to be released by main thread.Deadlock.Deadlock.The issue seems still exist in 4.1.2
WorkerExecutor.executeBlocking sometimes holds a lock on the WorkerExecutor after the blocking call has completed0.I ran into a Java deadlock in a large application, and based on the stack trace of the blocked threads, it seems to be caused by each thread running WorkerExecutor.executeBlocking on the same two WorkerExecutor instances, but in a different order. One thread looked like this:And the other like this:If that fut.onComplete call runs after the blockingCodeHandler has finished, the asyncResultHandler will be executed right away on the current thread. This means that we stay inside of the synchronized block as we continue on. If one thread does.we1.executeBlocking(...) followed by we2.executeBlocking(...), and another thread does it in the reverse order, and the first call each thread makes ends up running asyncResultHandler right away, you deadlock.Do you have a reproducer?Here is a simple JUnit test that reproduces it:Note that you also need to add a breakpoint on this line in WorkerExecuteImpl.executeBlocking, to ensure that the blockingCodeHandler always finishes before fut.onComplete runs:I used the Intellij "Evaluate and log" breakpoint option to insert a 200ms sleep on this line, which lead to consistent reproductions when executing the above test.Steps to reproduce.I suspect this is hard to reliably reproduce outside of a carefully constructed test, because the timing requirements are so specific.
ConnectionBase exception handler should not be called within the synchronized block7.No description provided.
Race condition in AsyncFileImpl writesOutstanding1.If an AsyncFileImpl is written to from a thread different to the context that created then the long counter writesOutstanding can get into an invalid state. This is because the counter is updated from both the caller's thread and also on the context when a write is complete.Fix is to use an AtomicLong for writesOutstanding:
WebClient: downloading url to file signals completion prematurely1.When I download file using BodyCodec.pipe(file) and try to read it right after completion, I sometimes read empty file (the data is not here).Following code reproduces bug (it's not 100% reproducible, for me it fails only on heavily loaded CI):It calls stream.end() which underneath calls AsyncFileImpl.close() and then signals completion right away, without waiting for close to finish. In fact this creates race condition between handler and file closing. So code which listens for completion event may receive this event before close() takes place (which means the data in file will be truncated).I fixed it locally by making my own copy of StreamingBodyCodec which signals completion to my code after file is closed via separate future (which is very hacky). After this fix bug stopped reproducing. But it would be nice to have it fixed in master as well.
Possible race condition with chunked responses after upgrading to 3.5.0 (now with reproducer)9Hello,after upgrading to vert.x 3.5.0 we've been seeing a consistent problem with one of our service APIs: when replying with chunked responses over SSL, clients are reporting encoding errors when processing the response.The problem is that after a certain load threshold (not sure if the number of such chunks of their size but I can consistently repro it in our service with >4000 chunks) the client will receive invalid chunks and break the connection.On the server side the problem surfaces with a long list of ClosedChannelException instances which for me I believe was a red herring as a root cause. Trying to debug the service, I noticed that the exceptions are legitimate and caused by a Connection reset by peer exception which is swallowed by netty:The channel will then be closed and triggering the large number of ClosedChannelException instances when the originally scheduled zrange commands complete.But there is no symptom on the server-side to describe an encoding problem so my hypothesis is that the client breaks the connection when it receives the first bad chunk thus triggering the Connection reset by peer which in turn surfaces the ClosedChannelException. So I think this is a symptom of a more delicate problem.When this happens, curl -raw on the web server's endpoint will display some chunks but then stop with one of the following errors:Also, comparing the payload observed by curl with what the server should provide, the chunk size appears to be correct every time but the chunk content is truncated at random points along the way.All of the above leads me to believe that the response is indeed corrupted but I can't figure out why/when/how or by whom the chunked response is corrupted in the stack:Other observations:the problem is isolated to versions above 3.5.0 (does not repro in 3.5.0.Beta1 or below and the service code which triggers this behavior hasn't been changed in a while)it only appears with https and toggling OpenSSL has no effect.for low number of chunks (less than 1000) it rarely reproduces on a cold service start but consistently reproduces afterwards.the actual payload (string content) does not affect it. Considering that there are times in which it does not reproduce with the same content, I believe it behaves like a race condition and not like an input validation bug.debugging appears to be adding synchronization and changes the behavior. I never succeeded in reproducing it while trying to observe this breakpoint. What I did was to condition the breakpoint to fire when contentLength == 0 (marking the end of the response). With the breakpoint enabled, it never reproduces. With the breakpoint muted, it always reproduces.looking at the change history, my initial idea was that the change which added this behavior was due to this issue (Serialize messages sent to the connection in the order of the synchronization monitor) which was closed with these changes. But after confirming that all messages are written from the event-loop thread, I think it's not relevant.HttpServerOptions.setMaxChunkSize has no effect.HttpServerResponse.setWriteQueueMaxSize has no effect.A reproducer can be consulted here.Any pointers/help to get to the root cause of this issue would be greatly appreciated.
Event loop context executing with wrong thread.I'm running it on a AWS instance, it shouldn't matter but maybe network performance is important in case this is caused by a race condition.
HttpClient race condition when getting a connection from the pool being closed7.Currently there is a race condition with the HttpClient between a client request and a connection close. The pool delivers the client request in the next tick and the connection might be removed from the pool between this the schedule and the delivery. At this moment the client does not know the connection was closed and is never aware of the connection close.We should avoid a next tick in this situation, i.e when the client request is fulfilled with a valid connection, it should be called immediately to avoid the race.Currently the fact that the connection pool can hold connections with multiples context does not allow to run the pool requests on the pool and thus the situation is hard to avoid without a re-check before the connection is delivered. A better solution might be to attach a vertx context on the pool and consider all connections will use the same context, this would allow to eliminate the race easily.
HTTP/1 stream should synchronise when updating the connection stream9.Currently when sending the request the HTTP/1 stream will update the connection state without synchronisation which can lead to race conditions. We should synchronise properly so the other parts reading the state will see the correct state.
Race Condition in HttpClientImpl.getConnectionForRequest, ConnectionManager.getConnection3.I am using the Vert.x 4.0.0-SNAPSHOT stack with an AdoptOpenJDK 11.0.7 on Ubuntu 20.04. The first time I encountered this was with a Hazelcast-managed distibuted Vertx system (with only one node) with both a standard HTTP server and a vertx-grpc server running on two different ports (both with TLS/SNI enabled). After making a few successful calls to both, I closed the client (cleanly) and I let it rest for a few minutes. Then the console started to output that same error at what seems to be a sub-second interval :Now I get it everytime I let the system run overnight. I'm the sole user of the system.Does this ring a bell ? I can't try to use it with the 3.x stack because the Vertx-grpc has a completely different codebase fo 4.0.0.
Connection pool can create connections which ignore keep alive settings8.Version.3.9.1 but this also happens on latest commit.Context.Since some time we've seen WebClient throwing Connection reset error on first request after a period of inactivity. I know that our gateway closes inactive connections after some time. The question was why WebClient doesn't close such connections, the default keep alive settings should remove inactive connections after 60 seconds. After some debugging I tracked the issue down to Pool having connections that don't have expirationTimestamp set. I believe there's some race condition with how connections are created and consumed. The flow is like this:First request is started. First connection will be created but it takes significant time for it to be connected.In the same time a second request is started. A second connection will be created but this one connects instantly.Second request finishes.Second connection is now available and it's used to complete the first request.First connection has finished connecting but the waiter list is empty. It will sit unused in the pool. expirationTimestamp is not set so keep alive settings are ignored.After some time our gateway closes the inactive connection by sending RST packet.Third request is started, it will use that first connection (now closed on the gateway side) and throw Connection reset error.Do you have a reproducer?See here.Note that I added a delay in HttpChannelConnector to simulate slow first connection. This reproduces the issue reliably. Of course in our environment I can reproduce the issue with unchanged vertx 3.9.1 and 3.9.4.Steps to reproduce.A pool now has a connection that won't be removed even if its keep alive time passes (expirationTimestamp of this connection is 0).Extra.
clientRequest not available in Interceptor6.Version.Context.In my interceptor, I want to know (once the request was done) what the request method was. To me it looks like I should retrieve the clientRequest from the HttpContext and from that object i can ask getMethod(). But the clientRequest method always returns null (although according to documentation, this should only happen during PREPARE_REQUEST phase, not in later phases.Reproducer.this test prints.ignore the rest of the project, that was for another issue.Extra.I debugged a bit. in HttpContext line 558, clientRequest field is assigned, and I figured that cool, it must be working. But when i let the code continue to execute, breakpoint at line 549 is hit and clientRequest is again set to null. So looks like there is a race condition or so between the request's onComplete handler and the continuation handler.
Two distinct HttpServer creations with port 0 yield the same random port (usually)9Summary.Two individual calls to vertx.createHttpServer(...) with port 0 yield the same random port (usually, see note on race condition futher down).In context of Quarkus, this is irritating for users because Quarkus will print something like:Furthermore, something seems to break after a very special sequence of QuarkusTests (that I'm not allowed to share due to closed source) in Quarkus 2.0.0 which uses vert.x 4.1, but that doesn't happen with Quarkus 1.13.7 which uses vert.x 3.something.See quarkusio/quarkus#18154 for details.This problem vanishes if I use a fixed port for https (my tests are using http anyway, not https).While debugging/adding logging, I think I have also found a concurrency issue in TCPServerBase that I have described here:Do you have a reproducer?See Quarkus based reproducer in quarkusio/quarkus#18154.Extra.
Pool can deliver lease with null connection2.A race can happen in the pool when it delivers a lease. Leases are created as pool post actions and sometimes can read a connection from a slot that became null in the meantime the post action is executed.This leads in an NPE in the HttpClient that instead should not get non null closed connection, instead of a null connection. (reported here #4220 (comment))
Race condition when use RecordParser with concatMapCompletable and observeOn.Version.vertx 4.1.7Context.I encountered some issues when using RecordParser with concatMapCompletable and observeOn like the following code to process records one by one on worker thread:The issues including:Sometime the RecordParser will suddenly stop emitting record, there's no error and not reached end of file and it is not disposed... it just stuck there.Sometime a MissingBackpressureException is thrown.After investigated and tried create a unit test for this issue, it looks like a race condition on RecordParserImpl:Usually the record is emitted from RecordParserImpl on event loop thread. (RecordParserImpl.handle)However, when backpressure exists (concatMapCompletable here), it can be the thread running on downstream to request item from upstream. (RecordParserImpl.fetch)In my case, the inner stream of concatMapCompletable is switched to a worker thread using observeOn, so it will be the worker thread requesting item from RecordParserImpl during backpressure.When the RecordParserImpl.handle running on event loop thread and the RecordParserImpl.fetch running on worker thread are called at same time, race condition happens because RecordParserImpl is not written in thread-safe way.The race condition includes but not limited to:In RecordParserImpl.handleParsing(), both thread may passed the parsing check and do the parsing concurrently.demand could be modified concurrently and result in unexpected value.When RecordParserImpl.handle has filled all demand and paused upstream, but before parsing is set to false, the RecordParserImpl.fetch could add demand and exit quickly due to parsing is true, and then it will stuck - upstream is paused so RecordParserImpl.handle will not be called again, downstream has requested item and is waiting for next item.Steps to reproduce.Here's a unit test that could reproduce the issue:In the printed log, we can see the xx: Read record yy log can sometime be printed on event loop thread (RecordParserImpl.handle) and sometime be printed on shared worker thread (due to backpressure, RecordParserImpl.fetch)The backtrace for issue 1 looks like:The source did not signal an event for 5 seconds and has been terminated.java.util.concurrent.TimeoutException: The source did not signal an event for 5 seconds and has been terminated.The backtrace for issue 2 looks like:Not sure if it is a problem in RecordParser or concatMapCompletable (is it expected for upstream to be requested on worker thread in this case?) or maybe it is not a desired to use them in this way?
Deploying HelloWorld Verticle in OpenShift v3.7.46 ThreadDumps.Thought I would start with this group, but it may be more of an OpenShift issue.We are seeing threads block deploying the simplest of applications.  We are new to Vertx, so it could be how we have something configured in the Dockerfile.  Attached is the console dump from the OpenShift pod, and the Dockerfile we use to build the image (we do not use fabric8 yet to deploy to our main OpenShift clusters).OpenShift Master: v3.7.46.Vertx Maven Version: 3.5.2.CR3.I can run the same, generated container locally in Docker (version 18.03) on CentOs without seeing these issues.Thanks for any help.
Vertx HTTP Thread block unknown warning issue9.times to times we get this thread-block warning exception. any idea?
Client connections on GlobalEventExecutor thread that are not propagated to the application7.Sometimes the Netty's bootstrap may notify client connections on the GlobalEventExecutor thread when it happens early in the boostrap when an event-loop has not been yet determined. Vertx expects notifications to happen on the event loop thread and this results in a Uh oh! Event loop context executing with wrong thread! failure.
executeBlocking callback run on the wrong thread for asynchronous future resolution8.Currently the executeBlocking(Handler<Future<T>>, Handler<AsyncResult<T>>) might execute the callback on the wrong thread when the runOnContext(...) execution loses the race against the thread that completes the future. This never happens when the worker thread completes the future, but it can happen when the worker thread hands off the future to another thread that completes it.This happen in the Mongo client and fails with a stack-trace like:
io.vertx.core.VertxException: Thread blocked9.Hi there,i am already using vertx.executeBlocking() but i always get this exception:For explanaition:I download some files from ftp server and this takes some time. So i have also tried to set the checker interval:but this doesn't have any effect.It is a clustered vertx instance (source file).What can i do to solve this problem? Or how can i increase the limit?
Vertx hangs instead of closing while using Micrometer4.I'm trying to use Vertx + Micrometer and I see that that when I want to call vertx#close the application doesn't exit and some threads from the Micrometer class stay there forever.However when I run the both standalone (either Vertx or Micrometer), everything is fine. So this issue only happens when I try to use both at the same time.Here's a screen shot of what I see when I try to close the vertx instance, but it hangs.Below you can find a link to the repository capable of reproducing the issue.
Verticles deployed with multiple instances do not scale processing evenly over the event loop threads4.Observation.Observed during profiling cpu processing that vertx event loop threads are not evenly utilized in the scenario where multiple verticle instances are deployed.Expected that with multi-reactor pattern, the event loop thread processing could be delegated evenly over the number of processing cores of the server.Context.During performance testing with vertx http servers listening and created over multiple verticle instances, we see in profiling that only 1 of the vert event loop threads are using for processing incoming http requests handled on the Router.Steps to reproduce.Start a http server using code similar to below test listeing on port 8080.Start http traffic into the server with a client tool.  I used 5 requests per second with client written using vertx and http2 protocol.  I can show the code for this client if needed.With a profiler (used yourkit), observe the threads cpu utilization and filter out the vertx event loop threads.Observe only 1 of the threads is used while the others are used in processing.Related question on stack overflow:
Example code of Hibernate reactive Docs might create another new Vertx instance3.Questions.Hibernate reactive Docs seems non-rigorous.It will create a new Vertx instance when I run the example code,and it might cause some operations to be not thread-safe.
HttpClient pool gets inconsistent on a synchronous connection failure.The HttpClient pool assumes that the underlying ConnectionProvider will make an asynchronous callback, which is the case with the asynchronous DNS resolver but not with the JVM synchronous resolver when the host cannot be resolved.
HttpServerResponse operation should not fail synchronously when the stream is is closed6.We already have made some improvements here and for we should have the same behaviour for other related responses methods.when the stream is ended (by the user) then it should fail synchronously because it is a programming mistake.when the stream is closed then it should not fail synchronously but should fail asynchronously. In Vert.x 4 the synchronous operation return a Future result that be failed in this case.
Do not lock the handler calls in AsyncFile4.Currently the AsyncFile implementation calls the file handlers in a synchronized block. This synchronization is only useful for reading the handler and should not be used when calling them.
Concurrency issue in listener of TCPServerBase#listen2.Version.Context.I stumbled upon a concurrency issue while debugging TCPServerBase in context of #3994:There is a race between what's happening in that listener that is registered via bindFuture.addListener(...) and the code that is running directly in that listen method after doBind() is done.That listener is setting the "outer" field id with the actual port but that field is also pre-set before that listener with port 0.So dependening who is faster (the listener or the outer method code), the final sharedNetServers.put(id, this); will either pick up the id with the actual port or the one with 0. This is even less predictable due to id not having volatile or synchronized.The effect while debugging was that if I waited before sharedNetServers.put(id, this);, I was getting another random port for each verticle (correct term?) while without debugging all got the same random port.
ArrayList concurrency error when deploying the verticles7.Hi.We have an intermitent problem with the startup of our applications, in a few attempts I can reproduce so it it quite often to occur.This is the error:method: registerVerticleFactory.The exception is thrown by ArrayList.sort, here is the code.So from my analysis the ArrayList is being modified while it is being sorted.We have 20 verticles in this application, so we iterate over the collection of class names and invoke the method below from.rxDeployVerticle(String name, DeploymentOptions options)Please let me know if any additional information is required.Thanks.
Locking on Jersey.I'm trying to use Lock and Unlock on Jersey Resource.User makes a POST request.System LOCK "A".System does some stuff.System UNLOCK "A".User makes another POST request.System LOCK "A".System does some stuff.System UNLOCK "A".The system crashes at point 6 (view attachment).If i try to make some LOCK-UNLOCK in a while loop it works, but when i make these LOCK-UNLOCK from different Threads it does not works.
Lock TTL.Any plan to add TTL to a Lock operation? Don't confuse with tryLock with TIME.I refer to a situation where a thread is dead and leave a resource locked (deadlock).
Deadlock while obtaining lock.Thread gets stuck while obtaining lock. Running 'keys L*' in redis-cli returns an empty list.The same lock name was likely concurrently obtained and held by another thread possibly on another jvm, and then released.Redisson version used: 1.1.5.
Lock reentrancy race.At line A the lock key is set only if it doesn't exist (NX), but on line B it's set assuming it still exists. If the lock timeouts between A and B, another process may obtain the lock, which is then overwritten at B.Here's a test case:This will reliably fail if:run in debug mode with a breakpoint placed in RedissonLock.tryLockInner(long, TimeUnit) on line B (line 306 for 508b903).the breakpoint is released after waiting at least half a second.The same problem is also present in the no-args method RedissonLock.tryLockInner(). (though a race condition is possible only if upgrading an expiring lock)I wonder if reentrancy support shouldn't be done purely in java.
RedissonLock locks up.Under heavy use on production, Redisson's locks get all locked up, and the application stalls. I'm using Redisson 1.1.5.I have 1 thread locked trying to release a lock:Also of note, I have about 30 other threads locked awaiting for a lock (a different one from the one used by the previous thread).I checked the threads with jstack, here is the relevant output:I checked Redis's state:this key is the one corresponding to the thread blocked trying to release a lock. The other threads, that are waiting for a separate lock are locked even though there is no-one taking up such lock....
when thread is interrupt...all Reference variables is locked!!the redisson is locked!all Reference variables is locked!!
java.lang.ClassCastException  java.lang.Integer cannot be cast to org.redisson.RedissonLock$LockValue.when use lock,but throw some class cast exception,sample code :error stack:
Use issues ----lock(long leaseTime, TimeUnit unit)when i run the method.com.lambdaworks.redis.RedisException: ERR wrong number of arguments for 'set' command.How to solve this problem?
lock.lock(2, TimeUnit.SECONDS)RLock lock = redisson.getLock("anyLock");lock.lock(2, TimeUnit.SECONDS);lock.unlock();run it ,report error:
lock.lock(2, TimeUnit.SECONDS) report error ERR wrong number of arguments for 'set' command.RLock lock = redisson.getLock("anyLock");lock.lock(2, TimeUnit.SECONDS);lock.unlock();run it ,report error:
[BUG]Thread dead lock when using distributed lock on 1.2.1.I run into a thread dead lock problem when using distributed lock on 1.2.1:After a certain period, some threads will be dead lock, and we can tell this by VisualVM's thread dump:
Thread dead lock bug when using distributed lock on 1.2.1.I run into a thread dead lock problem when using distributed lock on 1.2.1:There are ten threads call this function like this in a thread pool:After a certain period, some threads will be dead lock, and we can tell this by VisualVM's thread dump:I don't know why, is it a bug?
ConnectionManager call hangs forever if exception is thrown during Command processing.Bug found that can cause MasterSlaveConnectionManager to hang forever on get() call if exception is thrown anywhere in CommandHandler.To replicate the bug, you can use RedissonMap with JsonJacksonCodec to put instance of class that doesn't have default constructor. When you try to fetch that object by using RedissonMap.get() call, deserialization of object will fail in MapOutput because of missing appropriate constructor and thread calling RedissonMap.get() will block forever.In more details, this is happening because get() method awaits forever on Future object, which is released when Command.complete() is called. This complete() call is executed in decode() method of CommandHandler after RedisStateMachine processes Redis response. If, for example, RedisStateMachine throws an exception, complete() won't be called and result/exception will never be set to the Future object. This is causing calling thread to block forever in MasterSlaveConnectionManager.get() method.Pull request with test case that is proving this bug and bug fix proposition will be published shortly.
Issue in locking on key in concurrency.In my project, I have a servlet, that call a utility class.Utility class calls a wrapper class that  I have implemented over redisson lock.Wrapper class holds RLock object for a thread and provides lock and unlock methods that call RLock's lock and unlock method.But while running a apache ab-test tool for concurrency, Following exception occurs.Can you tell me if I am missing something over here?
Opening new connections in org.redisson.CommandExecutorService#async.Rmap.addAndGetAsync (and any *Async methonds) - as i understand it shouldn't lock the calling thread, and return value (Future) as soon as possible.But if for any reason connection is not established yet the calling thread will be blocked.And if redis server is not available at all it will be blocked for very long period of time.Maybe i'm wrong but it doesn't look like async nor lock-free.
RLock.tryLock() thows Exception.org.redisson.client.RedisException: ERR Error running script (call to f_2d027cdc209d32fe7dade9ba284110f88d497180): @user_script:1: WRONGTYPE Operation against a key holding the wrong kind of value . channel: [id: 0x24baaaa8, /192.168.99.1:64875 => /192.168.99.100:6379].ENV: redis 3.0.3/2.8.3.REDISSON: 2.1.1/1.3.1.
EOFException when I use RLock with SerializationCodec.I configure Redisson to use SerializationCodec instead of default JsonJacksonCodec. Then I run my code in environment with concurrent threads and use Lock object to sync thread. After that I get exception and unlock only after expiration in 30 sec. Previous major version of Redisson does't contain this issues. Similar problem I have when I use CountDown.
RLock.isLocked() get hung when I disable/enable my local network.I was crashed into a blocking issue when I was doing some configuration on my windows laptop. RLock.isLocked()  got hung forever with callstack as following:Callstack:The issue is repreduceable. Steps to reproduce the issue:My os is windows, and I haven't tested it on linux.Steps: disable network.  Then enable network. The program will be blocked with the callstack I pasted.
RLock did not work.It's very wired. I just run a very simple test case to try redisson lock. But the result is not good.Can anyone show me why RLock didn't work?The results are as below. It seems the lock failed completely.
ElasticacheCluster not working correctly with DistributedLocks.I am trying to use redisson 2.2.7 for distributed locks. I have a 3 node Elasticache cluster with one of them as master.When I try to get lock using the following code, i get an error saying that i cant write to a Slave. My assumption is that i can feed a list of nodes to the redisson client as Elasticache nodes and it will figure out who the master is...is that not true?The error that i get is shown below..Here is how i am setting up a redisson client.
Read Write lock cannot be correctly unlocked.in ReadLock and WriteLock,  the unlock method use thread id to determine whether it is owned by this thread. it works fine for a single machine.However, when it is used in a distributed processing framework such as MapReduce, the Read and Write lock cannot be correctly unlocked.I browse the source codes and find that the UUID is generated in the lock, I think it is better to use UUID as lock id  istead of thread id or provides a way to let users to set a unque id.
Deadlock using RedissonMultiLock.I am using simple app to run on 2 clients to test locks.While using RedissonMultiLock with 3 locks both clients blocks.
Error if first node is down in 3 node Master-Master cluster.I have a Master Master redis cluster of 3 (node1,node2,node3). Locking mechanism works fine if we block the network for node2 or node 3. But if we block node1's network then it fails with the below error.Below is how I'm creating the Redission client.
Rlock Exception in cluster mode.Hi.I am using redis 3.0.6 (4 node cluster  ) and redisson version 2.2.5.Sometimes when I try lock a key I get the following exceptions:my code :
Rlock performance issue.Hi.I am using reddison Rlock with a cluster setup , and sometimes I see latency( up to 1000ms)  when trying to acquire the lock or unlock.I saw this issue opened by zhxjouc (#455) with a similar problem  and I am working with 2.2.13 but I am still getting latency when a thread is trying  lock a key.My code is running with Java thread pool for accessing redis, I notice that if I work with pool of size 1-2.almost no latency when getting the lock, but working with 30-50 threads cause the lock delay.It could be thread overhead issue but I think that 1000ms is too long for that.Any help on how can I get better performance when locking an unlocking?.Thanks.
Dead Locks Happen in lock() Process.when heavy concurrency happens in my application, a few lock requests will "sink" without any responses, even after the lock lease time has passed. All of these requests wait at RedissonLock.lockInterruptibly().The exact position is RedissonLock.get() after RedissonLock.subscribe().In my opinion, this may be due to a thread removes the netty listener which is used by another thread. It can happen in this way:Thread A is in the loop of getting the lock after subscription.Thread B has also applied subscription and waits for result.Thread A gets the lock very soon and enters  RedissonLock.unsubscribe(). In this step, it possibly removes all the listeners on the same channel, which includes the listener used by Thread B. It causes Thread B can never get subscription response and hang on forever.The similar issue is at http. But I think it is not solved completely.Also I suggest  to apply the ttl algorithm to RedissonLock.get() because this step can cost some time. And if it has a timeout, dead lock can be prevented in a work-around way.This is the thread dump when dead lock happens:
Asynchronous lock release.Hi,I'm trying to create an infrastructure where different machines acquire shared locks through Redisson. Once the lock is acquired, some async tasks gets done, finally, when I finish the job, I'm releasing the Redisson lock through the thread currently running - but i receive the following error.java.lang.IllegalMonitorStateException: attempt to unlock lock, not locked by current thread by node id: xxxxx thread-id: 57.So, I understand the meaning of that, but since I want to perform asynchronous work, I cannot use the acquiring thread to perform the release.Is there a solution for asynchronous programming? Should I not use Redisson Lock?I also references the issue on SO.Thanks.
Deadlock on lock() and not only.Hi,I want to raise the issue with deadlocks again.This bug is still exist and making big headache. As before, It present itself only on very heavy loaded tasks, but in this case I is happens only when client talks to the claster which is located remotely.With a single-local or claster-local servers I was unable to reproduce it, but with remote server it happens with rate 1 / 20 (means from 20 runs of "heavy-load" JUnit test it happens only once)I can see where thread is locked down, it always stuck in CommandAsyncService.get(Future), on l.await() line and never exits from it. As I understand something wrong with mainPromise object, it is staying in incomplete state... and nobody change it. I tried to understand the logics in CommandAsyncService.async(...) function, which is actually deals with connection, retry, redirections and at end should release (or fail) the mainPromise object, but it is nightmare. All these spagetty with promises and futures made the code difficult to read and impossible to analyse. For sure BUG is there, but I am near to give-up. Any thoughts?
Exception in using RedissonMultiLock.I have 5 independent masters and I use single server config to create Redission instance.The RedissonMultiLock works fine if all the redis nodes are alive.If I shutdown one of the redis nodes, it will throw RedisConnectionException.Exception in thread "main" org.redisson.client.RedisConnectionException: Can't init enough connections amount! Only 0 from 5 were initialized. Server: /192.168.223.128:8000.According to The Redlock algorithm, it should try to lock another node but not throw exception. Do I use RedissonMultiLock correctly?It tries to acquire the lock in all the N instances sequentially, using the same key name and random value in all the instances. During step 2, when setting the lock in each instance, the client uses a timeout which is small compared to the total lock auto-release time in order to acquire it. For example if the auto-release time is 10 seconds, the timeout could be in the ~ 5-50 milliseconds range. This prevents the client from remaining blocked for a long time trying to talk with a Redis node which is down: if an instance is not available, we should try to talk with the next instance ASAP.
RLock trylock blocks forever.Found this in 2.2.16, seems like this was not around in 2.2.10. But still verifying.When tryLock is called with 0 wait time, the thread blocks forever. I took a thread dump and it looks like the stack pasted below.A note on the environment: We are running this against an Elasticache cluster in AWS and accessing from 4 EC2 instances. We have seen a lot of command timeouts. I am not sure if that is some way leading to this.
RedissonLock.isHeldByCurrentThread() doesn't check properly.I am using RedissonLock in my system.The lock realted logic in service is like this:But in the log I found:I can see the the thread name is same, so the thread got the lock and the thread try to unlock is same thread.Is there any problem in my code? Or in the logic of lock.isHeldByCurrentThread()?
loop lock blocked when master-slave failover.description: after master-slave failover , the test code blocked forever .redis cluster :test code:exception:
attempt to unlock lock, not locked by current thread by node id.redisson version:2.2.21.implement code:
tryLock method throw Exception:attempt to unlock lock, not locked by current thread by node id: 157ddcf5-0cc2-445d-bdea-98160a459bd5 thread-id: 42.@mrniko.hi, I stress test tryLock(long waitTime, long leaseTime, TimeUnit unit) method, throw Exception:attempt to unlock lock, not locked by current thread by node id: 157ddcf5-0cc2-445d-bdea-98160a459bd5 thread-id: 42.unnormal code:1:but use lock() or lock(long leaseTime, TimeUnit unit) method is normal, why?2:tryLock() can not be block? lock() is blocked?3:tryLock(long waitTime, long leaseTime, TimeUnit unit) method, waitTime must be more than leaseTime value, otherwise occasionally have "attempt to unlock lock, not locked by current thread by node id" ecxeption, in stress test condition, why?
redisson lock failure problem.redisson version:2.2.24.@mrniko.hi, i user jmeter pressure measurement  redisson lock.scene :Pressure test scenarios:Using two tomcat, simulating the two service requests, each request the server queue to obtain a lock, concurrency of 5000, continued request for 20 minutes,After acquiring the lock counter, increments by 1. It appears six times double counting problem, log is as follows::A server.lock and unlock code.
rlock error report.
RedissonRedLock & RedissonMultiLock lock method stuck.If more than 3 locks are supplied to RedissonRedLock or RedissonMultiLock instance then lock method could stuck.
RedissonRedLock trylock success while another thread already hold the lock in specific conditions.it's reproducible, my code is below.redlock lock result will be success, but when I check redis, I found the multi locks belong to different thread.like:I did more test and found:if the single lock is the first of redlock, it's ok.when I move multilock2 to first position ,it will return false correctly.the code below.I am wondering if it is a bug or not?
CommandAsyncService blocks indefinitely.Hi,I have a thread stuck in the CommandAsyncService#get() indefinitely waiting for the CountdownLatch. I don't have a particular repro case but this happens every once in a while on our servers (under load). Redis itself is still delivering events and the instance receives objects on other threads as well.Would it make sense instead of waiting indefinitely on the latch to only wait as long as the timeout is configured (as a safe belt) and abort the action if there hasn't been any success/failure by then?Cheers,Philipp.
DistributedLock - Lock is getting acquired by multiple threads.We are using distributed lock to ensure that only one thread can take a particular action, among a group of threads spread across multiple EC2 instances.Here is a snippet that represents what we are doing.The issue is that we are seeing multiple threads from being able to acquire and do the job.What might be the issue? Do you see an error in the way we are using distributed locks...is there a better way to solve this problem?Our goal is to ensure that for every job run...only one thread gets to do the job.
Documentation on locks.I was wondering if the documentation could be clarified on what happens to each of the lock types should one redisson node request a lock, then die before releasing it while another node is waiting on it.i.e.If the documentation could also note suggested ways to prevent deadlocking in such cases (should deadlocking occur) that would be good too. It's not clear if the distributed locking takes into account the loss of a node while holding a lock.
Unlock does not really unlock.If you unlock a FairLock from some thread, there is a delay until all threads actually see that lock as available.Example test clase.
RReadWriteLock is not reentrant.I was expecting the RReadWriteLock to be reentrant, as the wiki page describes it as (emphasis mine):Redisson distributed reentrant ReadWriteLock object for Java.However, in my simple test:The "reentrancy" clause of the ReentrantReadWriteLock Javadoc states:Additionally, a writer can acquire the read lock, but not vice-versa.My use of redisson relies on these locks being reentrant (at least within the same Thread), however this does not seem to be the case - is this a bug, or a configuration issue on my end?
Problem with Rlock - unlock not releasing lock to waiting threads.I am seeing an intermittent issue with Rlocks. I am implementing what is basically a distributed cyclic barrier using Redisson.I am running multiple instances of a service on different virtual machines. Each instance receives multiple HTTP REST api requests, handled by multiple threads. As each request is received it waits at a redisson count down latch until the threshold for # of requests has been reached.At this point, I need one thread to calculate the results and store it in redis so it can be available to all the other threads across all instances. So each thread tries to acquire an Rlock (with TTL of 5 seconds). One thread gets the lock, does the calculation, and stores the result back to redis. It then releases the Rlock.Now all the other threads that are waiting on the Rlock can (one at a time) acquire the lock, see that the data is already in redis, and simply release the lock and send the result back via HTTP, without repeating the calculation.We are using a master/slave redis cluster with 1 master and 2 or more slaves. As well as using redisson for the countdown latch and rlock, we use jedis for standard redis reads and writes, but never using the same keys as we use for the latch and locks.This is all working about 99% of the time. However periodically, I can see that after the Rlock is released, none of the threads that are waiting for it are able to acquire the lock. I was originally using lock(), and finding that HTTP threads were hanging forever. I switched to using trylock, and this returns, but it returns false, indicating a timeout waiting for the lock.I have added logging to be 100% sure that unlock is being called on the lock in the failure case. Any suggestions on how I can debug this issue, or any known problems with our configuration that might be causing it?We are running Redisson 3.2.3 and redis 3.2.5.Thanks for the help,Sue.
Indefinite lock lost during master failover.I've found my indefinitely held locks will sometimes disappear after a master/slave failover.  The block here will not reschedule a renewal if the attempt of the update fails.The update can fail during a failover, in which case the lock is gone for good.The exception that gets thrown when the update fails:
CommandAsyncService blocks indefinitely [without OutOfMemoryError].all application threads are blocked with stack trace.
RReadWriteLock is incompatible with reentry r&w op.While using RReadWriteLock, a bug (or not?) confuse me for a long time.I described a wrong ops yesterday, and fix it now. Sorry about this.There is a sure logic problem if ops like this:So if there is no holding check and running in a concurrent environment, op4 may throw IllegalMonitorException.But it's ok as follows:I found that "read unlock" will delete the whole lock without checking if there is any other write lock.Is it a bug, or it just shouldn't do and need to do in another way?
Cache ops taking too long.Currently i'm using Redisson 2.7.4  as JCache Provider, and some cache operations are taking a long time to execute, as can be seen in the log below:in some cases it takes ~30 minutes.Taking a thread dump I noticed that all EJB async threads (and some http-executor threads too) were in the same state, generating a huge queue in the async thread pool:Thread dump for some threads (Complete thread dump can be downloaded here):it looks like that is a lock issue, but i don't know how to solve it.
Concurrency Issues.Today I tried some tests to experiment with Redisson's performance using a build from /master and discovered significant concurrency issues. I'm not sure what the state of this code is, so perhaps these issues are known ones, but wanted to make sure you guys are aware of them.I have a very simple app that a) Creates a Redisson instance with a connection pool of 50 to masters and 50 to slaves, then b) creates a quick thread pool using Executors.newFixedThreadPool() and then c) feeds it Runnables, each of which generates a random string and adds it to the end of a Deque.Whenever I do any of these three things: 1) Add >1 millions of queued Runnables or 2) Increase the number of threads >20 or 3) add a .contains() call on the Deque (more on that in a sec), I get a ton of exceptions that don't include any of my code in the stack trace. Here's one such snippet:
RedisConnectionException in High Concurrence.I use redis as required data store, but in high concurrence, there are almost 50% connection failed. I want to know whether to optimize my redis server(use cluster) or my client? Is there any great suggestion? infinity retry until success? And My test code are list after:I use 1000 threads, and find that if i set the connection pool size to 500, the successful proportion achieves maximum.
RedissonSortedSet's order seems broken.Hi, we have been using RedissonSortedSet and just found that on certain cases the order can be broken and I assume RedissonSortedSet is supposedly thread safe.Assuming RedissonSortedSet of integers are added with the following integers : 103, 101, 102. RedissonSortedSet state (list in the redis server) should be: 101, 102, 103. However, there are cases when it is not. It may become 102, 101, 103 instead. And looking at RedissonSortedSet's source code, it uses binary search that requires the list/state to be always sorted. Therefore it breaks many other functionality.The use case that breaks (in my case) is when we are adding multiple items and there is another thread that delete an item in between. Looking the source code implementation briefly, it may be because when removing an item it does not consider getCurrentVersion() (just a quick guess).
Redisson map get makes the thread waiting forever.The map is a regular <Integer, String> map, and by calling a get on some key we see the following thread trace:
Incorrect RedissonRedLock.tryLock behaviour.Lock could acquired by several threads at once when leaseTime parameter less than waitTime parameter passed into RedissonRedLock.tryLock.
performance slow, when multiple thread.I use 1000 mutliple thread to test the list operation. the costTime is average 1000~2000ms. But when single thread it costs 1ms.
StackOverflowException in URLBuilder.URIBuilder is not thread-safe. Following scenario causes infinite recursion and StackOverflowException when someone tries to create new URL:URLBuilder.replaceURLFactory() - URLBuilder.currentFactory set to null, URL.factory set to custom.URLBuilder.replaceURLFactory() - URLBuilder.currentFactory set to custom, URL.factory set to custom again.new URL fails on StackOverflowException. Following code is an infinite loop because currentFactory points to "this":Affected version: 3.3.0.
RedisNodeNotFoundException : Never catch, so never release the countdownlatch.Hello,I have a cluster 3 master and 3 slaves, freshly created.You can see the conf in config dir and the creation script is init_and_launch_cluster.sh.I launch a Java junit test which use Redisson.The test initialise redisson (with a scan interval set to 100 ms in order to produce the bug quicker),and insert key into the redis cluster in 15 thread.I launch the kill-random-redis-master_local.sh script. This script kill a random master, wait 15s (nodetimeout is 10s),relaunch the killed master, wait 10s, and start over.In generally 10 minutes, I have this exception :I can't understand why there is this exception, but it is not catch anywhere.So the thread which handle this command is stuck in await of the countdownlatch (CountDownLatch:148)After 15 RedisNodeNotFoundException, all my insert thread are stuck, i can't insert any more.I made a little project on github, to help people reproduce this:Thanks for you're help.Ps :  In pj you found the log of the last reproction i made (redis 3.2.6 and redission 3.3.0)logs.tar.gz.
URIBuilder seems not to be thread safe.We test this in case of a multi cluster setup and sometimes went the java.net.URL.factory to null instead of the original factory.Best,ebersb.
RedissonList's Iterator Race Conditions.RedissonList iterator as it tries to keep "up to date" with data has a race condition in which if between the .hasNext() and the .next() call the set is emptied the list will throw NoSuchElementException.Unit Test.Additional Information.This also effects RedissonSet, but much less likely to hit the race condition. It causes RedissonSet line 96 to throw a null pointer exception.This is akin to issue #104.
Report possible race issues.Hi, Developers of mrniko/redisson,I am writing to report two race issues on use of ConcurrentHashMap. The issues are reported by our tool in an automatic way. Although manually confirmed, they would be false positives, given we do not know the specification of the program. We would very appreciate if you could check below for details and confirm with us whether they are real problems. For more information, please refer to our website:Line 120 is in the synchronized block on "ENTRIES". If the intention is to guarantee exclusive access for the !containsKey checking on "ENTRIES" and to ensure the atomicity of the checking and unsubscription (line 120,121), then the write operations on "ENTRIES" in line 66, 115, 117, 136 may break this. Relying on the ConcurrentHashMap to ensure exclusive access is dangerous since ConcurrentHashMap has no guarantee of exclusive access.
Redisson shutdown hangs if redis server was down.Start redis server.Create redisson bean.Stop on a breakpoint inside test.Shutdown redis server.call redisson.shutdown()What I see is that ConnectionWatchdog is scheduling tryReconnect indefinitely incrementing attempts and connection is not closing. But if I put break point in ConnectionWatchdog#channelInactive (line 64) and stop there, connections are closing properly. Perhaps there is race condition somewhere?
Possible race-condition in RemoteService.Race-condition during ack checking is possible. For example, ack timeout = 1000 ms:worker receives Request in 999 ms sends ack, but it comes in 1010ms or something like that.client checks ack in 1000 ms and throws RemoteServiceAckTimeoutException.worker invokes method.To solve this problem ackObject was introduced. Worker or client set it to 1 via SETNX command. Worker set it during ack sending. Client during ack receiving.Client check ackObject only if ack timeout has occurred. If client can't set it means that worker have done it already. So client should poll an element from queue again.If worker can't set ackObject then it means that ack timeout already occurred on client side and invocation should skipped.
Possible PublishSubscribe race-condition.There is a rare possibility of race-condition during channel re-connection and RTopic subscription invoked first time.
RScoredSortedSet race condition with Redis cluster.We are running two redis servers in AWS, one master, and a replica. The read mode is the default (slave).In our application code we have something like this..The issue is that since reads go to the slave, addasync returns from master and before the data replicates to the other server it tries to pull the new revrank and returns null. imo the following api would make more sense and avoid that race condition..In the meantime I will either add retry logic or change the read mode to master but that's not going to scale in the long run so it would be nice to have this addressed.Cheers!
Redisson hang on RBatch.execute()Hi, we are using redisson client for our multi-threaded application.We use RBatch to batch two commands as below:During the test, we noticed the hang several time.We first show this error first before the hang:Then our thread calling the batch hung. There is the stacktrack:I tried to set the lower lockWatchdogTimeout and set the batch timeout but this did not help.
RFairLock dead lock issue.Hi,I would like to report a locking issue we are seeing using redisson versions 3.3.2 to 3.5.4, using RFairLock.We have 3 instances of an application competing for a fair lock. After some time, the timeout of the fair lock queue gets set to a time many hours in the future, and no thread ever gets the lock.Here is what redis is reporting during one of the occurences - both commands ran at about 9:30AM PST on 10/17/2017 (1508257800000):You can see that the timeout (zscore of the timeout keys) is set about 6 hours in the future.So far, we haven't been able to reproduce the issue locally, but it is happening daily in our production environment. We are suspecting the problem is happening randomly when one of the application gets killed by the operating system (kill -9), something that can happen quite often due to the way we manage the apps.We were using redisson version 3.2.3 before, and never experienced the issue.Earlier this year, we tried updating to version 3.3.2 and started seeing the problem within a day or so. We reverted to 3.2.3, and waited for an updated version.Two days ago, we decided to upgrade to 3.5.4 assuming the problem was fixed, but it happened again within a few hours.I will keep on trying to reproduce the issue, but any help is appreciated.Thanks,Michael.
JVM crashes when using 1000 concurrent redisson threads.I am scheduling tasks on a Redisson executor. On every schedule a massive chunk of tasks are submitted on two Redisson executors. I have equal number of workers assigned to each of the two executors 500 and a maximum redisson thread count of 1000.I took periodic heap dumps of the JVM and the following was the last dump I captured before the container (my code is running in a docker container) went OOM. It appears to me that the netty's PoolThreadCache continues to increase.Here is a screenshot of the leak suspect report of the last dump I took:I am unable to get the crash dump report of the JVM. Will attach that as soon as I am able to figure out the issue with crash dumps not getting generated.I am using Redisson 3.5.5. The bundled Netty version is 4.1.16.
ReadLock not working correctly.Hi,Is this the expected behaviour of the ReadLock from ReadWriteLock?Clarifying because I did not encounter this in release 3.5.4.There are multiple threads that are trying to lock and unlock the readlock concurrently but I am encountering the "attempt to unlock lock, not locked by current thread by node id" exception.Here is a rough code sample to replicate the issue.The unlock part throws the following exception even though they're all read locks.
org.redisson.RedissonTopic.removeAllListeners got blocked on CountDownLatch.await.Hi there,We hit an issue in one of our server using Redission as Redis Client. We do have a single thread executor handling subscribe/unsubscribe for multiple channels. But last week, we found this got blocked on one server.The org.redisson.RedissonTopic.removeAllListeners is unable to respond request, and blocked in the acquireUninterruptibly FOREVER. I tried to dig into logs, there was one exception thrown before this in CommandAsyncService.syncSubscription line 125 "Subscribe timeout 9500ms". This is possibly related since this was the only "Subscribe timeout" message I saw in last 30 days logs and then this issue happened. But I still don't find prove on this. Looks like this is a rare case since we just hit once. I tried to reproduce locally but with no luck.I think the removeAllListeners should give an option to let caller pass in a timeout. But this is also not going to resolve the root cause. I am wondering if other people have observed this issue before. Please advise if you have any ideas on this, thanks.
RedissonRedLock.tryLock(long waitTime, long leaseTime, TimeUnit unit) still have something wrong.reference from the issue #1175 after it is closed.I change the test case in RedissonRedLockTest.testLockSuccess() like this:The test case above will fail in case of that lock1/lock2 is locked first in thread t1. However, it will pass in case of lock3 is locked first in thread t1.The  is as follows:No matter which lock is locked first in thread t1, the Red Lock will always can be locked or cannot.In this case, the Red Lock should always be locked for most of the lock is available.
[2.11.1] Unable to re-establish MasterConnectionPool after restarting redis.We are Trying to add a mechanism to re-subscribe to topics after restarting redis.Setting UP:Steps to recreate:See Following Error, Redis MasterConnectionPool is not re-established.We were initially at Redisson version 2.9.1, but when we following the previous steps, in step 5, we encountered the same issue as in #1268, which is removeListener thread blocks forever, so make the switch to 2.11.1.We also simulate the scenario with 2.9.1 version,  without doing removeListener (we found out there is pubsub Listener leak issue with 2.9.1, that is why we remove the old listener first), the MasterConnectionPool was able to re-establish.
ReadWriteLock timeout does not work as expected.I wrote a simple test to check the ReadWriteLock.The behaviour.First case: Read thread finishes executing properly, write thread hangs forever.Second case: Behaves as expected. Read thread locks, sleeps and unlocks. Write thread hangs until the read thread finishes and then locks and finishes.What I see in redis in both cases at the beginning:What I see in redis in both cases after some time:The rwlock_timeout entries expire in both cases, it seems like no one is updating their timeout in redis.In the first case, where the readThread finished running but the lock1 entry never expires and is stuck forever like this:What I expected.In the first case when the read thread finished running I'd expect the read locks to expire after some time and unlock the lock1 entry.In the second case I'd expect the rwlock_timeout entries not to expire since the read thread is alive, it sleeps but it is alive.
CommandAsyncService gets blocked at high concurrency [without OutOfMemoryError].Hi,While running a load test on jmeter of 200 users with ramp up time of 10 seconds and number of puts per user equal to 10, threads are getting  stuck. In this test we are using RTransaction and doing load testing on a single key by putting random values for a single key.here is the stack trace:Steps to reproduce or test case.
tryLock waitTime no valid.hello, I have test the tryLock with waitTime, but find it has bug. My test is as follows:As you can see, the second thread faild to acquire lock but the lock is released during 2 seconds...please tell me ,thanks.
severe bug !! when sending read operation to new slave loading previous data after failover.Steps to reproduce or test case.on each servlet request, i send read command like below to redis, in this case, it is problem.result = redissonClient.getScript().eval(Mode.READ_ONLY, script, RScript.ReturnType.VALUE), keyList, argvList.toArray());on each servlet request, i send read or write command like below to redis,  in this case, it is ok.result = redissonClient.getScript().eval(Mode.READ_WRITE, script, RScript.ReturnType.VALUE), keyList, argvList.toArray());send read operation each reqeuest on servlet server continusly.slave is loading data for syncing with master. it takes 10 minutes to load all data completely.during 10 minutes, servlet server is going strange behavior and throuput is very lower and don't return any exception. i think some threads is hanging around Redisson connection with new slave loading data.actually i expected returning error with timeout. why it is not returning any error until slave is completed loading data.after 10 minutes which completed load data from slave. it is going back to normal and throuput become higer.however, read operation with Mode.ReadOnly is going both master and slave not going to slave only.when loading data at slave, redisson is sending read operation to slave. and read opeartion thread is hanging aroud somewhere in redisson.read operation with Mode.ReadOnly is going well to slave only. however after failover, read operation is going both master and slave not going to slave only.
Lock watchdog does not monitor read locks.Hi, when we acquire multiple (concurrent) read locks and process a task for longer than the Config.lockWatchdogTimeout, we get errors when unlocking.Code to reproduce (wait the watchdog timeout):int threadCount = 100;Looking into Redis for the ttl of different keys, I noticed:The key 'mytestlock' gets an extended ttl.The key '{mytestlock}:UUID:THREADID:rwlock_timeout:1 is not set with new ttl and the key disappears before the unlock is called.We tested with Redisson 3.5.4 and 3.7.5.
unlock fail and expirationRenewal still execute.unlock success.unlock  fail.Steps to reproduce or test case.unlock ()client -> redis service network problem.unlock throw Exception.network recovery.expirationRenewal still execute.
Deadlock with RedissonLock used by JCache.We've ran into a number of issues with using the Redisson implementation of JCache via bucket4j in production for rate limiting.JCache.put(K key, V value) calls RedissonLock.lock() with no lease time, resulting in RedissonLock's creating a 'watchdog' thread that continuously renews the lease on the lock in Redis. We are using the default 'lockWatchdogTimeout' of 30 seconds so this thread runs every 10 seconds. What we've seen is in rare cases this watchdog thread never gets canceled, meaning it continues renewing the lease indefinitely until the instance containing the thread is restarted. This causes deadlock of all other threads trying to grab the lock and can ultimately bring down an application as threads build up.Threads using RedissonLock will wait forever for a lock to be released. There is no timeout when waiting for the lock.The 'unlock' method in RedissonLock can fail for a number of reasons -- e.g., timing out trying to unlock with Redis after 3 seconds (this happened in our case). When the unlock fails the watchdog thread is not canceled. This means the instance would have to be restarted to release the lock.A 'watchdog' thread should not exist. This is prone to issues where it never gets destroyed and a lock is held forever. A lease time should be used to ensure the lock is never held forever and somehow it should be verified that the lock is still held by the caller when making updates with it (an atomic check at update time) in case the first thread doesn't execute its update within the lease time and another thread grabs the lock.Threads should not wait forever for the lock. There should be some configurable timeout.A timeout on 'unlock' should be retried, but the lease time in 1 should handle any failure to unlock. If for some reason the 'watchdog' thread is kept, the unlock needs to ensure it is canceled if it fails.
Deadlock while obtaining lock.Thread gets stuck while obtaining lock.The same lock name was likely concurrently obtained and held by another thread possibly on another jvm, and then released.Observed intermittently.
two thread get the same lock by trylock?Steps to reproduce or test case.
RedisOutOfMemoryException.No errors should be thrown after setting maxmemory-policy :volatile-lru, maxmemory:4MB.When I am using two threads to put values through redisson to different database with limited memory(maxmemory-policy :volatile-lru, maxmemory:4MB),  error like org.redisson.client.RedisOutOfMemoryException: command not allowed when used memory > 'maxmemory' sprang up, and the database was flushed by redisson.But I replaced to use Jedis(2.8.0) to test again, no such exceptions came out, and the database was not flushed by Jedis. The items which exceed 4MB  can be deleted by redis automatically because of volatile-lru policy.Steps to reproduce or test case.Run commands on redis-cli, config set maxmemory-policy volatile-lru  config set maxmemory 4mb.Run the code twice(Two threads) using different database with type to 0.singleServer, refer to the codes for details.Codes I run.
Deadlock after Redis timeout.After a Redis response timeout, Redisson needs to be resilient and back to normal operation after Redis timeout and allow to lock and unlock critical sections again.After a Redis response timeout between attempts, Redisson adds the lock to scheduled renew task and never remove them. So, the application enters on a deadlock due to a lock renewed even after unlock.The task can run on the same thread due to reentrant lock feature, but when the task runs on another thread a deadlock occurs.Steps to reproduce or test case.Start Redis.Run the test application (https) with a simple locked task.Force a timeout on Redis runnning a client pause command: "CLIENT PAUSE 5000".
Cannot shutdown RedissonNode if there infinite lock is locked.To reproduce the problem:1 Create a Runnable job which will last ~ 1 minutes (greater than "internalLockLeaseTime" used by RedissonLock.java), get a lock and lock it by lock.tryLock(), release the lock after the job is done.2 Schedule and run this job using Redission framework.This is because RedissonLock.scheduleExpirationRenewal still running after we shutdown the node.Can Redisson stop the scheduleExpirationRenewal when user call node.shutdown()?
Unexpected exception while processing command.I got this problem when I trying to locking an object. Here is the problem:Date: Thu, 19 Oct 2017 10:15:31 GMT.what I'm missing?
The deadlock of RedissonMultiLock.When I used the RedissonMultiLock.lock(), I found the code of Redisson.This is my code.The program execution sequence is as follows:Thread A locked the 'lockSoundbox' of lockA. It's OK?Thread B locked the 'lock2000' of lockB. It's OK?Thread A locked the 'lock2000' of lockA. It's OK?Thread B locked the 'lockSoundbox' of lockB. It's OK?I think there's a deadlock between 3 and 4, but I'm not going to try it out.
Redisson 3.5.7 memory leak with locks.I'm using RLock and seeing memory leak via org.redisson.client.handler.CommandPubSubDecoder.Retained objects delta between heap dumps taken apart.Around 10MB leak per day.
RedissonRateLimiter acquire() method blocks forever.The acquire method should block until a permit is available, but it blocks indefinitely instead of the remaining time until a new permit becomes available. From my testing I have found that it doesn't acquire a permit when one should be available based on the rate and interval I tested. If I set the rate to 1 every 5 seconds, it should take a loop of 10 single permit acquires approximately 50 seconds to complete. Instead it acquires the first permit, then blocks forever on the next acquires call.
Multiple ReadLocks are not sharing locks properly after WriteLock releases.Hello!I've found something that I believe is a bug in Redisson's RReadWriteLock implementation where multiple ReadLocks seemed to become or at least behaved like WriteLock when it tried to lock on a lockpoint that another WriteLock has already acquired a lock, then released. Because of that, ReadLocks are not sharing the lockpoint like it should and are taking turns in locking the lockpoint. Theoretically, this will have a performance impact on applications that expected Redisson to have quicker locking mechanism based on inclusive locking mechanism but under the hood, it is not inclusive and those applications will spend some time to wait for locks to complete.I have tested this with Java's ReentrantReadWriteLock and it has worked exactly what I'd expected it to be.Note: I have only tested this on a single Redis server. I did not test this on clustered Redis servers.Writer Thread locks.Reader Thread 1 fails to lock and waits.Reader Thread 2 fails to lock and waits.Reader Thread 3 fails to lock and waits.Writer Thread unlocks.Read Thread 1 locks.Read Thread 2 locks.Read Thread 3 locks.Read Thread 1 unlocks.Read Thread 2 unlocks.Read Thread 3 unlocks.This behavior matches Java's ReentrantReadWriteLock behavior exactly.Writer Thread locks.Reader Thread 1 fails to lock and waits.Reader Thread 2 fails to lock and waits.Reader Thread 3 fails to lock and waits.Writer Thread unlocks.Read Thread 1 locks.Read Thread 2 fails to lock and waits.Read Thread 3 fails to lock and waits.Read Thread 1 unlocks.Read Thread 2 locks.Read Thread 3 fails to lock and waits.Read Thread 2 unlocks.Read Thread 3 locks.Read Thread 3 unlocks.This behavior does not match Java's ReentrantReadWriteLock behavior exactly.Steps to reproduce or test case.I have written a test code for you to review, download, and test.The link to my code is here:The test code includes two test suites that runs a control test using Java's ReentrantReadWriteLock and an experiment test with Redisson's RReadWriteLock.Each test suites has two test cases where one of the test does multiple ReadLocks locks on the lockpoint before WriteLock locks on it, and another one tests multiple ReadLocks locking on the lockpoint after WriteLock has locked on it. The failure on the latter test on Redisson's RReadWriteLock is what has prompted me to open this issue ticket. Java's ReentrantReadWriteLock passed that test.Overview:To run the test, run mvn test with Maven to test the code.Single server with default configuration created by Redisson's Config class.
RTransaction Unusual Locking Problem.Hi mrniko,We have two methods:As per our test we invoke method 1 and then immediately invoke method 2. The second transaction waits until the first one finishes and seems like the first transaction takes a pessimistic lock on one of the keys of the map being used here and we are totally fine with that.Now we have another method in a different test scenario:As per our previous test we expected a deadlock situation in this scenario, since transaction1 has the lock over the key and transaction2 is trying to alter it. But in our test both transactions commit successfully and maintain order.Please explain the ambigious locking in case of these to scenarios. Thank you.
Redisson continuously send lock request to redis after redis cluster master-slave switch.Evn : jdk 7, redisson version: 2.11.4,redis version:4.0.1,enable redis cluster with 3 masters and 3 slaves.After my redis cluster occurs an master-slave switch and then I try to get a lock, but redisson starts getting out of control, it continuously send Lua script command to get lock, while redis cluster return a MOVED error because of master-slave switch early, it seems that the redisson didn't know the master-slave switch and continuously send Lua script command to the same server to get lock. Approximately redisson sends 150000 lock request to redis in 5 seconds with merely more than a dozen application lock request. The network traffic between the redisson server and redis server rise up to 150Mbps, then it becomes normal after restart the redisson server.The getting lock code general show below,(I just receive 10+ lock request):The network packages general show as below:
redisson keep sending lock request to redis due to redis cluster master-slave failover.i use redisson redLock in my project, i think redLock should be work in any case, below is my code,but when redis cluster code master-slave failover, redisson failed to acquire lock and keep sending acquire lock request, below is request:then send same request again and again... cluster response again...
RedissonBlockingQueue.take() returns null in case of internal error.The take() method in RedissonBlockingQueue uses Future.getNow(), which returns null if an exception occurs. In our case, we have tons of NPEs when the application is stopped.
RedissonRedLock issue.
ReadWriteLock not working.Hello.I'm interested in this project. I am creating a sample project(https) using this,And I do not think readswriteLock is working properly.The error pattern is a bit different, but the example org.redission.example.ReadWriteLockExamples.java is not working properly. Version 2.10.5 does not work either.I do not know much about redis.I hope this project can be maintained.
Caused by: org.redisson.client.RedisException: ERR max number of clients reached.When I use Reentrant Lock in my application, this exception  ERR max number of clients reached occurs.Redisson version is 2.10.7, and redis version is 3.2.
Warn:"Invocation of destroy method 'close' failed on bean with name 'jCacheCacheManager'".I imported Redission in spring boot using distributedLock,but the log always show this WARN.
RedLock slow performance when one redis instance is down.According to RedLock Algorithm, Step 2 RedissonRedLock#tryLock should fail fast if redis instance is unavailable:During step 2, when setting the lock in each instance, the client uses a timeout which is small compared to the total lock auto-release time in order to acquire it. For example if the auto-release time is 10 seconds, the timeout could be in the ~ 5-50 milliseconds range. This prevents the client from remaining blocked for a long time trying to talk with a Redis node which is down: if an instance is not available, we should try to talk with the next instance ASAP.RedLock#tryLock blocks for duration equal to config.retryInterval * config.retryAttempts on each tryLock attempt.Such behavior makes business code not able to consume messages at requried rate, which in turn leads to other negative consequences.Setting config.retryAttempts=1 and config.retryInterval=50 does not work, since initial commands are not fast enough and connection establishment fails.Other timeouts doesn't seem to have relation to this issue.
CommandAsyncService blocks indefinitely [without OutOfMemoryError].Similarly to #889.throw exception.Thread indefinitely park.Using RMapCache and configure maploader to DB(oracle).Shutdown DB,3.RMapCache get operation will be blocked.
RRateLimiter.tryAcquire with timeout blocks forever.If it's not possible to acquire a permission before timeout elapses, tryAcquire should return a boolean false.Even if the timeout elapses, the method tryAcquire keeps blocked.Steps to reproduce or test case.
Unexpected exception while processing command while taking a lock on a key.Note:Same scenario works as expected when there is no exception or error inside the try block.
RedisResponseTimeoutException at getLockedLock for read operation in "replicatedServersConfig".Hi,I'm using jcache  implementation and using AWS Redis servers in "replicatedServersConfig", i'm getting response time out exception at "getLockedLock" method. raised another issue  related to jcache.Please do the needful.Thanks,
Race condition in RedissonLock subscribing to pubsub channel.We are currently using redisson for, among other things, distributed locking. In our use case, we can have hundreds of unique fair locks being acquired many times, usually for short periods of time (~1-2 seconds).During testing, we noticed that sustained load eventually, but consistently, results in threads failing to acquire the lock. To try to pinpoint the problem, we added extra logging in various places in code path used by RedissonLock. What we found was that it sometimes took a long time for the lock to subscribe to its pubsub channel, by which point the unlock message was already published. This results in the thread waiting on entry.getLatch().tryAcquire(ttl, TimeUnit.MILLISECONDS); until failure.In other words, the sequence of events seems to be:thread 1 acquires a lock.thread 2 tries and fails to acquire the lock, initiating pubsub subscription.thread 1 releases lock (semaphore is not released since nothing is waiting on it yet!)thread 2 completes subscription connection and waits on semaphore, eventually timing out.There seem to be two places where this delay happens:Acquiring an AsyncSemaphore in  the PublishSubscribe::subscribe method.The 'await' call in RedissonLock::tryLock.Are we missing something here? It seems strange that it can take upwards of several seconds for the lock to subscribe to its pubsub channel.
redlock unlock fail.
tryLockAsync and lockAsync methods of RedissonMultiLock object get hang.No description provided.
RedissonAtomicLong.get blocks indefinitely.Stack info:Steps to reproduce or test case.Small probability and we can't reproduce, maybe related to bad network.Redisson configuration.cluster default config.
PriorityBlockingQueue doesn't release lock after reconnect on network.reconnect to network.there is a lock  name redisson_sortedset_lock:{testqueue} which dosen't release,it seem's like it can't release lock cause network boke,however it get a lock before network boke.Steps to reproduce or test case.1.run code blow.2.disconnect your network.3.wait a few minute,then reconnect your network.
spring boot started incorrectly, but unlocking can work properly.
Concurrent calls to RemoteService should not result in an exception.Concurrent calls to RemoteService should not result in an exception.Concurrent calls to RemoteService should result in an exception.Steps to reproduce or test case.Redis version.4.0.9.Redisson version.3.6.5.Redisson configuration.
Mitigating Concurrency Issues.Hi,I was wondering what the standard approach is to Mitigate possible Concurrency issues when using Redisson as a spring data source.I am working on an application that gets information from a source application and then layers updated over the top before delivering the updated information to the client.The updates in this case are built for several sources, each one has its own queue.When a new message is placed on any given queue my app reads the message from the queue, and attempts to create or update an updateObject in my redis cache.My problem stems form the fact that messages are constantly being placed on these queues and updates to the updateObject are likely to come in quick succession.Im concerned that this will lead to concurrent read write issues:I have attempted to solve this using @transactional and the RedissonTransactionManager but it doesn't seem to be halting the second messages update while the first message is still updating.Any advice would be really appreciated,Thank you.
[CRITICAL] Redisson cluster sync failed.In cluster mode when master goes down and slave becomes a master, Redisson picks up new master but failes to sync slaves. Cluster nodes synchronisation crashes (never scheduled again) with IllegalArgumentException while processing slave nodes.Redis cluster setup:Relevant stacktrace:Code path which fails with IllegalArgumentException (please note it's 3.0.0 branch):scheduleClusterChangeCheck(cfg, null); is never executed thus Redisson stops sync cluster state. Any subsequent cluster change leads to a service outage.
redission AtomicObject in debug + breakpoint mode cause a different result.version:3.5.4.At first,I delete the key,confirm the key not exist.
Possible race-condition during write operation cancellation.No description provided.
RLock.unlock method returns true if lock doesn't exist.No description provided.
RedissonLock.renewExpiration will refresh forever.If the lock key no longer exist the refresh should cease.The renewExpiration() method ignores the return value of renewExpirationAsync() and reschedules itself forever.Steps to reproduce or test case.Should be as simple as manually removing a lock entry key from redis.On a side note, another thing we noticed while looking into this is that renewExpirationAsync() does a scripted call that returns 0 or 1 depending on HEXISTS. It seems unnecessary as according to the redis docs about PEXPIRE, it does the same thing out of the box in a single command call.
RedissonSemaphore doesn't take linux machine's down into account, like RedissonLock do.when machine is down, RedissonSemaphore should release it never released.Steps to reproduce or test case.
Exception when use RedissonRedLock + RedissonMultiLock.I'm trying to introduce RedLock algorithm in my project, I have 3 Redis nodes, and I need to lock several keys in each thread. Here is my implementation:no exception.Got exception with message: ""org.redisson.RedissonMultiLock cannot be cast to org.redisson.RedissonLock"".Steps to reproduce or test case.the root cause is this code in method tryLock(long waitTime, long leaseTime, TimeUnit unit) of class RedissonMultiLock.java:It trys to cast rLock to RedissonLock, but in my case the rLock is RedissonMultiLock. By this implementation, RedissonMultiLock actually can not group any RLock types but RedissonLock.
RedissonFairLock timeout drift.We've discovered additional conditions that produce timeout drift as originally described in #1104. To summarize the issue, we see that the timeouts in the redisson_lock_timeout sorted set created by RedissonFairLock can gradually increase over time to be hours or days in the future despite the queue only containing less than 10 threads; this condition continues until the fair wait queue empties, or one of the threads dies and the rest of the threads are forced to wait for the far in the future timeout on the dead thread to lapse (this creates a dead lock-like situation).I'll provide a PR to follow shortly with test cases and proposed changes to fix the issue. We greatly appreciate feed back on any misunderstanding of the issue described here and will do the same for feedback on the PR.The expected behavior is that which is documented in the second paragraph of 8.2 Fair Lock:All waiting threads are queued and if some thread has died then Redisson waits its return for 5 seconds. For example, if 5 threads are died for some reason then delay will be 25 seconds.As an example that replicates this documentation, given we have 7 threads trying to take one lock, we expect the redis data structures to contain roughly the following after a 7 threads try to acquire the lock once using RLock.tryLock(threadId) in order with short delays between:1) lock name of the 1st thread.2) "1".3) lock name of the 2nd thread.4) lock name of the 3rd thread.5) lock name of the 4th thread.6) lock name of the 5th thread.7) lock name of the 6th thread.8) lock name of the 7th thread.9) lock name of the 2nd thread.10) lock timeout + 5s.11) lock name of the 3rd thread.12) lock timeout + 10s.13) lock name of the 4th thread.14) lock timeout + 15s.15) lock name of the 5th thread.16) lock timeout + 20s.17) lock name of the 6th thread.18) lock timeout + 25s.19) lock name of the 7th thread.20) lock timeout + 30s.In the above we see that the 7th thread will need to wait 25s after the lock expiration (based on the lease time) until it can acquire the lock if the 1st to 6th threads die.Additionally, we expect that when a thread leaves the queue voluntarily either by wait timeout or by acquiring the lock, that timeouts adjust regardless of where the thread was in the queue. For example, if the 3rd thread in the above example left the queue via a RedissonFairLock.acquireFailedAsync call due to wait timeout, the sorted set would then be expected to be:1) lock name of the 2nd thread.2) lock timeout + 5s.3) lock name of the 4th thread.4) lock timeout + 10s.5) lock name of the 5th thread.6) lock timeout + 15s.7) lock name of the 6th thread.8)  lock timeout + 20s.9)  lock name of the 7th thread.10) lock timeout + 25s.And the 7th thread would then only need to wait 20s after the lock expiration to acquire the lock if the other threads died.In one of our usages of redisson we use the fair lock using 3-6 servers using each a single instance of Redisson running roughly the following code in one thread per server:We observe that since the work done within the locked portion of the code can take only ~100ms that the 6 server threads running this code quickly churn through the lock queue without issue, though a queue is always present since there is little time between when each thread unlocks and locks again. If we take periodic snapshots of the redisson_lock_timeout sorted set, we tend to see that the timeouts increase in increments of 5s over time until the timeouts are hours or days in the future. If one of the servers is killed, then we observe that the other servers stop doing work and are timing out trying to get the lock due to the dead server's lock holding the first position in redisson_lock_queue with a timeout in redisson_lock_timeout that may be hours or days in the future. We expect that under the case that we lose one server of 6, that the timeout values will be in the range of 10 to 35s in the future at any one time (5s lease time + 5s thread wait time * position in queue). It may be said that the above lock usage itself may be foolish, but its mostly working except for the chance of deadlock.Steps to reproduce or test case.I'll provide a PR for this issue to follow with additional test cases added to RedissonFairLockTest. However, what we started with was a modification of the testTimeoutDrift where we changed the wait time from 500ms to 3s and changed the lock holding time from 30s to 100ms (see Thread.sleep(30000)); with this test case, instead of the tryLock failing due to wait timeout, the threads are able to lock and unlock the lock quickly. The new version of the test, the test fails with a timeout drift into the futre, in a similar way that the test failed in #1104.The PR will contain other test cases with the hope that we cover all code changes and produce the expected behavior described above.We run a single redis server for use exclusively by redisson.
`redissonClient.getRedLock` does not work well with Read-Write block in a multi-JVM environment.Then call lock.tryLock() first, return true.Then call lock.tryLock() after the previous call has been made, now it should return false because there is a readLock under the same name.Then call lock.tryLock() first return true as expected.Then call lock.tryLock() after the previous one, now it return true i.e. have both write lock and read lock on the same ReadWriteLock.Steps to reproduce or test case.As described above.In case if this is useful information, if replace getRedLock with getMultiLock it actually works. Also if having only one JVM it also works.
Why do locks fail occasionally?Here's how I use it:But there are still high concurrency issues.Two of the same information is generated.Request to solve.thank.
if waitTime == -1 and leaseTime != -1,multiple threads get the lock at the same time.if waitTime == -1  and leaseTime != -1,multiple threads get the lock at the same time.Thank you.
@Transactional and RLock.unlock()As can be seen from the above, transaction committing occurs after the unlocking operation. Is there a possibility that other threads will have concurrent problems because the thread has been unlocked before the transaction has been submitted?I tested it with JMeter and found no concurrency problems between unlocking and committing transactions.If you know, please let me know. Thank you.
RLock unlock with a different thread that the one that locked gives: attempt to unlock lock, not locked by current thread by node id.I'm using RxJava and as a part of the sequence I use a RLock, at some point (in another process) I unlock it and if the thread to unlock is not the same as the one that blocked I get an exception (see below).I see that the lock has been done using the pool of redisson-netty, and the unlock is done by the same pool, but there are several threads in that pool, so any thread can potentially perform the unlock.That should be as simple as tryLock and unlock without having to control anything else right? Do we have to do something specific to ensure the proper behaviour?The lock gets unlocked regardless the thread that did the lock.An exception is thrown and the lock is not unlocked.
redission ScheduledFuture can not be execute after definite time.complete the ScheduledFuture after ten seconds.the schedule task above can not be executed after 10 seconds, the current thread beign blocked.Steps to reproduce or test case.using spring boot org.redisson.spring.starter.RedissonAutoConfiguration, just configure redis host and port in properties file.
when I unlock the lock, accept an exception.somebody can help me ?
RedissonFairLock deadlock.Seems related to the change on #2099.We have tried to upgrade to 3.11.2 from 3.10.7 and this behavior started occurring.The FairLock acquired on high load system without lease time defined.Redisson does not handle the case of Fair lock without lease time (on high load).when many different threads are asking for the same lock key the client is stuck and no thread acquires the lock.Steps to reproduce or test case.
LockWatchdogTimeout will cause the IllegalMonitorStateException.redis version: 3.0.504.redisson version:3.11.1.setLockWatchdogTimeout will cause the IllegalMonitorStateException.
threads blocked waiting on CountDownLatch.Hello,We are facing an an issue where some threads get blocked indefinitely waiting on the CountDownLatch in CommandAsyncService.get(). Relevant section of the thread dump:The reason for why the requests aren't able to complete isn't relevant here (we are deploying redis in kubernetes, and are working through various upgrade/failure scenarios that currently can result in severed connections). However, it's strange to me that the CommandAsyncService.get() method doesn't respect timeout configuration, and simply calls l.await().Is there any way around this? Please let me know if I am misunderstanding the situation.CommandAsyncService.get() should throw an exception if the future is unable to complete within a timeout.Many threads are hanging indefinitely in the CommandAsyncService.get() method.
thread blocked at closeNodeConnections.Hi,The call to closeNodeConnection waits indefinitely on call to MasterSlaveConnectionManager.closeNodeConnections.Section from thread dump:MasterSlaveConnectionManager.closeNodeConnections should throw an timeout exception if the same is not able to complete within a configured timeout.Thread hangs indefinitely at the request.
when aquire redisson lock , that throw redis response timeout sometimes.redisson.2.2.13.when occouring this problem,the redis has not much qps.
RBlockingQueue.take doesn't throw InterruptedException.No description provided.
RedissonLock.unlock() hangs infinitely waiting for RedissonPromise.Calls to RedissonLock.unlock() are expected to finish after the lock has been deleted from Redis.We have experienced several threads locked forever waiting inside calls to RedissonLock.unlock() in our production system within a few hours after upgrading to Redisson 3.11.4 from Redisson 3.8.0.Steps to reproduce or test case.Unfortunately I am unable to reproduce the problem on my own computer. We have however experienced it on several of our production servers within just a few hours after starting our newer version of the application with updated Redisson version. We upgraded from 3.8.0 in order to receive fixes for bug #1966 - Deadlock after Redis timeout, which we also experienced a couple of times. However right after start we have found some (29) exceptions in our logs coming from internal redisson threads:After that we were investigating issues with blocked threads by analyzing thread dumps. All stuck threads had exactly the same stack trace as below:After examining the source code of RedissonLock.java we have found that the NullPointerException occurs probably due to missing timeout object inside task:I'm not sure if it would solve the root cause why task.getTimeout() returns null, but calls to cancelExpirationRenewal method should not fail with an exception, otherwise, e.g. in RedissonLock.unlockAsync method the future will never receive a result (or a failure):Please add null check for timeout inside the ExpirationEntry object.We had to return to 3.8.0 version because of this problem. I think it is quire severe.Default configuration with sentinel servers.
Interrupted blocking methods aren't canceled.No description provided.
Reactive/Rxjava2 transaction doesn't unlock lock on rollback or commit.No description provided.
Lock expiration renewal stops working after connection error/timeout.After a connection problem during expiration renewal, if a new lock is created (with the same key), the expiration renewal should be working.If an error ocurred in the expiration renewal (ex: connection to redis timeout), it stops working for new locks.Steps to reproduce or test case.Create a Redisson lock.Force a expiration renewal timeout (ex: restarting redis-server)unlock (it will throw a exception, as expected)Create a new lock with the same key.wait for the expiration renewal time.unlock (it will throw a exception, because the expiration renewal is not working)Default, SingleServer, (LockWatchdogTimeout reduced just for testing)I believe the problem is in RedissonLock.unlockAsync method, when opStatus is null, the cancelExpirationRenewal method should be called.
RLock.unlock() results in ERR hash value is not an integer.The following code snippet will successfully create and lock RLock and will throw RedisException unlocking the lock.
Unable to remove lock automatically.The code is as follows, the lock cannot be automatically deleted, the lock will continue to live.
Unable to unlock  when thread isInterrupted.Unable to unlock when thread isInterrupted,Please,RUN it First!!!
RedissonLock fails to unsubscribe from channel when a lock is acquired.Expected behavior.The pub/sub channels that are created while trying to acquire a lock with RedissonLock should be unsubscribed when no longer needed.Actual behavior.Some pub/sub channels are still present (meaning that the client did not unsubscribe) even after some locks are no longer used (either they expired or were unlocked). These channels are never cleaned up (still present after 24h since created). The only workaround to get rid of these is to restart the client.This was observed because it caused a connection leak. When calling RedissonLock to obtain new locks this returned the error : Subscribe timeout: (7500ms). Increase 'subscriptionsPerConnection' and/or 'subscriptionConnectionPoolSize' parameters. We have increased the limits in order to have a workaround for this bug.Steps to reproduce or test case.use RedissonLock.java implementation with method lockInterruptibly(long leaseTime, TimeUnit unit) to obtain multiple locks simultaneously.use lower limits for  subscription-connection-pool-size and  subscriptions-per-connection to increase the probability to see the bug.wait until this error shows up Subscribe timeout: (7500ms). Increase 'subscriptionsPerConnection' and/or 'subscriptionConnectionPoolSize' parameters.connect to Redis server and fetch all lock keys and all pub-sub channels.if number of pub-sub channels > active locks , then the bug was reproduced.if number of pub-sub channels does not decrease at all long after locks are being created, this means that subscriptions have leaked, active long after these are no longer used in RedissonLock implementation.
tryLock(),Machines are not evenly distributed, and one server never gets the lock.Machines are not evenly distributed, and one server never gets the lock.It's going to last forever.
In a frequent job , sometime lock.tryLock() can not get lock.In a spring boot 1.5.6 application , We have a new job which is running every 30 seconds and try to acquire lock to do something . In recent days the job has tried to acquire one specific lock cause of business , We found from 00:13 on Feb 24  the job can not get the lock but after 15:03 the job can get the lock again . The operation on this lock only in this job .the code on the lock as below :Thanks.
Distlock Mutual exclusion problem?1、assume a Redis cluster has a 5 master node; A , B, C ,D ,E representative node name.2、Client A  lock to A,B,C three node. lock success.3、This time B,C Node happen Network partition.4、Client B lock to D,E tow node, lock success,Satisfy N/2 + 1 ,because have A,D,E three node.5、Violation of mutual exclusion.
RedissonBoundedBlockingQueue drainTo method got infinite hung.Expected behavior.RedissonBoundedBlockingQueue drainTo method will return result.Actual behavior.RedissonBoundedBlockingQueue drainTo method got infinite hung.
RLock throws java.lang.IllegalMonitorStateException in the thread where got the lock twice.In a spring boot 1.5.6 application, We have below operations on merchant account ,In PayServiceImpl class , using spring proxy (IOC) to call payHandlerService.balanceFreeze.It seems (B) throws the exception as below.Thanks.
RedissonLock.tryLock() interrupted and keep renew lock.Expected behavior.when tryLock is interrupted, watchdog renew listener is cancelled too.Actual behavior.when tryLock is interrupted, watchdog keeps renewing lock, this makes ifinite lock.Steps to reproduce or test case.
RedissonReactiveClient getLock and getLexSortedSet is executing on different thread.So here lockk is printed as true, but redissonReactiveClient.getLexSortedSet(redisKey) cannot execute and directly goes to onErrorResume as it is executing in different thread than lock.tryLock(). I'm new to this reactive style of programming, I don't know what I'm doing wrong maybe there is a better way to lock and get the sortedset? please help.
`tryLock` in redis cluster working not properly.Expected behavior.tryLock of RLock can not return true when the lock key still exist in cluster redis.Actual behavior.Sometimes tryLock can acquire the lock key although it still exists. Moreover, the TTL of the lock key also is reset.Steps to reproduce or test case.
Thread that called tryLock stops in standby state.Hello,Running the redisServer again does not resolve the issue.In particular, Scheduler threads cause serious problems.Is there a solution?Below is the test code.This result is Thread dump.
Is tryLock() method expected to return false when redis is not available.Expected behavior.tryLock() method return false after certain time when redis is not available.Actual behavior.tryLock() method is hanging forever.Steps to reproduce or test case.when application starts, the redis server is there, then bring down the redis.local redis docker without authentication.
spring-integration unlock error when use redisson.use redisson and spring-integration.Steps to reproduce or test case.this keyCommands() retuen this OBJECT occur recursion.Maybe,we need override method unlink in RedissonConnection.
redisson lock uncontrolled release.Hi all,I am facing strange problems with Redisson lock.I use the lock without expiration - which means is that watchdog mechanism in-charge of updating the lease time of the lock in redis.Threads that are using the lock, lock it for time ranging from couple of seconds to 300 seconds or even more.The problem:sometimes when a thread(that acquired it) tries to release the lock the thread receives IllegalMonitorStateException , which means that this lock is already released.One solution was to increase the watchdog timeout - I increased it to 100 seconds and it decreased the number of errors but they still occur from time to time...I don't want to increase the timeout even more because this doesn't address the root problem.How can I solve this? is anyone else encountered this behaviour?
Using lockAsync with Scala Akka.Hi,Does lockAsync / unlockAsync and bucket async APIs work with Akka actor using Scala/Play?I am getting the exceptin below:java.lang.IllegalMonitorStateException: attempt to unlock lock, not locked by current thread by node id.Is that because of unlockAsync being triggered inside Future of lockAsync? I am using Scala converters to get Scala Futures from these APIs.If I generate my own thread ids using random long then it does work.Thanks.Rakesh.
redisson 3.13.1 Always connected to redis cluster and can not to acquire lock,but no exception.redis version:5.0.8.redisson version: 3.13.1.i have built redis cluster with six instance.and use cluster nodes to view clusters info.
redisson,The current thread does not release the lock, will hava some problems.yesterday,i test the redisson lock,i use  Apache JMeter to test the concurrency.param: thread number = 1000,  loop = 1.i found when i not release the lock .but other thread can get the lock.example:
Don't release the lock actively, there is a problem in high concurrency.yesterday,i test the redisson lock,i use  Apache JMeter to test the concurrency.param: thread number = 1000,  loop = 1.i found when i not release the lock .but other thread can get the lock.example:
RedissonFairLock deadlock.RedissonFairLock result.Only more than 100 "endLok" were printed.if use RedissonLock,it can be print all.
possible concurrent issue in RedissonLock?When running multiple threads to acquire the lock with the same name, the below error happened from time to time.Not sure if we need any concurrency control in the ExpirationEntry of RedissonLock? e.g. by simply converting the threadIds to a synchronized map, the issue got fixed in our particular case.
Thread gets stuck when trying to acquire lock via tryLock()Expected behavior.The thread should get release i.e tryLock should return false.Actual behavior.Thread gets stuck in Waiting state.Steps to reproduce or test case.This is how we are calling the tryLock and multiple threads are scheduled to call this.Some threads gets stuck at the Waiting state at tryLock.This is observed when the shards are being added (Sharding is In Progress state).  After sharding is done then Restarting the application releases the threads.
FairLock is freezed after run more than 9000 times.this is my code.the log will not print after a few minute.
Non thread-safe initialization in RedissonObjectBuilder.Spring bean created successfully.Steps to reproduce or test case.We have several spring beans that initialise in prallel.
Redisson 3.10.7 & FST 2.57 - Data is not cleared from ThraedLocal while using Serialization of FST library.When we are saving some data in redis using FST codec then data is not cleared from ThreadLocal after data saved to the redis. This is causing issue of OutOfMemory as tomcat thread's ThreadLocalMap is filled with huge data which was used while FST serialization.Detail:Main code requested for putting some data in Redis.Redisson uses FST serialization to serialize data and then stores into Redis.FSTObjectOutput which was used while serialization has stored data in ThreadLocal object and it has not cleared data from ThreadLocal after it was saved in the Redis.I can see ThreadLocalMap contains all the data which was used while FST serialization. Please see below snapshot of heap dump:Ideally it should clear the data from ThreadLocal, once the data stored in Redis.
On Reading Locks.Multithread concurrency can be unlocked successfully.Multithread concurrency with only one thread successfully unlocked.Are there any problems with the script here?When a thread finds the corresponding key, it deletes the whole key. Is it impossible for other threads to find it?Steps to reproduce or test case.
RLock unlock always throw an exception what like 'not lock by current thread'.trylock and unlock success.RLock unlock always throw an exception what like 'not lock by current thread'.Steps to reproduce or test case.According to the stack information, the underlying asynchronous unlocking may cause the unlocking to throw an exception.This phenomenon can be reproduced 100%.
lock.isHeldByCurrentThread() verification  before unlocking Exception.RLock.lock.isHeldByCurrentThread() verification  before unlocking.but sometimes exception.
A lot of threads waiting on "org.redisson.spring.data.connection.RedissonConnection.sync".When I was doing the stress test, there were more than 180 threads (like:org.redisson.spring.data.connection.RedissonConnection.write() ,OR org.redisson.spring.data.connection.RedissonConnection.read(),and org.redisson.spring.data.connection.RedissonConnection.del())waiting for "org.redisson.spring.data.connection.RedissonConnection.sync()". What is the reason for this? And how to solve it?Thank you very much!
redisson-netty thread lock.Hi,We have setup Redis cluster and populated key/values. Some Keys contain more than 2M fields. When we call getAll of RMap using redission client, it takes more than 30 mins and goes to idle state. We analyzed the thread dump and noticed the below errors. We are using redission client (3.13.1). Is this known issue with Redisson client?
Spring Data Redis connection in multi mode may cause thread hang.No description provided.
InternalThreadLocalMap memory leak.Expected behavior.Actual behavior.after long running, application gets oom.Steps to reproduce or test case.our app runs about 1 month.Redis version.cloud service, not sure.very big array(1048576 size) in each threadlocal, new array was created in old-gen and always fail.
RedissonAtomicLong#getAndSet throws NullPointerException when value does not exist yet.Expected behavior.Either no Exception (just create the value, getAndSet shoudl be able to create the value as well) or documented behaviour.Actual behavior.Null Pointer Exception, but only during first execution. Reproducible if I manually delete the value in Redis again.Steps to reproduce or test case.When the value does not exist yet, counterInDb is not null but calling getAndSet on it without checking isExists as well in the if gives me a NullPointerException at RedissonAtomicLong.java:122. This is quite confusing as.The Null Pointer Exception is from the implicit cast from Long to long, which makes it really hard to spot.There is no JavaDoc warning that this might happen.
Redisson throws RedisTimeoutException before timeout has exceeded.Redisson throws RedisTimeoutException after timeout has exceeded.There is a race condition where Redisson may throw RedisTimeoutException after retryInterval has exceeded but before timeout has exceeded (in a nutshell when there are bursts of commands, commands will timeout even if timeout hasn't passed if retry interval is set to 0).line 765 in CommandAsyncService seems suspect as it uses retry interval for the initial timeout value..Steps to reproduce or test case.
RedissonConnection#del() doesn’t participate in pipeline.We’ve been experiencing intermittent NullPointerExceptions (stacktrace at end) that appeared to be race condition. The root seems to be that del(byte[] keys...) doesn’t properly participate when a connection is pipelined.I believe the simplest reproduction is:
RedissonSessionRepository topic listeners initialization has race condition.We’ve successfully migrated to using RedissonSessionRepository for our Spring sessions, but we get intermittent NPEs right after startup. It looks like the issue is that the first listener is added before all of the topics are initialized, and the conditional matching in onMessage doesn’t go in the same order as initialization.
RedissonSession#changeSessionId expiredKey race condition.We have a number of instances of our app in production, and we’ve encountered a race condition where the same session might have changeSessionId invoked on the same session separate servers. This means one server might receive -2 from PTTL, which isn’t currently handled.
RedissonFairLock blocks indefinitely after threadWaitTimeExpiry. Expected behavior. Threads that are alive and waiting for a RedissionFairLock will always get ownership of the lock it tried to acquire.Actual behavior.When a RedissonFairLock is locked by long-running Thread T1, and other Threads (T2, ..., Tn) attempts to access the lock and waits for longer than 5 minutes (the default threadWaitTime), even when T1 releases the lock all waiting Threads T2 to Tn hang indefinitely.This is due to future threads removing "expired" threads from the Queue, when in reality these threads are still alive and waiting for the lock. These "expired" threads never receive the PUBLISH message on unlock.Steps to reproduce or test case.
Redis is disconnected for a long time, and using redisson's trylock() will cause the thread to wait all the time.Expected behavior.notify thread and return false.or throw redis disconnected exception.Actual behavior.Keep waiting.Steps to reproduce or test case.1.Run application.2.Disconnect redis.3.Execute trylock()(1)Create a thread pool.CorePoolSize:8.QueueCapacity:200.ThreadNamePrefix:cmdpAsyncExecutor-.(2)Use trylock() in a thread.for loop 10 times.4.Wait a few minutes.5.Reonnect redis.6.View thread status.View thread stack information.We found that the thread had been waiting.
TimeSeries.iterator() doesn't respect the ordering.Expected behavior.Since timeseries is a sorted collection I'd expect iterator() to respect the ordering. There's a range method which in fact returns the values in order, however it returns Mono<Collection> instead of Flux which means entire set is loaded into the memory, while I'd like to read it reactively in chunks.Actual behavior.redisson.getTimeSeries("").iterator().next().block() != redisson.first.block()Steps to reproduce or test case.add multiple elements into the timeseries collection.open a redis desktop manager or use the cli to check the elements are indeed ordered by score.call iterator() method and check the ordering.I've checked also the non-reactive version and seems to work in the same way.
Concurrency issues with RQueue::removeIf / RQueue's iterator.Expected behavior.When having one producer and at least two consumer threads (or instances of an application) I expect RQueue's removeIf(lambda) method to remove elements without negative side effects.I expect the same when using an iterator to iterate over the queue and calling remove() on the iterator.Actual behavior.There are multiple types of errors that I observed:I've seen NullPointerExceptions inside the removeIf lambda (e.g. q.removeIf(element -> element.getId().equals("id"))), where element is null (we don't add null elements to the queue).Other times removeIf does not actually remove the elements from the queue, probably because it did not find the elements in the first place.As the default implementation of removeIf uses the iterator my guess is that basically the iterator causes this issue and may not work properly when another consumer of a queue modifies the queue while it is used.Steps to reproduce or test case.I have the following test case which can be used to reproduce the issue. It uses Lombok, Awaitility and Apache's RandomStringUtils for convenience (in case you want to run it without modification). Below I will explain steps to modify the test to get different results.The test has one producer thread pushing 200 random Strings (wrapped with QueueEntry objects) and one or more consumer threads "processing" these objects (parking them in an "in-progress-queue" - which is the problematic queue).If you run the test as it is here it will fail because the "in-progress-queue" is not empty.Modifications to change the test results:Change the number of consumer threads to 1 in the line marked with (4). Now the test should pass. Change the number back to something larger than 1.Comment in the RLock and its usages marked with (1). Now the test should pass. Comment the lines out again.Comment out the removeIf(...) statement (marked with (2)) and comment in the remove(...) statement (marked with (3)) instead. Now the test should pass.Instead of using removeIf() you could also modify the test to use an iterator and remove elements with it. The results will be similar.Default single server config with JsonJacksonCodec.
Rlock not able to handle 200 concurrent requests.Hi,I was trying to use distributed lock to do some operations and here I am facing an issue - it wasn't able to handle more than 200 concurrent requests.I am using redis as single server instance for this testing and here is my code.Please advise on what to look for to handle higher concurrency and handle the required requests per second. Our target was to handle around 1k concurrent requests.
Is there any method can tell multiLock is held by current thread?In RedissonMultiLock method isHeldByCurrentThread direct throw unsupportedOperationException.
RedissonFairLock.tryLock returns with false after timeout even when lock has been released.We use RedissonFairLock in our application to synchronize across multiple threads. Whenever there are several simultaneous lock attempts on the same lock, one or more of the waiting threads always errors out with tryLock returning false after the waitTime configured, which is almost 5 mins, even though I can clearly see that the other threads that were holding the lock have long since released it.Is there a known issues with RedissonFairLock? Also, would switching to a non-fair lock help (we don't really need fairness guarantees)?Code used to obtain lock -.Redisson version being used is v3.13.6.
the lock expiration  failed when the network breaked.I have a problem ,this is code:at imeUnit.MINUTES.sleep(5), break the network,the program throw a network exception,restore the network,find the lock expiration  failed,other thread can lock the key ,is a problem?
can i use redisson with spring cloud getway and webflux.when i use redisson in spring cloud getway filter,but when i unlock this rLock report error log not locked by current thread , this rLock is not the same thread when i locked, how can i fix this problem.
RMapCacheReactive.getLock returns org.redisson.RedissonLock instead of org.redisson.api.RLockReactive.Expected behavior.RMapCacheReactive.getLock should return an instance of org.redisson.api.RLockReactive.Actual behavior.RMapCacheReactive.getLock returns an instance of org.redisson.RedissonLock and throws exception:Steps to reproduce or test case.
Redisson 3.12.5 unlock hang forever.redisson version:3.12.5.redis version:5.0.7.redis cluster mode: 3 master 3 slave deployed in vm.question descript:In my production environment, a thread(dubheTaskExecutor-pool-2-thread-836) hangs up forever  whe invokes RedissonLock.unlock() . And I never found any error  in my application log. I am desired to find out if is a bug in redisson 3.12.5.The thread dump file is below(core part) :
internal RedissonBaseLock.evalWriteAsync() method isn't executed again if cluster slaves amount > 0.Expected behavior.Actual behavior.Steps to reproduce or test case.Redis version.Redisson version.Redisson configuration.
RedissonBaseLock can't be unlocked if exception got thrown from the same thread.RedissonBaseLock.java:312.opStatus is always null:RedissonPromise.java:183  (status of netty's Future is canceled due to Exception thrown from the same thread)I'm experience it permanently.At a first glance (if i'm not mistaken) this is a Blocker  level issue.RedissonPromise.java:187:Same behavior if Exception got thrown too early, such that f.getNow() results null.
Failed to cancel scheduled tasks and blocked threads.Expected behavior.Normal mission cancellation.Actual behavior.Failed to cancel the task and blocked the thread.Steps to reproduce or test case.When canceling a timed task, if the task is running, there is a chance that the task will not be canceled. If we use synchronous method to cancel the task, it will cause the current thread to block permanently, and the actual analysis shows that it will be in the scheduleCheck method.
There may be concurrency problems in org.redisson.RedissonLock#tryLockInnerAsync.if two thread get the same lock subject and run the method at same time, one's leaseTime is -1, and another's leaseTime is bigger than 30, the "-1" thread first cam in and set internalLockLeaseTime, then another thread came in set the internalLockLeaseTime bigger than 30, then if the bigger leaseTime one get the real redis lock! It while cause watchDog invaild and the lock be expired ahead of time.
Thread lock starvation with RSemaphore on high concurrency with hundreds of locks.We (@cstamas and me) are currently trying to integrate Redisson into Apache Maven Resolver. The purpose is to synchronize access to the Maven local repository across multiple processes. In our tests we experience a constant thread stravation where a write lock cannot be obtained in time, but tens of other threads are given access with RSemaphore. I assumed that we are having the same problem as in #1763, but the fiddling with config options did not work out. The lock is a hot path in dependency resolution.The amount of locks is around 500 accessed by around 100 Maven modules from 48 threads (T4C on a 12-core Xeon processor).Here is a sample condensed overview of semaphore permit acquisition and release:I can provide full debug output, actual source code (see PR), pre-compiled Maven distro and the sample project which almost always fails for me. Help would be very appreciated because I am at my wit's end.Note: The following lock types work flawlessly with the same project: rwlock-redisson (RReadWriteLock), semaphore-local (Semaphore) and rwlock-local (ReentrantReadWriteLock).
Async lock thread affinity.Hi,I'm working on a kotlin micronaut web app which is attempting to be fully async and we're using redisson for distributed locks.I'm somewhat unclear on how to exactly use it correctly.The official documentation says RLock object behaves according to the Java Lock specification. It means only lock owner thread can unlock it otherwise IllegalMonitorStateException would be thrown. Otherwise consider to use RSemaphore object. which sounds weird as in an async context, executing on a specific threads is never guaranteed.The code I'm using for now, in a kotlin suspendable func, is like this:The unlock line is not guaranteed to execute on the same thread as the lockAsync line.Thanks!
RLockReactive fail to unlock when subscribe to tryLock(Thread ID)RLockReactive fail to unlock with RedissonShutdownException when un lock it at the end of the reactive sequence with doFinally() or with then() operators. When testing it with StepVerifier the unlock is complete sucessfully but fail with subscribe().Example:
Trylock causes a deadlock,System restart can be unlocked.Expected behavior.Actual behavior.Steps to reproduce or test case.
RExecutorService does not work,block at future.get()redis_version:6.0.2.code.
Redisson renewExpiration not working after redis lock expires.Expected behavior.After a lock is expired or removed on Redis-Server a new lock should be created with renewalExpiration renewing TTL of a lock.Actual behavior.A new lock is created , but for some reason this new lock do not call renewExpiration but add this ThreadId to a map and  I dom't figure it out how it uses this map to renewal.On this specific case I was trying to use the same thread to reaquire this log,  I tried with a new thread but the result was the same.If I restart the applications, as this map is empty  the ttl renewal works as spected.Steps to reproduce or test case.Set a HIgh watchdog (30s) aquire a lock expire it manually on server. Aquire it on the same application.
Redis renewExpiration  ttl problem.Expected behavior.Allways renew the ttl key .Actual behavior.after looses a lock and try to aquire it again, sometimes the renewal feature doesn't work as it supposed.Steps to reproduce or test case.To reproduce it I had to use debug stop the execution before this call.with redis client remove the key.Then release the flow.if the key does'nt exists the renewal doesn't fire for this specific key, and it's everything ok.So my code aquire this lock again since it's not the owner, and everything seems to work except that it doesn't renew the ttl key lock because of this.In my point of view if you add.cancelExpirationRenewal(threadId)when the renewal fails , it solves the problem.
Thread lock starvation with RSemaphore on high concurrency with hundreds of locks (2)This is a followup to #3573. We have now received the first user feedback from @jebeaudet which exhibits the same behavior as in the previous issue.We have tried with Maven 3.8.x from the maven-3.8.x and Resolver 1.7.1 with Redisson's RSemaphore. Several runs observe the same behavior. One thread tries to acquire a lock which is not held by anyone else and after the time is up (30 s) no lock is obtained. @jebeaudet privately provided me log files of those builds and I have distilled the lock workflows from it. Here is a sample:The write lock artifact:ch.qos.logback:logback-classic:1.2.3 is not held by someone, yet it fails. I can provide several SQLite databases which contain raw imported data, prepared data as well as joined by lock workflows/transitions. The thread dump of a hanging build looks always the same:Nothing, but localhost....PS: There is a similar issue with RReadWriteLock, but I will report this seperately.
FairLock deadlock again.Expected behavior.redisson_lock_queue and redisson_lock_timeout keys disappear once the lock that was taken by the first thread is timed out automatically after lock lease time, and then the second thread successfully takes the lock.Actual behavior.Sometimes, even though the lock from the first thread is timed out and released automatically, the redisson_lock_queue and redisson_lock_timeout keys are stuck and the second thread can't acquire the lock, even though nobody is holding it. The second thread is stuck for full acquisition timeout. Then, it fails to get the lock eventually, and keys redisson_lock_queue and redisson_lock_timeout disappear, making the lock available again. Feels like lock expiry notification gets lost and Redisson waits for nothing. P.S. In this regard, would be great to have FairSpinLock not to rely on pubsub.Related? #2883.Steps to reproduce or test case.1 redis, 1 sentinel.
redis lock not working as expected.I have the following test scenario:My expectation is the following: if the lock couldn't be acquired in 1s  then it should show Failed to acquire lock. If the lock is acquired then, in 30s, it should be automatically released by Redis (this 30s are very important as I only need to allow another run after this time). Only one should should be acquired in 30s. The problem is that most of the times I get multiple Lock acquired between 30s intervals.The code launches 1000 parallel execution tasks. Since Redis is (mostly) single threaded then I wasn't expecting any concurrency problems (and I'm pretty sure they aren't on the Redis side). I am also using a single node Redis.I have also tried another solution using Zookeeper Leader Election and that one works as expected using the same caller function. It acquired a single lock at a time.Am I doing something wrong here are we having a Redisson issue?
Different effects when Mix using of multiLock and Lock.version:redisson-3.14.1.When try to using both multiLock and Lock with same key.There are different effects according to the lock order.When lock lock1 first ,then lock multiLock1.multiLock1 stun for a few seconds and then, lock success(lock1 are not released)When lock multiLock1 first ,then lock lock1.lock1 are stoped and waitting for the multiLock to release.it's confused.
RedissonKeys.Delete() method pass empty array will block the thread in redis cluster.Expected behavior.Exception throws like single redis server.Actual behavior.In cluster mode, this method will block the thread and no exception throws.Steps to reproduce or test case.
RedissonLock Perhaps leak.The tryLock(long waitTime, long leaseTime, TimeUnit unit) method, if the subscribe timeout, and then subscribeFuture.cancel success,when to call unsubscribe , the subscribeFuture.onComplete should move out ,before the if block.
RedissonFairLock does not automatically clear non-running LockName and non-reference lock methods after a restart and cannot be automatically renewed.Expected behavior.Actual behavior.Production environment with a large number of requests queued.At around 19:23 on the night of September 9, due to the existence of a large Key in the store, after submitting a work order to delete the large Key, there was still an imbalance of data in the cluster slice, and the DBA tried to perform a memory analysis of the cluster, resulting in a Redis slice switch and the loss of some of the Keys.Redisson's unlock mechanism will go to determine the LockKey Delete state, if Delete succeeds then publish a message to notify other clients subscribed to this lock to compete for the lock, if Delete fails then there is no message notification (Delete is a failure state when Key does not exist), thus causing the clients subscribed to the lock to wait until the thread wait timeout time (default is 5 minutes) to compete for the lock again.Steps to reproduce or test case.Creating a large number of concurrent requests for locks.The business processing request time to obtain the lock is set longer than 30s.If thread A acquires the lock and manually deletes the key of the lock, i.e. RawName.Observing other threads acquiring locks, you will find that you need to wait a long time to get the lock, and after restarting the main method of Test, you cannot empty the last waiting queue, but can only wait for the default 5 minutes to clear the non-running state LockName(id:ThreadId)1. RedissonFairLock.lock will not automatically renew.
RLockReactive tryLock waits longer than waitTime.Expected behavior.long id = Thread.currentThread().getId();... lock.tryLock(0, 10, TimeUnit.SECONDS, id) ...Expect this to exit immediately - reporting "true" if it can get the lock now ; and reporting "false" if it can not.Actual behavior.It seems to block and wait until the lock becomes available.The waitTime (set as 0 above) does not seem to influence the behaviour.Steps to reproduce or test case.Invoke "lock.tryLock(0, 10, TimeUnit.SECONDS, id)", when the lock is already taken.Expect it to instantly return with "false". It does not.((Note: I am working inside Quarkus, and using "reactor.core.publisher.Mono" to convert the Redisson Reactive Core (Mono) into Mutiny (Uni) - this does not seem related ; but it makes it harder for me to frame a pure-Redission test case))
Redisson getLock operation does not respect timeout.Expected behavior.Redisson getLock timeouts after timeout.Actual behavior.Redisson getLock timeouts after timeout + 1 second.
org.redisson.client.RedisResponseTimeoutException: Redis server response timeout when use RLock.My Redis don't use cluster just only one node address.I used RLock for locking but was not successful because of Redis timeout. I restarted the service, it worked but it's not the best way in the production stage.I referred to the same issue about Redis timeout such as an increased nettyThreads or timeout config but it's not successful.
Concurrent attempt to RReadWriteLock writeLock leaves thread in blocked state after lock is released.Two Threads, thread-1 and thread-2 attempt to acquire a RReadWriteLock.writeLock() using lock().  thread-1 gets the lock, thread-2 waits for lock to be released.  thread-1 releases lock, thread-2 acquires lock.When thread-1 releases lock (using unlock()), thread-2 does not acquire lock.  Wait for lock expires after 35 seconds.2 threads call lock() on the same RReadWriteLock at or near the same time.thread that gets the lock release the lock (after a second or two).Note that thread-2 doesn't automatically get the released lock..Redisson version 3.16.3.
Stuck on joining HashedWheelTimer worker thread when shutting down.Sometimes Redisson seems to be stuck when calling Redisson.shutdown(). Some Services are stuck for mutliple days. But it seems to be a very rare event,This is the Thread netty tries to join. Stack trace does not seem to change over time (took multiple thread dumps)Expected behavior.Redisson.shutdown() should be blocking but not for ever.Actual behavior.Redisson.shutdown() is stuck for days (in some rare cases) and the Timer thread is taking huge amounts of processing power (this is not a typical profiler bug with parking threads but acutally showing in the process list)Steps to reproduce or test case.Not tried but maybe if you could point us in a direction.
Got a NullPointerException in WriteReadLock.the redis model is cluster.
Error while aquiring ReadWriteLock and Lock at the same time.When I call method runWithRwLock and method runWithMultiLock at same time, it works fine. When I call method runWithLock and method runWithRwLock/runWithMultiLock, the following error is frequently reported.This is my test code:
Error while acquire Lock and ReadLock.Steps to reproduce or test case.Let me simplify the reproduce process:Thread x get Lock "A" and hold the Lock.Thread y get ReadLock "A" and also hold the ReadLock.Thread x and thread y all acquired lock.Thread y call method unlock to release ReadLock y.After a few seconds, thread x also call method unlock to release Lock x,and then it causes an exception.Here is stacktrace.
multilock fail.Expected behavior.lock successful.Actual behavior.lock fail when get first lock in multi lock case.Steps to reproduce or test case.when I set waitTime=0in multi lock , it fail, but I set waitTime=0 in RLock, it success, why?then , debug RedissonMultiLock,  until Line 413.I think it is a bug in RedissonMultiLock.
multi lock fail.Expected behavior.lock successful.Actual behavior.lock fail when get first lock in multi lock case.Steps to reproduce or test case.when I set waitTime=0in multi lock , it fail, but I set waitTime=0 in RLock, it success, why?then , debug RedissonMultiLock,  until Line 413.I think it is a bug in RedissonMultiLock.
MultiLock causes deadlocks in high concurrency cases.Expected behavior.No DeadLock.Actual behavior.DeadLock.Steps to reproduce or test case.The complete code is as follows.Deadlocks don't happen every time. After running the program for a while, I found that the process stopped and the keys in redis were fixed, not added or reduced. What is even more strange is that the expiration time I added the lock is 800ms, and if I use the ttl command to get the expiration time, the result should be 0 or 1, or a negative number. But, I found a number greater than 1! And it keeps changing, like this: 4s -> 3s -> 2s -> 4s.
Thread safe problem when acquiring locks resulting in a pubsub channel not being unsubscribed · 64 ·.This seems to be related to:  #2575.Expected behavior.The pub/sub channels that are created while trying to acquire a lock with RedissonLock should be unsubscribed when no longer needed.Actual behavior.In some cases when 2 pub sub channels are created around the same time for (2 different lock keys) one of the two throws an error:Subscribe timeout: (7500ms). Increase 'subscriptionsPerConnection' and/or 'subscriptionConnectionPoolSize' parameters.After this error the failed lock key will lock fine when not locked. If it is however locked by another thread it will fail instantly.see the logging from the test case below, notice how thread [pool-1-thread-12] successfully locks key 0. When thread [pool-1-thread-19] tries to lock lockkey-0  it instantly throws the subscribe timeout exception (notice it does not wait 7500ms).If there is no locking going on and the app is still running it kept a channel active for lockkey-0.Some logging from the test case when it fails:
ERR WAIT cannot be used with slave instances.This occurs when trying to acquire a lock.Currently, we are using redisson library 3.16.3 with readmode: SLAVE.
Redisson : Issue while unlock lock for a key ,  attempt to unlock lock, not locked by current thread.Hi, I am getting below errors in my logs.I am using below dependency for redisson:And I've implemented a wrapper that will acquire and release the lock:But the log "Released lock for key" is logged rarely. The task after acquiring the lock gets finished within 1 or 2 seconds and the wait and lease time are 5 and 15 secs respectively.RedisConfig is :We have redis server in Kubernetes cluster with 2 pod instances both as master.Would appreciate any suggestions?
response timeout in a job lead to deadlock.redisson version: 3.9.0.action:creating a thread makes a job every 5 seconds.job acquire fairlock  trylock(10, 10, timeunit.seconds)redis busy or bad network, response timeout(default 3s)5s, this instance will reentrance  lock.other intance will not acquire lock.suggestion:when redis response timeout , should redisson release the lock?
Unable to Acquire Fair Lock.In my code, I am trying to acquire a fair lock with a fixed waitTime and leaseTime as so.fairLock.tryLockAsync(waitTime, leaseTime, timeUnit, ID)Sometimes, all the requests will get queued up for apparently no reason, while no request has acquired the lock.I can see a wait queue being formed for the lock, but the actual lock is not held by anyone.Eventually, all of these waiting requests will time out and fail, and new requests will work fine.This seems to happen randomly when there are a lot of simultaneous requests.I am not able to reproduce this locally.
when i set waitTime=0 in multi lock, it fail.about issue#4033,I don't need to wait,so i set waittime=0,but the result is failure,why Rlock set waittime=0 result is successful?i need get the lock fail,end business at once.see source code,when waitime=0,leaseTime assignment zero.Expected behavior.Actual behavior.Steps to reproduce or test case.
Only one thread can get lock after unlock timeout.I have an annotation RLock which works with an aspect, it work well in some programs.Here is the codes:Someday a program throws Exception at every tryLock in two of three containers. I checked ELK and find RedisTimeoutException before the first exception.By the way, every container works with only one thread to consume kafka messages.It seems taht  the first RLock unlock timeout and then redisson treats next RLocks as reentrant cause they have the same thread Id.How to ensure that the lock could be released correctly after unlock timeout.
isHeldByCurrentThread() goes wrong.
RScheduledExecutorService's  WorkerOptions.workers() doesn't work.and i found it will consume 16 thread once instead of 5.
RateLimiter error during unpack on redisson 3.15.0 - bad argument #2 to 'unpack' (string expected, got nil)Expected behavior.No error when trying to acquire a RRLimiter lock.  Error seen when multiple threads from one application try to acquire a lock from a 3 member redis cluster.Actual behavior.Intermittent error seen,I believe this internal script's function is the source of the problem:I am not so sure why v would be nil.Steps to reproduce or test case.Not easily reproducible as of yet.3 member redis cluster with sentinel for HA, running in kubernetes 1.20.2.
RLock lease expiry not being extended in spite of RLock being acquired.Our Microservice is running in Azure. It is deployed with 2 pods. Each pod attempts to lock the same RLock, the idea being that only one pod at a time holds the lock.RLock is obtained using org.redisson.api.RLockAsync.lockAsync() and onComplete() lambda is used to get notified about lock acquisition.We observed following: pod1 had the lock but then pod2 got notified of lock acquisition.So we lock again and within 1 msec get notified about success: heldByCurrentThread=true, ttl=29998.This cycle continues from then on:we get the lock without issues, very quickly.within 30 seconds we lose the lock and so go back to 1)I think 30 seconds matches lockWatchdogTimeout setting. I see no warn/error log related to Redis connectivity.I have logs for around 3 hours when this issue has persisted but it might have well lasted for much longer.Expected behavior.If a client acquires RLock then it should also be able to extend its (lease) time.Actual behavior.A client acquires RLock but then seems unable to extend its (lease) time.
Spring Data Connection in multi mode causes thread stuck.No description provided.
ClassLoader issues when using Redisson Transactions with asynchronously acquired RedissonClient.Expected behavior.When using an asynchronously acquired RedissonClient (via. CompletableFuture.supplyAsync) with transactions I expect the class loader used to be correct.Some constraints to narrow the scope of the problem:I only encountered this when using Redisson Transactions.If the client was acquired in a synchronous way, I did not see this behavior.Actual behavior.Similar to #2984 I am seeing that on the first request to my WAR on a web server, the correct class loader is used.But then on subsequent requests, it is using the URLClassPath loader what I assume to be the default for Jetty.After this breaks, I get consistent ClassNotFound errors.Steps to reproduce or test case.I created a minimal jetty project here in an effort to reproduce the code we are running.In words, my steps were as follows:Create a Jetty Server running 9.4.30.v20200611, and deploy a WAR on it. Place objects into Redis, using transactions, and try to retrieve them. After the first request succeeds, observe ClassNotFound issues when trying to retrieve other objects. The stack trace can be found in a gist.
RedissonList concurrency problem.How is it ever safe to rely on RedissonList.size() when each invocation can technically return a different value? This would mean that calling subList(0, size()), or even two consecutive calls to hasNext() on the iterator (which is normally what happens) can potentially cause an exception.
Subscribe Timeout Exceptions - After master Re-election.None, or fewer Exceptions (just during failover period)After a master re-election (and not otherwise) a constant percentage (about 1%) of the requests fail with this exception until the Redisson client is re-initialised manually with a pod bounce.Steps to reproduce or test case.I'm running 3 redis (master, slave, slave) alongside 3 sentinels.I'm running a constant load of requests through a redisson client which can run indefinitely without any issue. .When I introduce a chaos script to trigger a failure of the master and thus a master re-election I start to see the errors, yet only for a very small percentage of the requests (but that percentage remains constant indefinitely until a manual restart of the redisson client happens)Each request acquires and releases an RLock with a chance of collision with another request being 1 in 100.If I lower the collision probability to be 1 in 10000 this problem almost completely disappears to the point where I lose only perhaps a single request, during the failover period (which is the expected behaviour for the higher collision case also)
io.netty.resolver.dns.DnsNameResolver#doResolveCached race condition java.lang.IndexOutOfBoundsException.It's an known bug in io.netty 4.1.19.Final and fixed in 4.1.20.Final.netty/netty#7583.Sometimes my AWS Lambdas fail by creation of the redisson client.
Ack object used to prevent race-condition during ack receiving should be created per request.Because of this issue RemoteServiceAckTimeoutException could be thrown prior the end of ackTimeout.
Long latency issue while call Redis.I use Redisson 2.2.10 to implement distributed lock in my project. And then I run load test for this distributed lock using SINGLE redis mode. But I found there are many long latency APIs and it's wired that almost all the long latency APIs are a little greater than 1000ms. The average latency and 99 percent latecy is about 15ms and 30ms respectively. And then I used jstack to get the call stack and found most of the threads are WAITING at the same place just as the following.Meanwhile, during the test, I dumped the network packages using tcpdump and found that it took about 1000ms before the first package was sent to Redis. And in Redis server, I didn't find any slow query whose latency is more than 10ms. So I think, for the long latency API, most of the time might be cost in the project not the network. So can you help me to have a check or give me some suggestion about this issue?  Many thanks.
SAdd in RedisAsyncConnection uses wrong AddValues.line 629 uses addMapValues for sadd rather than addValues.This causes JsonJacksonCodec to fail on Longs stored with the mapObjectMapper to fail to be deparsed by objectMapper;Change line 629.Test code follows :
RedissonList concurrency problem.How is it ever safe to rely on RedissonList.size() when each invocation can technically return a different value? This would mean that calling subList(0, size()), or even two consecutive calls to hasNext() on the iterator (which is normally what happens) can potentially cause an exception.
"Connection Leak" in RedissonBucket.Hi guys!When I get some error, eg: redis servers down, in getAsync and setAsync the connection is not released, once the release is done in a Promise listener, that never executes, can you check this please?Regards,Marcio.
ClassLoader problem causing file lock error. If you attempt to open a connection on Windows through two seperate ClassLoader(CL) who each have their parent set to null you will get the ;DB in use by another process error. This occurs even if the first CL has been null'd because either the lock hasn't been GC'd or it has to do with Windows not releasing the lock properly. The easiest way to reproduce this is to create two JUnit tasks which both attempt to open a connection then run those JUnit tasks through ANT.
NIO lock problem. I am getting an exception when trying to allocate a connection. I tried to clean database files before running the program, with no success. It was tested on Compaq Tru64 platform, java version. The same code _works_ on Win2000 or Linux (also java ver 1.4) with no problem, so maybe this is just a buggy Java NIO API on Tru64. But I need to make it running on Tru64 somehow. Is there any way to avoid using NIO API in hsqldb? Right now I just commented all code in
Server hangs on select statement. The attached zip file contains a SELECT statement that worked in 1.7.1 but in 1.7.2 RC1 doesn't work. In 1.7.2 the server gets locked up and CPU utilization goes to 100%. I then have to kill the server. I tested this statement with org.hsqldb.util. DatabaseManagerSwing and with DbVisualizer (commercial SQL manager) and with my Java application. I got the same results. This statement works fine with a MySQL version of my database. The zip files contains the SELECT statement, the db script (pinyin.script), the server.properties and pinyin. properties and db.properties
File Locks not released when CMI Fails.We would like to make it so that we can restore the Hypersonic data when it gets corrupted but when getConnection() fails it doesn't release the locks on the files so we can't do this. I just created a database and then changed some of the data values in the .script file and I got this exception: If you need any more information then let me know.
lockfile and mac osx. We have mac osx server where users home directories are mounted by several clients. On the client, hsqldb fails to start due to a problem in the locking system. Here is an example with hsqldb logging turned on. Notice that the directory is empty, and hence the database cannot exist yet, and hence is not being accessed by any other process. However, hsqldb fails to start saying org.hsqldb.HsqlException: The database is already in use by another process: It runs fine if I use a local disk. I presume this is some incompatibility between the hsqldb file locking system and apple's afp disk mounting system???
Restart of engine fails due to spurious locking issue. I have noticed that despite closing all connections and exiting a standalone database, at least one connection still remains. The defect is manifested in HSQLDB 1.7.3 and HSQLDB 1.8.0 RC 8. If you compile and run the sample code below, it will run fine for the first time. Second time (if it is run immediately after the first time say within 2-3 seconds after completion of first execution) it fails everytime citing a database locked exception! The only workaround I could think of was to use the un-documented function:
TEXT tbl - source file is locked until restarting the engine. Hello, hsql team! TEXT table - after making it READONLY the source file is still locked and cannot be changed until restarting the DB-engine. It happens only when the table is switched from read-write to read-only mode. When the table has already been READONLY, the source file is not locked. I hope, that the source file should be unlocked after setting READONLY and/or DESC mode.
CMI hangs on shutdown in 1.8. Hi, I'm getting a deadlock when doing a database shutdown. The deadlock is easier to reproduce on slower system. It appears that HsqlTimer$Task first locks the cancel_mutex and then inside HsqlTimer$TaskQueue.signalTaskCancelled, a task queue. The HsqlTimer.nextTask first locks the queue, and then inside HsqlTimer$Task.isCancelled, the cancel_mutex.
Deadlock when using setWriteDelay. while executing "SET WRITE DELAY" I get a deadlock (as usual not every time) between the Timer thread and the thread executing the SQL. Here's the thread
Deadlock in 1.9a2. Using hsqldb 1.9 alpha2, I encountered a deadlock, wherein one thread was attempting to get a new connection to the db, while another was in the middle of checkpoint. I'm using a mixture of cached and memory tables. Relevant bits from the stack dump below.
Deadlock in b3. Circular dependency between SessionManager and Logger causes thread deadlocks (see attachment). However, it seems to be easily avoided in this instance by removing line Log.java:430 in addition to the code commented out below it. There's a second reference to database.sessionManager on 780, but it seems safe.
hsqldb.b3 locks up. I ran a multithreaded test and had several threads lock up in this state: I'm running on 1.6.0_03-b05/RHEL 2.6.9
1.9rc4: Deadlock. Hello, hsqldb went into a deadlock (please see attachment stack_trace.txt for stack trace). Beginning of the script file is at the end of the above attachment.multiple, concurrent read threads, setup using single write thread, executing concurrently with the reads. db is a mixture of memory and cached tables, cached table has blobs, total db size ~1GB
Apparent deadlock with r 3281. While running an application against a HSQL server which imports and exports XML files, I found that after upgrading from HSQL 1.9 revision 3271 (or perhaps 3274) to revision 3281, situations which resemble a deadlock started occurring. They involve the affected client only - other clients connected to the same server can continue working undisturbed, and the server still responds to them. Basing on the method in which the client hangs, it could be attempting to execute something like: ...but I am not sure if this is always the method which hangs. Others might be involved, simultaneous activities by other clients might be involved, and the SQL involved might vary. A thread dump from the client looks like this:
ClassNotFoundException while testing r3310. While trying to gather data about the deadlock problem which I described previously, I attempted to start testing with SVN revision 3310. However, transitioning to this revision in regular manner failed, producing ClassNotFoundExceptions. After issuing these exceptions, the server utilized a lot of CPU for 7 minutes (sorry, couldn't wait longer), and didn't manage to reach readiness for work. That's when I sent it the "-QUIT" signal to produce a thread dump.
Deadlock in 2.0.0 RC8. Concurrent transactions frequently cause deadlock. Downgrade to 1.8 fixes the issues
Deadlock between CMI and CMI. I'm using HSQLDB 2.0.0 (in-memory) with Hibernate and am experiencing a deadlock when multiple threads persist lobs concurrently. I've replicated the problem in a very simple test case, and it happens immediately. See below for stack traces from deadlocked threads.
Deadlock between CMI and CMI. Similar to the bug 3072706, i can reproduce this problem with hsql in-memory (14-Nov-2011,2.0.1-rc3SNAPSHOT) and hibernate 3.6. I got a deadlock when multiple threads persist lobs concurrently. One thread got a lock on the database, but have to wait before a synchronized method (LobManager.adjustUsageCount). The other thread wants the lock too, but is in the synchronized method LobManager.setCharsForNewClob. See below for stack traces from deadlocked threads.
deadlock in ScriptWriterBase (checkpoint). sync order against BufferedOutputStream and ScriptWriterText are inconsistent; checkpoint causes deadlock:
CMI can cause deadlock. I am experiencing deadlock when two different threads attempt to close the database with hsqldb 2.0.0 and 2.1.0. The issue is that Thread-1 acquires a lock on SessionManager@58a983 when SessionManager.closeAllSession() is called. Thread-1 is also attempting to invoke Session.close() the active sessions (i.e. Session@e107d9) which requires a lock on the session. Meanwhile, Thread-2 has already acquired a lock on Session@e107d9 when it called Session.execute(). Thread-2 is also attempting to acquire a lock on SessionManager@58a983 for SessionManager.closeAllSessions, but the lock was already obtained by Thread-1. I have posted a more concise summary and the relevant thread dumps below. Deadlock summary
HSQLDB hangs forever on a deadlock case. Get a table with two rows and a primary key: Then, in two connections: I expect an exception "deadlock" on query 4. However, HSQL hangs forever yet on query 2 :( Is there a workaround? How can I detect a dead lock? My application can work with several different RDBMS, and with other systems I can detect a deadlock, roll the current transaction back and re-try my queries a bit later. With HSQL DB in this case my application hangs forever :(
Checkpoint deadlock in mvcc read_committed mode versions 2.2.6. I use HSQLDB for an image caching system. The images themselves are stored on disk but an image statistics (last access, usages etc) and location on disk are stored in HSQLDB. I experience deadlocks at the point the logfile is processed and emptied. Even though not sure this is because of my own code (it locks rows when updating statistics and when records are evicted, Spring and Hibernate should commit the sessions), I haven't experienced any deadlock issues with MYSQL so far. I have a unittest that reproduces this issue on my machine. It runs 10 concurrent threads that read from the database (read and update same record's stats) and 1 thread that inserts records. I tried both direct jdbc:hsqldb:file: connections and standalone instance with jdbc:hsqldb:hsql connections. My connection string (issue happens with both cached and normal table settings): Furthermore I use a 20 connection pool (dbcp2). I captured the session data together with a threaddump, see attachment.
Deadlock when using HyperSQL 2.3.4 with Flyway migrations. When using the Flyway database migration system with HyperSQL 2.3.4, the attempted application of migrations results in deadlock. Version 2.3.3 works as expected and I found that the code change causing the deadlock is the following code added to TransactionManagerCommon.java line 558 in commit r5537: For some context, one of the first things that Flyway does when it starts is open up a connection to the database and lock its metadata table. The migrations are applied in a separate connection. The code pasted above gets run a few method calls below the executeCompiledStatement method in Session.java (this method is what is trying to execute my first migration). The getTransactionSessions method is adding the session that Flyway uses to lock its metadata table to the "tempSet" of the session that is executing the migration. This effectively causes the executeCompiledStatement method in Session.java (around line 1355) to wait forever -- the session that Flyway is using to lock its metadata table doesn't commit (and therefore release its lock) until all the migrations have run, so the count down latch will never decrement: The deadlock described above can be replicated by using Spring Boot's flyway sample and modifying the pom.xml so that the application uses HyperSQL 2.3.4 instead of H2. The unit tests for the sample show the deadlock occuring -- the build and units tests get run by running the following command: The output of the command will stop and lock up after outputting the following:
Error DELETE with referential integrity DELETE ON CASCADE. We use HSQLDB 2.3.3. We decided not to use 2.3.4 because of bug 1441 (Deadlock when using HyperSQL 2.3.4 with Flyway migrations). We had a problem trying to delete a record from a table woth referential constraint in DELETE CASCADE. Below you can find DDL to create some tables of our database
The row from table core.mainCols contains a columnMap that throws this exception because the number of elements of array is less than columnMap indexes.
HSQLDB Deadlock using 2.4.1 and MVCC. We encounter a recurrent deadlock issue using the 2.4.1 version in one production environment. We have been using this version for a long time in various environments and have not seen deadlock issues until now. The issue appeared recently in this production environment, and is reappearing regularly after restarting the application and waiting for a few day of usage. The database is quite big, with million rows in some tables. The hsqldb server is configured in MVCC mode and sessions use READ_COMMITED transaction isolation. We did not change the defaults for hsqldb.tx_conflict_rollback or hsqldb.tx_interrupt_rollback. I wonder if these settings should be modified to prevent the deadlock from happening. After connecting to the database manually and inspecting the session, we observed that a checkpoint session is waiting for the following session, which never seems to terminate: The checkpoint statement, on the other hand is preventing many transactions to complete (which I believe is expected). We tried to manually end the blocking session 5969 using ALTER SESSION RELEASE (and then CLOSE), but the session does not seem to respond, we tried as well END STATEMENT without success. We tried as well to use ALTER SESSION RELEASE | CLOSE on the CHECKPOINT (6076), and here it disappeared, but all other sessions waiting for this checkpoint session 6076 were not unlocked and are still displaying THIS_WAITING_FOR 6076. It is quite difficult to understand the root cause of the issue as we cannot identify the origin of the 5969 session, which statement it ran, etc. If we could forcefully provoke an error on this transaction, we could hopefully see in our logs the error and thus could identify the origin. For now we are leaving the server in this deadlock state to hopefully investigate further.
Extraneous sleep  in Log class. In version 1.61 of hsqldb in the CMI method there is a 3 second sleep that seems to be there to do some sort of concurrency checking. I could not find anyplace where the lock file is actually created. I believe this is unnecessary code and can be removed. It would significantly improve start-up time.
Lock File Does Not Work. In the latest release the Database in use exception is no longer thrown. I have tracked this down to substantial changes in the lock file functionality. I turned on the new hsqldb.lock_file property. In this new version instead of getting a SQL State 08001 with a nice Database in use message you get File input/output error XXXX.backup java.io.FileNotFoundException: XXXX.data (The requested operation cannot be performed on a file with a user-mapped section open) In some cases during my testing my database got corrupted and I had to restore a backup in order to continue testing.
LOCK TABLE <TABLE> WRITE causes NullPointerException. When you perform a "LOCK TABLE <TABLENAME> WRITE" the following exception is thrown.
Multiple start possible - lock file does not work. The lock file does not work - at least in NON NIO case. The attached patch improves the situation for me. The issue #2547727 may be related.
rollback handling. Looks like a similar issue to comment #2 to 2828178. Transaction failed with index constraint violation (which shouldn't be possible, but I'm investigating), and an attempt to rollback caused a series of calamities (please see attached stack trace). Another unfortunate consequence was the fact that the CountUpDownLatch was left in locked state:
Table never unlocked after select with autoCommit off.Seems to me this is another aspect to bug 2805121 but I couldn't attach to that so here's a new bug. This is against the latest support snapshot (3rd Sep)Simply doing a select statement in a session with autoCommit off will mean the referenced table(s) NEVER get unlocked. Obviously this is bad.The statement in bug 2805121 that the latch count does not increase for select statmeents seems to be false. I originally hit this with 3 sessions doing selects from a table and another trying to do an insert (each in separate threads). The count in the CountUpDownLatch was 3 and never changed causing the program to lock up. This can be easily reproduced with the code attached.
Dead lock with Hibernate test suite. I tried to update Hibernate test suite to use hsqldb 2.0 but it seems there is a dead lock. You can get the code from http. Simply update parent/pom.xml to use org.hsqldb:hsqldb:2.0.0 instead of hsqldb:hsqldb:1.8.0.2 and run mvn clean test -Phsqldb. The build is stuck on test org.hibernate.test.jpa.lock.JPALockTest. Here is the thread dump: Do you have an idea of the cause of the issue.
Select keeps readlock even if resultset is processed. It seems that a select issued in 1 connection (autoCommit=false) keeps a lock on the table even if the select is completely processed (it does not have any results in our case). An insert executed from another connection blocks on the select statement. See attached sample program, this program completes using hsql 1.8 but hangs using version 1.9.
lock java process. When I use the latest version of hsqldb, after insert data with unitils-dbunit, I have a lock of java process. Below, the tread dump.
Embeded Blocks in Multithreaded environment I'm usgin v2.0.1RC2 embeded with my application. It get blocked arbitrarily when two threads that share the same connection executes two statements (one each). Is not possible to share connections?
CountUpDownLatch seems insufficiently concurrent. In a highly threaded environment (64-256 simultaneous connections all accessing an embedded db with a mix of read/write, mostly read), I get stuck at the latch.await() call in Session.executeCompiledStatement(); various others have reported this on the forum. It is a bear to replicate, but seems more prone to happen with more cores (my machine is a quad-core). In looking at the code, CountUpDownLatch seems problematic; the filed 'count' is not volaitle, which means under concurrent access, different threads can increment incorrectly. FOr example: Thread 1 calls countDown(), in its memory picture 3-1 becomes 2. at the same time Thread 2 call countUp(); in its memory picture it has it's own copy of count, no volatile, so 3+1 becomes 4, when it should have been 3. In fact, while volatile will help, it should probably be an AtomicInteger, or similar structure. Volatile would however be enough if the CountUpDownLatch is accessed by separate threads under an exclusive lock; a surface look indicates this is the case. Exclusive locking however does not guarantee the memory barrier effect of volatile. Without doing more looking in TransactionManagerCommon et al, I can't tell if the methods within CountUpDownLatch are meant to be atomic; setCount() in particular seems like it should execute synchronized from all other CountUpDownLatch methods, but the usage pattern of CountUpDownLatch may protect against this.
missing syncronized block for time conversion. The following error occurs on HSQL version 1.8.0 & 1.8.1: Problem is in class: HsqlDateTime the following method should be fixed - to add syncronized block on the calendar object:
dead lock when checkpoint. When hsqldb execute a checkpoint, the entry point is org.hsqldb.persist.Logger.checkpoint, at here it will get a lock of logger, Then it will go to session.commit, in here, it will try to get a write lock of transaction, that's ok. But when we execute a select sequence statement, the execute order of updating of sequence is: get a write lock of transaction,then try to get a lock of logger, that's dead lock. Note: I am running on a non-English environment, so I translate some key-word to English by myself, maybe some words are different.
Failed to use Hibernate StatelessSession with version 2.1. It seems there is an issue when you try to get a result from an Hibernate query using a StatelessSession, no result is returned and it causes a dead lock. When using version 1.8 everything works fine.
Concurrent write while backup. sorry if this issue was already discussed in another thread that I didn't find. We're currently implementing a solution where the backup is performed while the server is running. The SQL statements are executed using the JDBC connection. As you know, while the backup is performed, the database is locked. This means all reading/writing processes are blocked. Here comes the bug description: After the backup is complete, all writing processes are unblocked, thus a new .log file is being created. And it starts with the INSERT statement (for table XXX). If you close the Java program without closing the database (e.g. process got killed, power failure,..), the log file continues to exist. On next startup, you get an exception that the table XXX was not found and the rest of the log file is being ignored. Finally, the log file is deleted so it's not even obvious WHY the table XXX was not found. And here comes the workaround: if you perform a checkpoint right after the backup, everything seems to be fine (was not able to reproduce this error) - but I won't bet .. Attached, you have a cropped version of the source code to reproduce the problem (~80% of executions). You may need to adjust the iterations and backup waiting time. The idea is that the backup starts WHILE the thread is writing data.
NPE when data fiel is locked on disk. I think, HSQLDB should throw a meaningful exception that cane be recognized as "datafile is locked".
sql in SQL routine fail to acquire lock. Please see attached doc file for description and related ready-to-run files. In a normal SQL, if we use MVCC+Read_Committed, and setAutoCommit(false), then when issue SQL like "Update xx", we will get the row lock until commit. In SQL routine, we cannot acquire row lock when issue SQL like "Update xx". However, it will be able to detect the lock acquired by other transactions. [Will pause if lock acquired by other transactions] Below is a simple example to simulate the case: 1. A Java program called Locker to acquired a row lock on a row on FIX.LOCK_TBL table; 2. A Java program called SPRunner to run a SQL routine. The SQL routine will run below. Note that when the SQL routine run to 2.2, it will pause because a open transaction by 1. This is to simulate a "Sleep" inside the SQL routine. 3. A Java program called Attacker to run. If the row lock of BB table execId='100' has been taken by SPRunner, then Attacker should wait until SPRunner release. However, the result is that Attacker can run immediately, check the table, found the f_2 value changed to 6. It proves sql statement in SQL routine fail to acquire row lock.
Starvation issues with LOCKS transaction mode. I'm seeing starvation problems with LOCKS transaction mode. Basically, in a single-writer/multiple-readers scenario, there seem to be many reader threads hanging on the latch.await call, with very little reason, given the waited sessions committed already (which should count down the latch). I had some little time to look at the code and I believe it may be an issue in the transaction manager resetLocks and resetLatches methods: waited sets are computed by the former in session order, and then latches are released/updated in the *same* order; this may cause the first session (the one who got the locks) to terminate *before* the dependant sessions have inserted themselves as waiting sessions, hence causing the waiting sessions (the ones who do not got the locks) hanging forever on the latch (as the session that should have counted them down completed already). Here's attached a patch to "unlock" sessions after all waiting sets have been computed.
invalid transaction state: active SQL-transaction in stateme. I have two methods. They have test porpouses. One method executes a lot of insertions and updates in a few concurrent threads for one table. For such concurrent case I'm using MVCC transactional control mode. So, I set such mode before test will do any action. As soon as first method finished - second method starts and tries to return transactional mode back to defualt (LOCKS). But it failed with such error: What I've tried and checked: Nothing helped me. Could you check - is there any issues with switching between different transaction modes or I'm doing something totally wrong? Is it allowed to change such mode for one connection multiple times?
Pessimitic lock is lost after changing a property using SQL. In our application we're synchronizing threads using pessimistic lock on a row in a revision table We have following scenario: ERROR: After changing a property in HSQLDB an another thread (with a another transaction) that is waiting in select for update gets the lock (the first transaction is not closed yet!) From this point both threads are thinking, they have the lock they and are doing their job simultaneously. This leads at the end to an inconsistency in our data.
READ COMMITTED isolation level and table locks. Insert new data to the table block other transactions on the same table. This does not happens in MySQL and PostgreSql. This behavior makes it hard to tests complex application using HSQLDB as in memory database. See example code in attachment. Is there a workaround for this compatibility problem
Concurrency problem with CMI. I have noticed a thread locking problem when concurrently querying database for large results containing timestamps (calling org.hsqldb.jdbc.JDBCResultSet.getTimestamp() repeatedly from different threads). After some debugging I narrowed it down to this method: HsqlDateTime.convertMillisToCalendar() which contains synchronized block on a static Calendar field tempCalGMT: This obviously presents a bottleneck when getting timestamp results from ResultSet using multiple threads, so I am wondering if it can perhaps be refactored in order to be more suitable for multithreaded applications?
SessionManager <-> Session Deadlock. Please, find the attachment with the Bamboo report for the Dead Lock around Session and SessionManager. The synchronized Session.close() waits for the synchronized SessionManager.removeSession(), when the synchronized SessionManager.close() waits for the synchronized CMI. Thank you in advance!
SELECT...FOR UPDATE locking unexpected behaviour. There is a thread about this that has some background info here: Basically, I'm trying to use SELECT...FOR UPDATE to coordinate access to a shared pool of tasks where each task is represented as a row. The goal is to ensure that each task is picked up by one thread only. Each thread queries for a "pending" task and then, while holding the update lock, changes that task from "pending" to "processing" so that no other "pending" query will return that task. This method has the expected behaviour on other DBs I've tried but not HSql. I've attached a test program showing the issue. You can just import it into Eclipse. I tried this program against a SAP HANA database and it works in the expected way.
Database Lock Acquisition Failure. We run hsqldb in a web environment, with Servlets under Jboss. Our stable version uses 2.3.2 but we encountered issues with query aggregates which were solved with a 2.3.4 upgrade. However upon restarting, we get a lock acquisition failure, which we have determined to be caused by the Database.reopen failing due to RuntimeException raised by LoggerFramework which is as it turns out because the name of the Database is empty. We have created a simple patch to change the database name check, but have not verified any other use cases in hsqldb aside from our own. Apologies if this is a duplicate issue, we have compared against trunk and saw there are no similar changes.
MVCC: write lock broken on savepoint rollback. Hello HSQLDB team. I've just encounter a bug on HSQLDB v 2.3.4. I've updated library to 2.4.0 but bug still exists. I will attach TestNG test case to illustrate a bug. Main scenario is (all performs in single transaction): I think it is a bug, because: I'm using in-memory MVCC HSQLDB to test my application on a fast embedded DB. I know about "select ... for update" statement, but a "dummy update" should be tested too (some DB has bugs with "for update", like MariaDB)
Issue with select for update statement. I am encountering an issue using hsqldb and "select for update" statement In a nutshell, when the "select for update" is executed, it does not seem to lock the row (or the table) during the time of the transaction. I have made a small program that reproduces the issue on hsqldb which can be found in the attachment. The same program works with Oracle and other dbs. The program executes the following statements by 3 threads: Then each thread updates a global oid. If another thread founds the same oid as it own, the program stops. As it is, the program should not stop. I have the same behavior with the 2.5.0 version and the trunk version. All the configuration I am using can be found in the resources directory.
DDL blocking on transaction in other connection with mvcc transaction model. When a transaction is running on one connection, and some data is selected in that transaction, then DDL in another connection will hang, but only with the MVCC transaction mode. From the docs it seems to me there should be no blocking in this case. See attached sample java program, run agains the latest 2.6.1 build
Sporadic lock timeout. We see sporadic errors with HSQLDB (2.3.6+) that have to do with lock timeouts. The error is unfortunately not easily reproducible, but it happens a few times a week in the Hibernate testsuite. You can download the report here which contains more information about the error in documentation/target/reports/tests/test/index.html: The test failing here is org.hibernate.userguide.sql.SQLTest#test_sql_hibernate_scalar_named_query_example, but sometimes it's a different test that fails. Here is another failure in a different test: The relevant portion of the stacktrace is this: No idea why it runs into a timeout only from time to time. Other databases work fine.
outofmemory error. We have multithreded application and many thread use DB at the same time (I know that HSLQ does not support this.Each one have to wait).They are using SQL statements like. then I have got java.lang.outofmemory.
NPE at ServerConnection thread. We had been using HSQLDB for a long time and some time ago have switched to 2.x version. Unfortunately, recently we have got freezing of our application in our old and reliable DB related code. The reason is NPE in HSQLDB's ServerConnection thread. The problem is stable reproduced on the latest HSQLDB 2.2.5 and looks like this. Exception in thread "HSQLDB Connection @1ebd75b" java.lang.NullPointerException This NPE brings to opening new server socket without closing the old one. As result, client code doesn't know anything about server side problem and waiting answer on the old socket forever: I've attached java example for the problem reproducing. Just start it and NPE will appear. For now, I've found temp. workaround for the problem: set FETCH_SIZE const in the example to zero. Another known workaround is to use separate connection per table access via ResultSet.
IllegalArgumentException in multi-threaded application. The current version 2.2.8 is not completely thread safe. Under heavy load the method org.hsqldb.HsqlDateTime.convertMillisFromCalendar may fail. You get an IllegalArgumentException, if this happens.
HSQLDB can hang if the database is corrupted. HSQLDB can sometimes hang on start after unsuccessful shutdown (e.g.: power loss). We've observed this against 2.3.2 and subsequent versions. The stack trace is as follows (trunk). Apparently, if (length == 0) condition, under certain circumstances, can never be met: Since database corruption can't be avoided entirely, it makes sense to replace the while (true) loop with at least while (!Thread.interrupted()), so that the client can interrupt the thread and break out of an endless loop.
Spurious unique index constraint violation on inserts. When inserting rows into the following table: using READ_COMMITTED / MVCC mode, and multiple threads to do inserting, spurious SQLIntegrityConstraintViolations are thrown (and also, mysteriously, all my stderr output seems to get consumed, don't know why but it's made debugging this very tricky) Having checked to ensure that there is in fact no violation of the constraint actually occurring, I wondered whether it might be that the unique index itself is failing because the resource_name field contents are "too big". Sure enough, using much shorter resource_names makes the problem go away. Spent all day tracking this one down :)
create view problem. this causes a race condition. Don't know if you should stop or just say don't do it cause it hurts. Fix if you do it is to edit .script file and delete view.
Race condition in CMI. This method waits for current state to change from SERVER_STATE_OPENNING. But the implementation of Server.java 1.8.0RC8 does not always hold that condition. The code reads: The problem is that if the newly started ServerThread is not (or too late) activated by the OS. Therefore it cannot set the state field to SERVER_STATE_OPENING and the while-loop is never entered. To fix the race condition, make sure the state equals SERVER_STATE_OPENING before entering the wait loop. I cut&pasted line 1973 from Server.run() and now it looks like:
Explicitly calling CMI causes low-probability hangs. Attached is a demo program which starts an HSQLDB server using the Server class. It tries to terminate the server by calling stop() and then shutdown(). With a rather low but still real probability (about a once per 100 runs) the shutdown() call hangs. I have observed that omitting the explicit shutdown() call and nulling the Server object instead works more reliably, perhaps even 100% reliably. It seems like some race condition or threading issue, but I'm not qualified to dig deeper. You can compile the sample using: